{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "indirect-least",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1507, -0.9086]])\n",
      "tensor([[-2.0003, -1.2439]], device='cuda:0')\n",
      "tensor([[-1.1507, -0.9086]])\n",
      "False\n",
      "tensor([[-1.1507, -0.9086]], device='cuda:0')\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "from numba.core.errors import NumbaDeprecationWarning,NumbaPendingDeprecationWarning, NumbaWarning\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', category=NumbaDeprecationWarning)\n",
    "warnings.simplefilter('ignore', category=NumbaWarning)\n",
    "import sys\n",
    "sys.path.append('/kaggle/code/ConeDetectionPointpillars')\n",
    "\n",
    "from second.data.CustomNuscDataset import * #to register dataset\n",
    "from models import * #to register model\n",
    "import torch\n",
    "from second.utils.log_tool import SimpleModelLog\n",
    "from second.builder import target_assigner_builder, voxel_builder\n",
    "from second.protos import pipeline_pb2\n",
    "from second.pytorch.builder import (box_coder_builder, input_reader_builder,\n",
    "                                    lr_scheduler_builder, optimizer_builder,\n",
    "                                    second_builder)\n",
    "from second.pytorch.core import box_torch_ops\n",
    "import torchplus\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from google.protobuf import text_format\n",
    "# from lyft_dataset_sdk.utils.geometry_utils import *\n",
    "from lyft_dataset_sdk.lyftdataset import Quaternion\n",
    "# from lyft_dataset_sdk.utils.data_classes import Box\n",
    "from nuscenes.utils.geometry_utils import *\n",
    "from nuscenes.utils.data_classes import Box\n",
    "from second.utils.progress_bar import ProgressBar\n",
    "#commented second.core.non_max_suppression, nms_cpu, __init__.py, \n",
    "# pytorch.core.box_torch_ops line 524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fresh-blame",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(model_cfg, measure_time=False):\n",
    "    voxel_generator = voxel_builder.build(model_cfg.voxel_generator)\n",
    "    bv_range = voxel_generator.point_cloud_range[[0, 1, 3, 4]]\n",
    "    box_coder = box_coder_builder.build(model_cfg.box_coder)\n",
    "    target_assigner_cfg = model_cfg.target_assigner\n",
    "    target_assigner = target_assigner_builder.build(target_assigner_cfg,\n",
    "                                                    bv_range, box_coder)\n",
    "    box_coder.custom_ndim = target_assigner._anchor_generators[0].custom_ndim\n",
    "    net = second_builder.build(\n",
    "        model_cfg, voxel_generator, target_assigner, measure_time=measure_time)\n",
    "    return net\n",
    "\n",
    "\n",
    "def merge_second_batch_multigpu(batch_list):\n",
    "    if isinstance(batch_list[0], list):\n",
    "        batch_list_c = []\n",
    "        for example in batch_list:\n",
    "            batch_list_c += example\n",
    "        batch_list = batch_list_c\n",
    "\n",
    "    example_merged = defaultdict(list)\n",
    "    for example in batch_list:\n",
    "        for k, v in example.items():\n",
    "            example_merged[k].append(v)\n",
    "    ret = {}\n",
    "    for key, elems in example_merged.items():\n",
    "        if key == 'metadata':\n",
    "            ret[key] = elems\n",
    "        elif key == \"calib\":\n",
    "            ret[key] = {}\n",
    "            for elem in elems:\n",
    "                for k1, v1 in elem.items():\n",
    "                    if k1 not in ret[key]:\n",
    "                        ret[key][k1] = [v1]\n",
    "                    else:\n",
    "                        ret[key][k1].append(v1)\n",
    "            for k1, v1 in ret[key].items():\n",
    "                ret[key][k1] = np.stack(v1, axis=0)\n",
    "        elif key == 'coordinates':\n",
    "            coors = []\n",
    "            for i, coor in enumerate(elems):\n",
    "                coor_pad = np.pad(\n",
    "                    coor, ((0, 0), (1, 0)), mode='constant', constant_values=i)\n",
    "                coors.append(coor_pad)\n",
    "            ret[key] = np.stack(coors, axis=0)\n",
    "        elif key in ['gt_names', 'gt_classes', 'gt_boxes']:\n",
    "            continue\n",
    "        else:\n",
    "            ret[key] = np.stack(elems, axis=0)\n",
    "\n",
    "    return ret\n",
    "\n",
    "def buildBBox(points,color = [1,0,0]):\n",
    "    #print(\"Let's draw a cubic using o3d.geometry.LineSet\")\n",
    "    # points = [[0, 0, 0], [1, 0, 0], [0, 1, 0], [1, 1, 0], [0, 0, 1], [1, 0, 1],\n",
    "    #           [0, 1, 1], [1, 1, 1]] x0y0z0, x0y0z1, x0y1z0, x0y1z1, x1y0z0, x1y0z1, x1y1z0, x1y1z1\n",
    "\n",
    "    points = points[[0,4,3,7,1,5,2,6],:]\n",
    "    lines = [[0, 1], [0, 2], [1, 3], [2, 3], [4, 5], [4, 6], [5, 7], [6, 7],\n",
    "             [0, 4], [1, 5], [2, 6], [3, 7]]\n",
    "    colors = [color for i in range(len(lines))]\n",
    "    line_set = o3d.geometry.LineSet()\n",
    "    line_set.points = o3d.utility.Vector3dVector(points)\n",
    "    line_set.lines = o3d.utility.Vector2iVector(lines)\n",
    "    line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "\n",
    "    return  line_set\n",
    "def merge_second_batch(batch_list):\n",
    "    if isinstance(batch_list[0], list):\n",
    "        batch_list_c = []\n",
    "        for example in batch_list:\n",
    "            batch_list_c += example\n",
    "        batch_list = batch_list_c\n",
    "\n",
    "    example_merged = defaultdict(list)\n",
    "    for example in batch_list:\n",
    "        for k, v in example.items():\n",
    "            example_merged[k].append(v)\n",
    "    ret = {}\n",
    "    for key, elems in example_merged.items():\n",
    "        if key in [\n",
    "            'voxels', 'num_points', 'num_gt', 'voxel_labels', 'gt_names', 'gt_classes', 'gt_boxes'\n",
    "        ]:\n",
    "            ret[key] = np.concatenate(elems, axis=0)\n",
    "        elif key == 'metadata':\n",
    "            ret[key] = elems\n",
    "        elif key == \"calib\":\n",
    "            ret[key] = {}\n",
    "            for elem in elems:\n",
    "                for k1, v1 in elem.items():\n",
    "                    if k1 not in ret[key]:\n",
    "                        ret[key][k1] = [v1]\n",
    "                    else:\n",
    "                        ret[key][k1].append(v1)\n",
    "            for k1, v1 in ret[key].items():\n",
    "                ret[key][k1] = np.stack(v1, axis=0)\n",
    "        elif key == 'coordinates':\n",
    "            coors = []\n",
    "            for i, coor in enumerate(elems):\n",
    "                coor_pad = np.pad(\n",
    "                    coor, ((0, 0), (1, 0)), mode='constant', constant_values=i)\n",
    "                coors.append(coor_pad)\n",
    "            ret[key] = np.concatenate(coors, axis=0)\n",
    "        elif key == 'metrics':\n",
    "            ret[key] = elems\n",
    "        else:\n",
    "            ret[key] = np.stack(elems, axis=0)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def _worker_init_fn(worker_id):\n",
    "    time_seed = np.array(time.time(), dtype=np.int32)\n",
    "    np.random.seed(time_seed + worker_id)\n",
    "\n",
    "\n",
    "def example_convert_to_torch(example, dtype=torch.float32,\n",
    "                             device=None) -> dict:\n",
    "    device = device or torch.device(\"cuda:0\")\n",
    "    example_torch = {}\n",
    "    float_names = [\n",
    "        \"voxels\", \"anchors\", \"reg_targets\", \"reg_weights\", \"bev_map\", \"importance\"\n",
    "    ]\n",
    "    for k, v in example.items():\n",
    "        if k in ['gt_names', 'gt_classes', 'gt_boxes', 'points']:\n",
    "            example_torch[k] = example[k]\n",
    "            continue\n",
    "\n",
    "        if k in float_names:\n",
    "            # slow when directly provide fp32 data with dtype=torch.half\n",
    "            example_torch[k] = torch.tensor(\n",
    "                v, dtype=torch.float32, device=device).to(dtype)\n",
    "        elif k in [\"coordinates\", \"labels\", \"num_points\"]:\n",
    "            example_torch[k] = torch.tensor(\n",
    "                v, dtype=torch.int32, device=device)\n",
    "        elif k in [\"anchors_mask\"]:\n",
    "            example_torch[k] = torch.tensor(\n",
    "                v, dtype=torch.uint8, device=device)\n",
    "        elif k == \"calib\":\n",
    "            calib = {}\n",
    "            for k1, v1 in v.items():\n",
    "                calib[k1] = torch.tensor(\n",
    "                    v1, dtype=dtype, device=device).to(dtype)\n",
    "            example_torch[k] = calib\n",
    "        elif k == \"num_voxels\":\n",
    "            example_torch[k] = torch.tensor(v)\n",
    "        else:\n",
    "            example_torch[k] = v\n",
    "    return example_torch\n",
    "\n",
    "\n",
    "def time_to_str(t, mode='min'):\n",
    "    if mode == 'min':\n",
    "        t = int(t) / 60\n",
    "        hr = t // 60\n",
    "        min = t % 60\n",
    "        return '%2d hr %02d m' % (hr, min)\n",
    "\n",
    "    elif mode == 'sec':\n",
    "        t = int(t)\n",
    "        min = t // 60\n",
    "        sec = t % 60\n",
    "        return '%2d min %02d sec' % (min, sec)\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "recreational-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/pipeline.config'\n",
    "# config_path = '/kaggle/code/ConeDetectionPointpillars/customNuscenes/configs/cones_pp_initial_v2.config'\n",
    "# config_path = '/kaggle/code/ConeDetectionPointpillars/customNuscenes/configs/cones_pp_initialak.config'\n",
    "model_dir = f'/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874'\n",
    "result_path=None\n",
    "create_folder=False\n",
    "display_step=50\n",
    "pretrained_path=\"/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_9:53/voxelnet-5850.tckpt\"\n",
    "multi_gpu=False\n",
    "measure_time=False\n",
    "resume=True\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_dir = str(Path(model_dir).resolve())\n",
    "if create_folder:\n",
    "    if Path(model_dir).exists():\n",
    "        model_dir = torchplus.train.create_folder(model_dir)\n",
    "model_dir = Path(model_dir)\n",
    "if resume:\n",
    "    cur_time = time.localtime(time.time())\n",
    "    cur_time = f'{cur_time.tm_mday}-{cur_time.tm_mon}-{cur_time.tm_year}_{cur_time.tm_hour}:{cur_time.tm_min}'\n",
    "    model_dir = model_dir / cur_time\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "if not resume and model_dir.exists():\n",
    "    raise ValueError(\"model dir exists and you don't specify resume\")\n",
    "\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "if result_path is None:\n",
    "    result_path = model_dir / 'results'\n",
    "config_file_bkp = \"pipeline.config\"\n",
    "\n",
    "if isinstance(config_path, str):\n",
    "    config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "    with open(config_path, \"r\") as f:\n",
    "        proto_str = f.read()\n",
    "        text_format.Merge(proto_str, config)\n",
    "else:\n",
    "    config = config_path\n",
    "    proto_str = text_format.MessageToString(config, indent=2)\n",
    "\n",
    "with (model_dir / config_file_bkp).open('w') as f:\n",
    "    f.write(proto_str)\n",
    "# Read config file\n",
    "input_cfg = config.train_input_reader\n",
    "eval_input_cfg = config.eval_input_reader\n",
    "model_cfg = config.model.second  # model's config\n",
    "train_cfg = config.train_config  # training config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "identified-headquarters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num parameter:  64\n"
     ]
    }
   ],
   "source": [
    "# Build neural network\n",
    "net = build_network(model_cfg, measure_time).to(device)\n",
    "\n",
    "target_assigner = net.target_assigner\n",
    "voxel_generator = net.voxel_generator\n",
    "print(\"num parameter: \", len(list(net.parameters())))\n",
    "torchplus.train.try_restore_latest_checkpoints(model_dir, [net])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "private-smile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning pretrain is loaded after restore, careful with resume\n",
      "Load pretrained parameters: \n",
      "global_step torch.Size([1])\n",
      "voxel_feature_extractor.pfn_layers.0.linear.weight torch.Size([64, 10])\n",
      "voxel_feature_extractor.pfn_layers.0.norm.weight torch.Size([64])\n",
      "voxel_feature_extractor.pfn_layers.0.norm.bias torch.Size([64])\n",
      "voxel_feature_extractor.pfn_layers.0.norm.running_mean torch.Size([64])\n",
      "voxel_feature_extractor.pfn_layers.0.norm.running_var torch.Size([64])\n",
      "voxel_feature_extractor.pfn_layers.0.norm.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.0.1.weight torch.Size([64, 64, 3, 3])\n",
      "rpn.blocks.0.2.weight torch.Size([64])\n",
      "rpn.blocks.0.2.bias torch.Size([64])\n",
      "rpn.blocks.0.2.running_mean torch.Size([64])\n",
      "rpn.blocks.0.2.running_var torch.Size([64])\n",
      "rpn.blocks.0.2.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.0.4.weight torch.Size([64, 64, 3, 3])\n",
      "rpn.blocks.0.5.weight torch.Size([64])\n",
      "rpn.blocks.0.5.bias torch.Size([64])\n",
      "rpn.blocks.0.5.running_mean torch.Size([64])\n",
      "rpn.blocks.0.5.running_var torch.Size([64])\n",
      "rpn.blocks.0.5.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.0.7.weight torch.Size([64, 64, 3, 3])\n",
      "rpn.blocks.0.8.weight torch.Size([64])\n",
      "rpn.blocks.0.8.bias torch.Size([64])\n",
      "rpn.blocks.0.8.running_mean torch.Size([64])\n",
      "rpn.blocks.0.8.running_var torch.Size([64])\n",
      "rpn.blocks.0.8.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.0.10.weight torch.Size([64, 64, 3, 3])\n",
      "rpn.blocks.0.11.weight torch.Size([64])\n",
      "rpn.blocks.0.11.bias torch.Size([64])\n",
      "rpn.blocks.0.11.running_mean torch.Size([64])\n",
      "rpn.blocks.0.11.running_var torch.Size([64])\n",
      "rpn.blocks.0.11.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.1.1.weight torch.Size([128, 64, 3, 3])\n",
      "rpn.blocks.1.2.weight torch.Size([128])\n",
      "rpn.blocks.1.2.bias torch.Size([128])\n",
      "rpn.blocks.1.2.running_mean torch.Size([128])\n",
      "rpn.blocks.1.2.running_var torch.Size([128])\n",
      "rpn.blocks.1.2.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.1.4.weight torch.Size([128, 128, 3, 3])\n",
      "rpn.blocks.1.5.weight torch.Size([128])\n",
      "rpn.blocks.1.5.bias torch.Size([128])\n",
      "rpn.blocks.1.5.running_mean torch.Size([128])\n",
      "rpn.blocks.1.5.running_var torch.Size([128])\n",
      "rpn.blocks.1.5.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.1.7.weight torch.Size([128, 128, 3, 3])\n",
      "rpn.blocks.1.8.weight torch.Size([128])\n",
      "rpn.blocks.1.8.bias torch.Size([128])\n",
      "rpn.blocks.1.8.running_mean torch.Size([128])\n",
      "rpn.blocks.1.8.running_var torch.Size([128])\n",
      "rpn.blocks.1.8.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.1.10.weight torch.Size([128, 128, 3, 3])\n",
      "rpn.blocks.1.11.weight torch.Size([128])\n",
      "rpn.blocks.1.11.bias torch.Size([128])\n",
      "rpn.blocks.1.11.running_mean torch.Size([128])\n",
      "rpn.blocks.1.11.running_var torch.Size([128])\n",
      "rpn.blocks.1.11.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.1.13.weight torch.Size([128, 128, 3, 3])\n",
      "rpn.blocks.1.14.weight torch.Size([128])\n",
      "rpn.blocks.1.14.bias torch.Size([128])\n",
      "rpn.blocks.1.14.running_mean torch.Size([128])\n",
      "rpn.blocks.1.14.running_var torch.Size([128])\n",
      "rpn.blocks.1.14.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.1.16.weight torch.Size([128, 128, 3, 3])\n",
      "rpn.blocks.1.17.weight torch.Size([128])\n",
      "rpn.blocks.1.17.bias torch.Size([128])\n",
      "rpn.blocks.1.17.running_mean torch.Size([128])\n",
      "rpn.blocks.1.17.running_var torch.Size([128])\n",
      "rpn.blocks.1.17.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.2.1.weight torch.Size([256, 128, 3, 3])\n",
      "rpn.blocks.2.2.weight torch.Size([256])\n",
      "rpn.blocks.2.2.bias torch.Size([256])\n",
      "rpn.blocks.2.2.running_mean torch.Size([256])\n",
      "rpn.blocks.2.2.running_var torch.Size([256])\n",
      "rpn.blocks.2.2.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.2.4.weight torch.Size([256, 256, 3, 3])\n",
      "rpn.blocks.2.5.weight torch.Size([256])\n",
      "rpn.blocks.2.5.bias torch.Size([256])\n",
      "rpn.blocks.2.5.running_mean torch.Size([256])\n",
      "rpn.blocks.2.5.running_var torch.Size([256])\n",
      "rpn.blocks.2.5.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.2.7.weight torch.Size([256, 256, 3, 3])\n",
      "rpn.blocks.2.8.weight torch.Size([256])\n",
      "rpn.blocks.2.8.bias torch.Size([256])\n",
      "rpn.blocks.2.8.running_mean torch.Size([256])\n",
      "rpn.blocks.2.8.running_var torch.Size([256])\n",
      "rpn.blocks.2.8.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.2.10.weight torch.Size([256, 256, 3, 3])\n",
      "rpn.blocks.2.11.weight torch.Size([256])\n",
      "rpn.blocks.2.11.bias torch.Size([256])\n",
      "rpn.blocks.2.11.running_mean torch.Size([256])\n",
      "rpn.blocks.2.11.running_var torch.Size([256])\n",
      "rpn.blocks.2.11.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.2.13.weight torch.Size([256, 256, 3, 3])\n",
      "rpn.blocks.2.14.weight torch.Size([256])\n",
      "rpn.blocks.2.14.bias torch.Size([256])\n",
      "rpn.blocks.2.14.running_mean torch.Size([256])\n",
      "rpn.blocks.2.14.running_var torch.Size([256])\n",
      "rpn.blocks.2.14.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.2.16.weight torch.Size([256, 256, 3, 3])\n",
      "rpn.blocks.2.17.weight torch.Size([256])\n",
      "rpn.blocks.2.17.bias torch.Size([256])\n",
      "rpn.blocks.2.17.running_mean torch.Size([256])\n",
      "rpn.blocks.2.17.running_var torch.Size([256])\n",
      "rpn.blocks.2.17.num_batches_tracked torch.Size([])\n",
      "rpn.deblocks.0.0.weight torch.Size([64, 128, 1, 1])\n",
      "rpn.deblocks.0.1.weight torch.Size([128])\n",
      "rpn.deblocks.0.1.bias torch.Size([128])\n",
      "rpn.deblocks.0.1.running_mean torch.Size([128])\n",
      "rpn.deblocks.0.1.running_var torch.Size([128])\n",
      "rpn.deblocks.0.1.num_batches_tracked torch.Size([])\n",
      "rpn.deblocks.1.0.weight torch.Size([128, 128, 2, 2])\n",
      "rpn.deblocks.1.1.weight torch.Size([128])\n",
      "rpn.deblocks.1.1.bias torch.Size([128])\n",
      "rpn.deblocks.1.1.running_mean torch.Size([128])\n",
      "rpn.deblocks.1.1.running_var torch.Size([128])\n",
      "rpn.deblocks.1.1.num_batches_tracked torch.Size([])\n",
      "rpn.deblocks.2.0.weight torch.Size([256, 128, 4, 4])\n",
      "rpn.deblocks.2.1.weight torch.Size([128])\n",
      "rpn.deblocks.2.1.bias torch.Size([128])\n",
      "rpn.deblocks.2.1.running_mean torch.Size([128])\n",
      "rpn.deblocks.2.1.running_var torch.Size([128])\n",
      "rpn.deblocks.2.1.num_batches_tracked torch.Size([])\n",
      "rpn.conv_cls.weight torch.Size([8, 384, 1, 1])\n",
      "rpn.conv_cls.bias torch.Size([8])\n",
      "rpn.conv_box.weight torch.Size([28, 384, 1, 1])\n",
      "rpn.conv_box.bias torch.Size([28])\n",
      "rpn_acc.total torch.Size([1])\n",
      "rpn_acc.count torch.Size([1])\n",
      "rpn_precision.total torch.Size([1])\n",
      "rpn_precision.count torch.Size([1])\n",
      "rpn_recall.total torch.Size([1])\n",
      "rpn_recall.count torch.Size([1])\n",
      "rpn_metrics.prec_total torch.Size([7])\n",
      "rpn_metrics.prec_count torch.Size([7])\n",
      "rpn_metrics.rec_total torch.Size([7])\n",
      "rpn_metrics.rec_count torch.Size([7])\n",
      "rpn_cls_loss.total torch.Size([1])\n",
      "rpn_cls_loss.count torch.Size([1])\n",
      "rpn_loc_loss.total torch.Size([1])\n",
      "rpn_loc_loss.count torch.Size([1])\n",
      "rpn_total_loss.total torch.Size([1])\n",
      "rpn_total_loss.count torch.Size([1])\n",
      "False _amp_stash\n",
      "max_num_voxels: 70000\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    }
   ],
   "source": [
    "if pretrained_path is not None:\n",
    "    print('warning pretrain is loaded after restore, careful with resume')\n",
    "    model_dict = net.state_dict()\n",
    "    pretrained_dict = torch.load(pretrained_path)\n",
    "\n",
    "    new_pretrained_dict = {}\n",
    "    for k, v in pretrained_dict.items():\n",
    "        if k in model_dict and v.shape == model_dict[k].shape:\n",
    "            new_pretrained_dict[k] = v\n",
    "    print(\"Load pretrained parameters: \")\n",
    "    for k, v in new_pretrained_dict.items():\n",
    "        print(k, v.shape)\n",
    "    model_dict.update(new_pretrained_dict)\n",
    "    net.load_state_dict(model_dict)\n",
    "    net.clear_global_step()\n",
    "    net.clear_metrics()\n",
    "if multi_gpu:\n",
    "    net_parallel = torch.nn.DataParallel(net)\n",
    "else:\n",
    "    net_parallel = net\n",
    "optimizer_cfg = train_cfg.optimizer\n",
    "loss_scale = train_cfg.loss_scale_factor\n",
    "fastai_optimizer = optimizer_builder.build(\n",
    "    optimizer_cfg,\n",
    "    net,\n",
    "    mixed=False,\n",
    "    loss_scale=loss_scale)\n",
    "if loss_scale < 0:\n",
    "    loss_scale = \"dynamic\"\n",
    "if train_cfg.enable_mixed_precision:\n",
    "    max_num_voxels = input_cfg.preprocess.max_number_of_voxels * input_cfg.batch_size\n",
    "    print(\"max_num_voxels: %d\" % (max_num_voxels))\n",
    "    from apex import amp\n",
    "    net, amp_optimizer = amp.initialize(net, fastai_optimizer,\n",
    "                                        opt_level=\"O1\",\n",
    "                                        keep_batchnorm_fp32=None,\n",
    "                                        loss_scale=loss_scale)\n",
    "    net.metrics_to_float()\n",
    "else:\n",
    "    amp_optimizer = fastai_optimizer\n",
    "torchplus.train.try_restore_latest_checkpoints(model_dir, [fastai_optimizer])\n",
    "lr_scheduler = lr_scheduler_builder.build(optimizer_cfg, amp_optimizer, train_cfg.steps)\n",
    "\n",
    "if train_cfg.enable_mixed_precision:\n",
    "    float_dtype = torch.float16\n",
    "else:\n",
    "    float_dtype = torch.float32\n",
    "\n",
    "if multi_gpu:\n",
    "    num_gpu = torch.cuda.device_count()\n",
    "    print(f\"MULTI_GPU: use {num_gpu} gpus\")\n",
    "    collate_fn = merge_second_batch_multigpu\n",
    "else:\n",
    "    collate_fn = merge_second_batch\n",
    "    num_gpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rising-specialist",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_map_size [1, 400, 600]\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-trainval...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "64386 instance,\n",
      "12 sensor,\n",
      "10200 calibrated_sensor,\n",
      "2631083 ego_pose,\n",
      "68 log,\n",
      "850 scene,\n",
      "34149 sample,\n",
      "2631083 sample_data,\n",
      "1166187 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 21.197 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 5.3 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "dataset = input_reader_builder.build(\n",
    "    input_cfg,\n",
    "    model_cfg,\n",
    "    training=True,\n",
    "    voxel_generator=voxel_generator,\n",
    "    target_assigner=target_assigner,\n",
    "    multi_gpu=multi_gpu\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=input_cfg.batch_size * num_gpu,\n",
    "    shuffle=True,\n",
    "    num_workers=input_cfg.preprocess.num_workers * num_gpu,\n",
    "    pin_memory=False,\n",
    "    collate_fn=collate_fn,\n",
    "    worker_init_fn=_worker_init_fn,\n",
    "    drop_last=not multi_gpu\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "optical-receptor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2021-01-29 17:31:17,128 - transforms - finding looplift candidates\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'voxels': array([[[ 1.07951928e+01, -5.69504188e-01, -3.43896583e-02,\n",
       "          -4.33333333e-01,  0.00000000e+00],\n",
       "         [ 1.07864057e+01, -5.08966396e-01, -3.20995047e-02,\n",
       "          -4.41176471e-01,  0.00000000e+00],\n",
       "         [ 1.07134437e+01, -5.79637576e-01, -3.38105015e-02,\n",
       "          -4.41176471e-01,  4.97601032e-02],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00]],\n",
       " \n",
       "        [[ 2.47440946e+01, -1.69396491e+01, -7.32347492e-01,\n",
       "          -4.76470588e-01,  4.97601032e-02],\n",
       "         [ 2.47943269e+01, -1.69241512e+01, -7.27681054e-01,\n",
       "          -4.64705882e-01,  1.49842024e-01],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00]],\n",
       " \n",
       "        [[-6.03824578e+00, -1.29378773e+01,  6.73340551e-03,\n",
       "          -5.00000000e-01,  1.49842024e-01],\n",
       "         [-6.05509114e+00, -1.29785782e+01,  6.49558363e-03,\n",
       "          -5.00000000e-01,  1.99620962e-01],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 9.54049863e+00,  8.13921747e+00,  2.15990060e-01,\n",
       "          -3.47058824e-01,  9.95349884e-02],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00]],\n",
       " \n",
       "        [[ 2.17100185e+01, -1.22197670e+00, -1.27428035e-01,\n",
       "          -4.52941176e-01,  1.99620962e-01],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00]],\n",
       " \n",
       "        [[-1.00746117e+01, -7.86834479e+00, -8.25580890e-02,\n",
       "          -4.68627451e-01,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00]]]),\n",
       " 'num_points': array([4, 2, 2, ..., 1, 1, 1], dtype=int32),\n",
       " 'coordinates': array([[  0, 194, 407],\n",
       "        [  0,  30, 547],\n",
       "        [  0,  70, 239],\n",
       "        ...,\n",
       "        [  0, 281, 395],\n",
       "        [  0, 187, 517],\n",
       "        [  0, 121, 199]], dtype=int32),\n",
       " 'num_voxels': array([22246]),\n",
       " 'metrics': {'voxel_gene_time': 0.022517919540405273,\n",
       "  'prep_time': 0.9102461338043213},\n",
       " 'anchors': array([[-30.      , -20.      ,  -2.6     , ...,   0.5     ,   0.8     ,\n",
       "           0.      ],\n",
       "        [-29.899834, -20.      ,  -2.6     , ...,   0.5     ,   0.8     ,\n",
       "           0.      ],\n",
       "        [-29.799665, -20.      ,  -2.6     , ...,   0.5     ,   0.8     ,\n",
       "           0.      ],\n",
       "        ...,\n",
       "        [ 29.799665,  20.      ,  -2.6     , ...,   0.6     ,   1.8     ,\n",
       "           1.57    ],\n",
       "        [ 29.899834,  20.      ,  -2.6     , ...,   0.6     ,   1.8     ,\n",
       "           1.57    ],\n",
       "        [ 30.      ,  20.      ,  -2.6     , ...,   0.6     ,   1.8     ,\n",
       "           1.57    ]], dtype=float32),\n",
       " 'anchors_mask': array([0, 0, 0, ..., 0, 0, 0], dtype=uint8),\n",
       " 'gt_names': array(['traffic_cone', 'pedestrian', 'traffic_cone', 'traffic_cone',\n",
       "        'traffic_cone', 'pedestrian', 'traffic_cone', 'traffic_cone',\n",
       "        'traffic_cone', 'pedestrian', 'traffic_cone'], dtype='<U12'),\n",
       " 'labels': array([-1, -1, -1, ..., -1, -1, -1], dtype=int32),\n",
       " 'reg_targets': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " 'importance': array([0., 0., 0., ..., 0., 0., 0.], dtype=float32),\n",
       " 'metadata': {'token': 'a0edf6edd121431785948038c54e036e'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "convertible-acting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3125"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "interracial-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [line for line in dataset[0]['reg_targets'] for i in line if i != 0]\n",
    "# res = []\n",
    "# for line in dataset[1]['reg_targets']:\n",
    "#     for i in line:\n",
    "#         if i != 0:\n",
    "#             print(line)\n",
    "#             res.append(line)\n",
    "#             break\n",
    "# #     break\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "alleged-novelty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203992"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in dataset[1]['labels'] if i == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "loving-trader",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "960000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[1]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "after-knowing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "757022"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in dataset[1]['labels'] if i == -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "vanilla-collector",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1705"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in dataset[2]['labels'] if i == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "current-annex",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1028"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in dataset[10]['labels'] if i == 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "radical-termination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-1, 1, 2}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([i for i in dataset[1]['labels'] if i != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "rough-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(dataset)):\n",
    "#     if len(set([i for i in dataset[i]['labels'] if i != 0])) > 2:\n",
    "#         print(i)\n",
    "#         print(set([i for i in dataset[i]['labels'] if i != 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-tampa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fitting-intention",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_map_size [1, 400, 600]\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-trainval...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "64386 instance,\n",
      "12 sensor,\n",
      "10200 calibrated_sensor,\n",
      "2631083 ego_pose,\n",
      "68 log,\n",
      "850 scene,\n",
      "34149 sample,\n",
      "2631083 sample_data,\n",
      "1166187 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 22.945 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 5.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = input_reader_builder.build(\n",
    "    eval_input_cfg,\n",
    "    model_cfg,\n",
    "    training=False,\n",
    "    voxel_generator=voxel_generator,\n",
    "    target_assigner=target_assigner)\n",
    "eval_dataloader = torch.utils.data.DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=input_cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=eval_input_cfg.preprocess.num_workers,\n",
    "    pin_memory=False,\n",
    "    collate_fn=merge_second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "forty-harris",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-corruption",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "naughty-supply",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: {\n",
      "  second: {\n",
      "    network_class_name: \"VoxelNet\"\n",
      "    voxel_generator {\n",
      "      full_empty_part_with_mean: false\n",
      "      point_cloud_range : [-30, -20, -2.6, 30, 20, 2.2] #[-80, -20, -2 , 60, 40, 3] # will use [0,1,3,4] means [-100,-100,100,1000]\n",
      "      voxel_size : [0.1, 0.1, 4]\n",
      "      max_number_of_points_per_voxel : 100\n",
      "    }\n",
      "    voxel_feature_extractor: {\n",
      "      module_class_name: \"PillarFeatureNet\"\n",
      "      num_filters: [64]\n",
      "      with_distance: false\n",
      "      num_input_features: 5\n",
      "    }\n",
      "    middle_feature_extractor: {\n",
      "      module_class_name: \"PointPillarsScatter\"\n",
      "      downsample_factor: 1\n",
      "      num_input_features: 64\n",
      "    }\n",
      "    rpn: {\n",
      "      module_class_name: \"RPNV2\"\n",
      "      layer_nums: [3, 5, 5]\n",
      "      layer_strides: [1, 2, 2]\n",
      "      num_filters: [64, 128, 256]\n",
      "      upsample_strides: [1, 2, 4]\n",
      "      num_upsample_filters: [128, 128, 128]\n",
      "      use_groupnorm: false\n",
      "      num_groups: 32\n",
      "      num_input_features: 64\n",
      "    }\n",
      "    loss: {\n",
      "      classification_loss: {\n",
      "        weighted_sigmoid_focal: {\n",
      "          alpha: 0.75 #0.25\n",
      "          gamma: 2.0 #2.0\n",
      "          anchorwise_output: true\n",
      "        }\n",
      "      }\n",
      "      localization_loss: {\n",
      "        weighted_smooth_l1: {\n",
      "          sigma: 3.0\n",
      "          code_weight: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "        }\n",
      "      }\n",
      "      classification_weight: 1.0\n",
      "      localization_weight: 2.0\n",
      "    }\n",
      "    num_point_features: 5 # model's num point feature should be independent of dataset\n",
      "    # Outputs\n",
      "    use_sigmoid_score: true\n",
      "    encode_background_as_zeros: true #false\n",
      "    encode_rad_error_by_sin: true\n",
      "    sin_error_factor: 1.0\n",
      "\n",
      "    use_direction_classifier: false # this can help for orientation benchmark\n",
      "    direction_loss_weight: 0.2 # enough.\n",
      "    num_direction_bins: 2\n",
      "    direction_limit_offset: 1\n",
      "\n",
      "    # Loss\n",
      "    pos_class_weight: 1.0\n",
      "    neg_class_weight: 1.0 #1.0\n",
      "\n",
      "    loss_norm_type: NormByNumPositives\n",
      "    # Postprocess\n",
      "    post_center_limit_range: [-30, -20, -2.6, 30, 20, 2.2] #[-120, -60, -4 , 80, 60, 5] #[0, -40, -2.2, 70.4, 40, 0.8]\n",
      "    nms_class_agnostic: false # only valid in multi-class nms\n",
      "\n",
      "    box_coder: {\n",
      "      ground_box3d_coder: {\n",
      "        linear_dim: false\n",
      "        encode_angle_vector: false\n",
      "      }\n",
      "    }\n",
      "    target_assigner: {\n",
      "\n",
      "      class_settings: {\n",
      "        class_name: \"traffic_cone\"\n",
      "        anchor_generator_range: {\n",
      "          sizes: [0.45, 0.5, 0.8] # wlh\n",
      "          anchor_ranges: [-30, -20, -2.6, 30, 20, 2.2]\n",
      "          rotations: [0, 1.57] # DON'T modify this unless you are very familiar with my code.\n",
      "          # custom_values: [0, 0] # velocity vector base value\n",
      "        }\n",
      "#        anchor_generator_stride: {\n",
      "#          sizes: [0.36, 0.36, 0.8] # wlh\n",
      "#          strides: [0.14, 0.14, 0.0] # if generate only 1 z_center, z_stride will be ignored\n",
      "#          offsets: [0.07, -20.09, -1.465] # origin_offset + strides / 2\n",
      "#          rotations: [0, 1.57] # 0, pi/2\n",
      "#        }\n",
      "        matched_threshold : 0.4\n",
      "        unmatched_threshold : 0.3\n",
      "        use_rotate_nms: false\n",
      "        use_multi_class_nms: false\n",
      "        nms_pre_max_size: 1000\n",
      "        nms_post_max_size: 300\n",
      "        nms_score_threshold: 0.02\n",
      "        nms_iou_threshold: 0.3\n",
      "        region_similarity_calculator: {\n",
      "          distance_similarity: {\n",
      "            distance_norm: 1.414 # match range\n",
      "            with_rotation: false\n",
      "            rotation_alpha: 0.0 # rot error contribution\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "\n",
      "      class_settings: {\n",
      "        class_name: \"pedestrian\"\n",
      "        anchor_generator_range: {\n",
      "          sizes: [0.6, 0.6, 1.8] # wlh\n",
      "          anchor_ranges: [-30, -20, -2.6, 30, 20, 2.2]\n",
      "          rotations: [0, 1.57] # DON'T modify this unless you are very familiar with my code.\n",
      "          # custom_values: [0, 0] # velocity vector base value\n",
      "        }\n",
      "        matched_threshold : 0.5\n",
      "        unmatched_threshold : 0.35\n",
      "        use_rotate_nms: false\n",
      "        use_multi_class_nms: false\n",
      "        nms_pre_max_size: 1000\n",
      "        nms_post_max_size: 300\n",
      "        nms_score_threshold: 0.02\n",
      "        nms_iou_threshold: 0.3\n",
      "        region_similarity_calculator: {\n",
      "          distance_similarity: {\n",
      "            distance_norm: 1.414 # match range\n",
      "            with_rotation: false\n",
      "            rotation_alpha: 0.0 # rot error contribution\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "\n",
      "#      class_settings: {\n",
      "#        class_name: \"animal\"\n",
      "#        anchor_generator_range: {\n",
      "#          sizes: [0.36, 0.73, 0.51] # wlh\n",
      "#          anchor_ranges: [-30, -20, -2.6, 30, 20, 2.2]\n",
      "#          rotations: [0, 1.57] # DON'T modify this unless you are very familiar with my code.\n",
      "#          # custom_values: [0, 0] # velocity vector base value\n",
      "#        }\n",
      "#        matched_threshold : 0.5\n",
      "#        unmatched_threshold : 0.35\n",
      "#        use_rotate_nms: false\n",
      "#        use_multi_class_nms: false\n",
      "#        nms_pre_max_size: 1000\n",
      "#        nms_post_max_size: 300\n",
      "#        nms_score_threshold: 0.02\n",
      "#        nms_iou_threshold: 0.3\n",
      "#        region_similarity_calculator: {\n",
      "#          distance_similarity: {\n",
      "#            distance_norm: 1.414 # match range\n",
      "#            with_rotation: false\n",
      "#            rotation_alpha: 0.0 # rot error contribution\n",
      "#          }\n",
      "#        }\n",
      "#      }\n",
      "\n",
      "\n",
      "      sample_positive_fraction : -1\n",
      "      sample_size : 512\n",
      "      assign_per_class: true\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "train_input_reader: {\n",
      "  dataset: {\n",
      "    dataset_class_name: \"CustomNuscDataset\"\n",
      "    kitti_info_path: \"/media/starlet/LdTho/data/sets/nuscenes/v1.0-trainval/infos_train.pkl\"\n",
      "    kitti_root_path: \"/media/starlet/LdTho/data/sets/nuscenes/v1.0-trainval\"\n",
      "  }\n",
      "  batch_size: 1\n",
      "  preprocess: {\n",
      "    num_workers: 4\n",
      "    shuffle_points: true\n",
      "    max_number_of_voxels: 70000\n",
      "\n",
      "    groundtruth_localization_noise_std: [0.0, 0.0, 0.0]\n",
      "    groundtruth_rotation_uniform_noise: [0.0, 0.0]\n",
      "    global_rotation_uniform_noise: [-0.3925, 0.3925]\n",
      "    global_scaling_uniform_noise: [0.95, 1.05]\n",
      "    global_random_rotation_range_per_object: [0, 0] # pi/4 ~ 3pi/4\n",
      "    global_translate_noise_std: [0.0, 0.0, 0.0] #[0.2, 0.2, 0.2]\n",
      "    anchor_area_threshold: 1 # very slow if enable when using FHD map (1600x1200x40).\n",
      "    remove_points_after_sample: true\n",
      "    groundtruth_points_drop_percentage: 0.0\n",
      "    groundtruth_drop_max_keep_points: 15\n",
      "    remove_unknown_examples: false\n",
      "    sample_importance: 1.0\n",
      "    random_flip_x: true\n",
      "    random_flip_y: true\n",
      "    remove_environment: false #false\n",
      "    database_sampler {\n",
      "#      rate: 1.0\n",
      "#      database_info_path: \"/media/starlet/LdTho/data/sets/nuscenes/v1.0-mini/nusc_custom_dbinfos_train.pkl\"\n",
      "#      sample_groups {\n",
      "#        name_to_max_num {\n",
      "#          key: \"traffic_cone\"\n",
      "#          value: 30\n",
      "#        }\n",
      "#      }\n",
      "#      database_prep_steps {\n",
      "#        filter_by_min_num_points {\n",
      "#          min_num_point_pairs {\n",
      "#            key: \"traffic_cone\"\n",
      "#            value: 2\n",
      "#          }\n",
      "#        }\n",
      "#      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "train_config: {\n",
      "  optimizer: {\n",
      "    adam_optimizer: {\n",
      "      learning_rate: {\n",
      "        one_cycle: {\n",
      "          lr_max: 1e-3\n",
      "          moms: [0.95, 0.85]\n",
      "          div_factor: 10.0\n",
      "          pct_start: 0.4\n",
      "        }\n",
      "      }\n",
      "      weight_decay: 0.01\n",
      "    }\n",
      "    fixed_weight_decay: true\n",
      "    use_moving_average: false\n",
      "  }\n",
      "#  optimizer: {\n",
      "#    adam_optimizer: {\n",
      "#      learning_rate: {\n",
      "#        exponential_decay: {\n",
      "#          initial_learning_rate: 0.0002\n",
      "#          decay_length: 0.1\n",
      "#          decay_factor: 0.8\n",
      "#          staircase: True\n",
      "#        }\n",
      "#      }\n",
      "#      weight_decay: 0.0001\n",
      "#    }\n",
      "#    fixed_weight_decay: false\n",
      "#    use_moving_average: false\n",
      "#  }\n",
      "  steps: 15000 # 120000 * 4 * 2 / 22680 ~ 40 epoch\n",
      "  steps_per_eval: 150 #500 # 1238 * 5\n",
      "  save_checkpoints_secs : 1800 # half hour\n",
      "  save_summary_steps : 10\n",
      "  enable_mixed_precision: true\n",
      "  loss_scale_factor: -1\n",
      "  clear_metrics_every_epoch: true\n",
      "}\n",
      "\n",
      "eval_input_reader: {\n",
      "  dataset: {\n",
      "    dataset_class_name: \"CustomNuscEvalDataset\"\n",
      "    kitti_info_path: \" \"\n",
      "    kitti_root_path: \"/media/starlet/LdTho/data/sets/nuscenes/v1.0-trainval\"\n",
      "  }\n",
      "  batch_size: 1\n",
      "  preprocess: {\n",
      "    max_number_of_voxels: 120000\n",
      "    shuffle_points: true\n",
      "    num_workers: 4\n",
      "    anchor_area_threshold: -1\n",
      "    remove_environment: false #false\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "num samples: 3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-ad533a793f8e>:25: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for example in tqdm_notebook(dataloader):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01dffae1abfc441d914238885c7b29d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0.8405, 0.8049, 0.7980, 0.7876, 0.7805, 0.7758, 0.7666, 0.7649],\n",
      "       device='cuda:0')\n",
      "step=150, epoch=0.048, steptime=1.195, valid=0.0, loss=2.511, cls_loss=0.2831, loc_loss=2.228, lr=0.0001014, eta= 0 hr 00 m\n",
      "Model saved: /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30, 150\n",
      "########################\n",
      " EVALUATE\n",
      "########################\n",
      "Generating eval images...\n",
      "eval images saved at /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30/images/150/\n",
      "eval images saved at /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30/images/150/\n",
      "eval images saved at /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30/images/150/\n",
      "eval images saved at /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30/images/150/\n",
      "eval images saved at /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30/images/150/\n",
      "tensor([1, 0, 1, 0], device='cuda:0')\n",
      "tensor([0.7739, 0.7567, 0.7411, 0.7349], device='cuda:0')\n",
      "generate eval images finished(406.39/s). continue training:\n",
      "tensor([0, 0, 0, 1, 0], device='cuda:0')\n",
      "tensor([0.7244, 0.7216, 0.6098, 0.6081, 0.6005], device='cuda:0')\n",
      "step=200, epoch=0.064, steptime=1.304, valid=0.0, loss=2.621, cls_loss=0.288, loc_loss=2.332, lr=0.0001024, eta= 0 hr 01 m\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0], device='cuda:0')\n",
      "tensor([0.7349, 0.7210, 0.7004, 0.6837, 0.6439, 0.6421, 0.6320, 0.6164, 0.6091,\n",
      "        0.5962], device='cuda:0')\n",
      "step=250, epoch=0.08, steptime=1.127, valid=0.0, loss=2.365, cls_loss=0.3004, loc_loss=2.065, lr=0.0001038, eta= 0 hr 02 m\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0.8500, 0.8375, 0.8341, 0.8320, 0.8281, 0.8277, 0.8239, 0.8229, 0.8228,\n",
      "        0.8228, 0.7983], device='cuda:0')\n",
      "step=300, epoch=0.096, steptime=1.119, valid=0.0, loss=18.26, cls_loss=16.19, loc_loss=2.065, lr=0.0001055, eta= 0 hr 03 m\n",
      "Model saved: /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30, 300\n",
      "########################\n",
      " EVALUATE\n",
      "########################\n",
      "Generating eval images...\n",
      "eval images saved at /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30/images/300/\n",
      "eval images saved at /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30/images/300/\n",
      "eval images saved at /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30/images/300/\n",
      "eval images saved at /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30/images/300/\n",
      "eval images saved at /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30/images/300/\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1], device='cuda:0')\n",
      "tensor([0.7334, 0.6761, 0.6576, 0.6457, 0.6252, 0.6056, 0.6043, 0.6040, 0.5994],\n",
      "       device='cuda:0')\n",
      "generate eval images finished(310.55/s). continue training:\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0.8287, 0.7947, 0.7496, 0.7467, 0.7339, 0.7307, 0.7222, 0.7100, 0.6963],\n",
      "       device='cuda:0')\n",
      "step=350, epoch=0.112, steptime=1.34, valid=0.0, loss=2.387, cls_loss=0.2587, loc_loss=2.128, lr=0.0001075, eta= 0 hr 04 m\n",
      "tensor([0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0.8046, 0.7668, 0.6706, 0.6430, 0.6319, 0.6072], device='cuda:0')\n",
      "step=400, epoch=0.128, steptime=1.128, valid=0.0, loss=2.293, cls_loss=0.2481, loc_loss=2.045, lr=0.0001098, eta= 0 hr 05 m\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0.8461, 0.8382, 0.8271, 0.8259, 0.8219, 0.8205, 0.8036, 0.8016, 0.7808,\n",
      "        0.7778, 0.7698, 0.7691, 0.7654], device='cuda:0')\n",
      "step=450, epoch=0.144, steptime=1.138, valid=0.0, loss=2.521, cls_loss=0.3428, loc_loss=2.179, lr=0.0001124, eta= 0 hr 06 m\n",
      "Model saved: /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30, 450\n",
      "########################\n",
      " EVALUATE\n",
      "########################\n",
      "Generating eval images...\n",
      "eval images saved at /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30/images/450/\n",
      "eval images saved at /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30/images/450/\n",
      "eval images saved at /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30/images/450/\n",
      "eval images saved at /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30/images/450/\n",
      "eval images saved at /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30/images/450/\n",
      "generate eval images finished(206.84/s). continue training:\n",
      "tensor([0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0.8420, 0.8405, 0.7987, 0.7917, 0.7560], device='cuda:0')\n",
      "step=500, epoch=0.16, steptime=1.445, valid=0.0, loss=2.402, cls_loss=0.2481, loc_loss=2.153, lr=0.0001153, eta= 0 hr 08 m\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0], device='cuda:0')\n",
      "tensor([0.7947, 0.7472, 0.7233, 0.7230, 0.7131, 0.7106, 0.6908, 0.6625, 0.5938,\n",
      "        0.5770, 0.5726, 0.5104, 0.5053], device='cuda:0')\n",
      "step=550, epoch=0.176, steptime=1.134, valid=0.0, loss=2.475, cls_loss=0.2435, loc_loss=2.232, lr=0.0001185, eta= 0 hr 09 m\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0.8721, 0.8644, 0.8628, 0.8524, 0.8284, 0.8230, 0.8226, 0.8180],\n",
      "       device='cuda:0')\n",
      "step=600, epoch=0.192, steptime=1.142, valid=0.0, loss=2.451, cls_loss=0.2426, loc_loss=2.209, lr=0.000122, eta= 0 hr 09 m\n",
      "Model saved: /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30, 600\n",
      "########################\n",
      " EVALUATE\n",
      "########################\n",
      "Generating eval images...\n",
      "eval images saved at /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30/images/600/\n",
      "eval images saved at /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30/images/600/\n",
      "eval images saved at /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30/images/600/\n",
      "eval images saved at /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30/images/600/\n",
      "eval images saved at /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30/images/600/\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([0.7909, 0.7870, 0.7564, 0.7485], device='cuda:0')\n",
      "generate eval images finished(324.37/s). continue training:\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0.8830, 0.8822, 0.8515, 0.8091, 0.7724, 0.7551, 0.7428, 0.7298],\n",
      "       device='cuda:0')\n",
      "step=650, epoch=0.208, steptime=1.329, valid=0.0, loss=2.352, cls_loss=0.2378, loc_loss=2.114, lr=0.0001257, eta= 0 hr 11 m\n",
      "tensor([0, 0, 1, 0, 1, 0, 0], device='cuda:0')\n",
      "tensor([0.7741, 0.7659, 0.7608, 0.7120, 0.6977, 0.6897, 0.6626],\n",
      "       device='cuda:0')\n",
      "step=700, epoch=0.224, steptime=1.157, valid=0.0, loss=2.414, cls_loss=0.2704, loc_loss=2.144, lr=0.0001298, eta= 0 hr 12 m\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0\n",
      "tensor([1, 0, 0, 1, 1, 0, 0, 1], device='cuda:0')\n",
      "tensor([0.8129, 0.7687, 0.7542, 0.6718, 0.6556, 0.6002, 0.5846, 0.5740],\n",
      "       device='cuda:0')\n",
      "step=750, epoch=0.24, steptime=1.144, valid=0.0, loss=13.12, cls_loss=11.15, loc_loss=1.962, lr=0.0001342, eta= 0 hr 12 m\n",
      "Model saved: /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30, 750\n",
      "########################\n",
      " EVALUATE\n",
      "########################\n",
      "Generating eval images...\n",
      "eval images saved at /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30/images/750/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval images saved at /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30/images/750/\n",
      "eval images saved at /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30/images/750/\n",
      "eval images saved at /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30/images/750/\n",
      "eval images saved at /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_17:30/images/750/\n",
      "tensor([0, 0, 1, 0, 1, 0], device='cuda:0')\n",
      "tensor([0.8670, 0.8403, 0.7921, 0.7356, 0.6938, 0.6579], device='cuda:0')\n",
      "generate eval images finished(268.46/s). continue training:\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 0, 1], device='cuda:0')\n",
      "tensor([0.8840, 0.8606, 0.7807, 0.7766, 0.7766, 0.7715, 0.7491, 0.7472, 0.7441],\n",
      "       device='cuda:0')\n",
      "step=800, epoch=0.256, steptime=1.363, valid=0.0, loss=2.416, cls_loss=0.2334, loc_loss=2.182, lr=0.0001388, eta= 0 hr 14 m\n",
      "tensor([0, 0, 0, 0, 0, 0, 1], device='cuda:0')\n",
      "tensor([0.8278, 0.8197, 0.7955, 0.7894, 0.7785, 0.7610, 0.6553],\n",
      "       device='cuda:0')\n",
      "step=850, epoch=0.272, steptime=1.151, valid=0.0, loss=2.349, cls_loss=0.2638, loc_loss=2.085, lr=0.0001437, eta= 0 hr 15 m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-ad533a793f8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamp_optimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                     \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spconv12cuda11/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spconv12cuda11/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm, tqdm_notebook\n",
    "import time\n",
    "model_logging = SimpleModelLog(model_dir)\n",
    "model_logging.open()\n",
    "model_logging.log_text(proto_str + \"\\n\", 0, tag=\"config\")\n",
    "\n",
    "start_step = net.get_global_step()\n",
    "total_step = train_cfg.steps\n",
    "t = time.time()\n",
    "steps_per_eval = train_cfg.steps_per_eval\n",
    "clear_metrics_every_epoch = train_cfg.clear_metrics_every_epoch\n",
    "\n",
    "amp_optimizer.zero_grad()\n",
    "step_times = []\n",
    "step = start_step\n",
    "run = True\n",
    "ave_valid_loss = 0.0\n",
    "\n",
    "try:\n",
    "    start_tic = time.time()\n",
    "    print(\"num samples: %d\" % (len(dataset)))\n",
    "    while run == True:\n",
    "        if clear_metrics_every_epoch:\n",
    "            net.clear_metrics()\n",
    "        for example in tqdm_notebook(dataloader):\n",
    "            lr_scheduler.step(net.get_global_step())\n",
    "            example.pop(\"metrics\")\n",
    "            example_torch = example_convert_to_torch(example, float_dtype)\n",
    "\n",
    "            ret_dict = net_parallel(example_torch)\n",
    "            loss = ret_dict[\"loss\"].mean()\n",
    "            cls_loss_reduced = ret_dict[\"cls_loss_reduced\"].mean()\n",
    "            loc_loss_reduced = ret_dict[\"loc_loss_reduced\"].mean()\n",
    "\n",
    "            if train_cfg.enable_mixed_precision:\n",
    "                if net.get_global_step() < 100:\n",
    "                    loss *= 1e-3\n",
    "                with amp.scale_loss(loss, amp_optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), 10.0)\n",
    "            amp_optimizer.step()\n",
    "            amp_optimizer.zero_grad()\n",
    "            net.update_global_step()\n",
    "\n",
    "            cls_preds = ret_dict[\"cls_preds\"]\n",
    "            labels = example_torch[\"labels\"]\n",
    "            cared = ret_dict[\"cared\"]\n",
    "\n",
    "            net_metrics = net.update_metrics(cls_loss_reduced,\n",
    "                                             loc_loss_reduced, cls_preds,\n",
    "                                             labels, cared)\n",
    "            step_time = (time.time() - t)\n",
    "            step_times.append(step_time)\n",
    "            t = time.time()\n",
    "            metrics = {}\n",
    "            global_step = net.get_global_step()\n",
    "\n",
    "            if global_step % display_step == 0:\n",
    "                net.eval()\n",
    "                det = net(example_torch)\n",
    "                print(det[0]['label_preds'])\n",
    "                print(det[0]['scores'])      \n",
    "                net.train()\n",
    "                eta = time.time() - start_tic\n",
    "                if measure_time:\n",
    "                    for name, val in net.get_avg_time_dict().items():\n",
    "                        print(f\"avg {name} time = {val * 1000:.3f} ms\")\n",
    "\n",
    "                metrics[\"step\"] = global_step\n",
    "                metrics['epoch'] = global_step / len(dataloader)\n",
    "                metrics['steptime'] = np.mean(step_times)\n",
    "                metrics['valid'] = ave_valid_loss\n",
    "                step_times = []\n",
    "\n",
    "                metrics[\"loss\"] = net_metrics['loss']['cls_loss'] + net_metrics['loss']['loc_loss']\n",
    "                metrics[\"cls_loss\"] = net_metrics['loss']['cls_loss']\n",
    "                metrics[\"loc_loss\"] = net_metrics['loss']['loc_loss']\n",
    "\n",
    "                if model_cfg.use_direction_classifier:\n",
    "                    dir_loss_reduced = ret_dict[\"dir_loss_reduced\"].mean()\n",
    "                    metrics[\"dir_rt\"] = float(\n",
    "                        dir_loss_reduced.detach().cpu().numpy())\n",
    "\n",
    "                metrics['lr'] = float(amp_optimizer.lr)\n",
    "                metrics['eta'] = time_to_str(eta)\n",
    "                model_logging.log_metrics(metrics, global_step)\n",
    "\n",
    "                net.clear_metrics()\n",
    "            if global_step % steps_per_eval == 0:\n",
    "                torchplus.train.save_models(model_dir, [net, amp_optimizer],\n",
    "                                            net.get_global_step())\n",
    "                model_logging.log_text(f\"Model saved: {model_dir}, {net.get_global_step()}\", global_step)\n",
    "                net.eval()\n",
    "                result_path_step = result_path / f\"step_{net.get_global_step()}\"\n",
    "                result_path_step.mkdir(parents=True, exist_ok=True)\n",
    "                model_logging.log_text(\"########################\", global_step)\n",
    "                model_logging.log_text(\" EVALUATE\",global_step)\n",
    "                model_logging.log_text(\"########################\", global_step)\n",
    "                model_logging.log_text(\"Generating eval images...\", global_step)\n",
    "                t = time.time()\n",
    "                detections = []\n",
    "                prog_bar = ProgressBar()\n",
    "                net.clear_timer()\n",
    "                cnt = 0\n",
    "                for example in iter(dataloader):\n",
    "                    cnt += 1\n",
    "                    detection = example_convert_to_torch(example, float_dtype)\n",
    "                    detection = net(detection)\n",
    "#                     filtered_sample_tokens = eval_dataset.dataset.filtered_sample_tokens\n",
    "                    filtered_sample_tokens = dataset.dataset.filtered_sample_tokens\n",
    "                    index = filtered_sample_tokens.index(detection[0]['metadata']['token'])\n",
    "                    det_boxes = detection[0]['box3d_lidar'].cpu().detach().numpy()\n",
    "                    det_labels = detection[0]['label_preds'].cpu().detach().numpy()\n",
    "                    det_scores = detection[0]['scores'].cpu().detach().numpy()\n",
    "                    \n",
    "                    det_boxes_cones = det_boxes[det_labels == 0]\n",
    "                    det_scores_cones = det_scores[det_labels == 0]\n",
    "                    det_labels_cones = det_labels[det_labels == 0]\n",
    "                    \n",
    "                    det_boxes_pedes = det_boxes[det_labels == 1]\n",
    "                    det_scores_pedes = det_scores[det_labels == 1]\n",
    "                    det_labels_pedes = det_labels[det_labels == 1]\n",
    "#                     gt_example = eval_dataset.dataset.get_sensor_data(index)\n",
    "                    gt_example = dataset.dataset.get_sensor_data(index)\n",
    "                    points = gt_example['lidar']['points']\n",
    "        \n",
    "                    gt_boxes = gt_example['lidar']['annotations']['boxes']\n",
    "                    gt_labels = gt_example['lidar']['annotations']['names']\n",
    "                    gt_boxes_cones = gt_boxes[gt_labels == \"traffic_cone\"]\n",
    "                    gt_labels_cones = gt_labels[gt_labels == \"traffic_cone\"]\n",
    "\n",
    "                    gt_boxes_pedes = gt_boxes[gt_labels == \"pedestrian\"]\n",
    "                    gt_labels_pedes = gt_labels[gt_labels == \"pedestrian\"]\n",
    "\n",
    "                    gt_scores = np.ones(len(gt_labels_cones))\n",
    "                    c = points[:, 3].reshape(-1, 1)\n",
    "                    c = np.concatenate([c, c, c], axis=1)\n",
    "                    points = points[:, 0:3]\n",
    "                    pc = o3d.geometry.PointCloud()\n",
    "                    pc.points = o3d.utility.Vector3dVector(points)\n",
    "                    pc.colors = o3d.utility.Vector3dVector(c)\n",
    "                    mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=2.0,\n",
    "                                                                                   origin=[-0, -0, -0])\n",
    "                    geo = [pc, mesh_frame]\n",
    "                    rbbox_corners_cones = box_np_ops.center_to_corner_box3d(det_boxes_cones[:, :3],\n",
    "                                                                      det_boxes_cones[:, 3:6],\n",
    "                                                                      det_boxes_cones[:, 6],\n",
    "                                                                      origin=(0.5, 0.5, 0.5), axis=2)\n",
    "                    rbbox_corners_pedes = box_np_ops.center_to_corner_box3d(det_boxes_pedes[:, :3],\n",
    "                                                                      det_boxes_pedes[:, 3:6],\n",
    "                                                                      det_boxes_pedes[:, 6],\n",
    "                                                                      origin=(0.5, 0.5, 0.5), axis=2)\n",
    "                    gt_cones_rbbox_corners = box_np_ops.center_to_corner_box3d(gt_boxes_cones[:, :3],\n",
    "                                                                         gt_boxes_cones[:, 3:6],\n",
    "                                                                         gt_boxes_cones[:, 6],\n",
    "                                                                         origin=(0.5, 0.5, 0.5), axis=2)\n",
    "                    gt_pedes_rbbox_corners = box_np_ops.center_to_corner_box3d(gt_boxes_pedes[:, :3],\n",
    "                                                                         gt_boxes_pedes[:, 3:6],\n",
    "                                                                         gt_boxes_pedes[:, 6],\n",
    "                                                                         origin=(0.5, 0.5, 0.5), axis=2)\n",
    "                    for i in range(len(rbbox_corners_cones)):\n",
    "                        geo.append(buildBBox(rbbox_corners_cones[i], color=[1, 0, 0]))\n",
    "                    for i in range(len(rbbox_corners_pedes)):\n",
    "                        geo.append(buildBBox(rbbox_corners_pedes[i], color=[1, 1, 0]))\n",
    "                    for i in range(len(gt_cones_rbbox_corners)):\n",
    "                        geo.append(buildBBox(gt_cones_rbbox_corners[i], color=[0, 1, 0]))\n",
    "                    for i in range(len(gt_pedes_rbbox_corners)):\n",
    "                        geo.append(buildBBox(gt_pedes_rbbox_corners[i], color=[0, 0, 1]))\n",
    "#                     o3d.visualization.draw_geometries(geo)\n",
    "                    vis = o3d.visualization.Visualizer()\n",
    "                    vis.create_window(visible=True)\n",
    "                    for i in range(len(geo)):\n",
    "                        vis.add_geometry(geo[i])\n",
    "                        vis.update_geometry(geo[i])\n",
    "                    vis.poll_events()\n",
    "                    vis.update_renderer()\n",
    "                    img_dir = str(Path(f'{model_dir}/images/{global_step}').resolve())\n",
    "                    img_dir = Path(img_dir)\n",
    "                    img_dir.mkdir(parents=True, exist_ok=True)\n",
    "                    vis.capture_screen_image(f'{model_dir}/images/{global_step}/eval_{cnt}.png')\n",
    "                    model_logging.log_text(f\"eval images saved at {model_dir}/images/{global_step}/\", global_step)\n",
    "                    vis.destroy_window()\n",
    "                    if cnt >= 5:\n",
    "                        break\n",
    "                if 1 in detection[0]['label_preds']:\n",
    "                    print(detection[0]['label_preds'])\n",
    "                    print(detection[0]['scores'])\n",
    "                sec_per_ex = len(dataset) / (time.time() - t)\n",
    "                model_logging.log_text(\n",
    "                    f'generate eval images finished({sec_per_ex:.2f}/s). continue training:',\n",
    "                    global_step)\n",
    "                net.train()\n",
    "                \n",
    "            step += 1\n",
    "            if step >= total_step:\n",
    "                break\n",
    "        if step >= total_step:\n",
    "            break\n",
    "except Exception as e:\n",
    "    model_logging.log_text(str(e), step)\n",
    "    model_logging.log_text(json.dumps(example['metadata'], indent=2), step)\n",
    "    torchplus.train.save_models(model_dir, [net, amp_optimizer], step)\n",
    "    raise e\n",
    "finally:\n",
    "    model_logging.close()\n",
    "    net.train()\n",
    "torchplus.train.save_models(model_dir, [net, amp_optimizer], step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-peace",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/media/starlet/LdTho/data/sets/nuscenes/v1.0-trainval/nusc_custom_dbinfos_train.pkl', 'rb') as f:\n",
    "    cus_info_file = pickle.load(f)\n",
    "with open('/media/starlet/LdTho/data/sets/nuscenes/v1.0-trainval/infos_train.pkl', 'rb') as f:\n",
    "    info_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-toronto",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "impaired-table",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-special",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "distinct-spectrum",
   "metadata": {},
   "source": [
    "## Visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-activity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "alien-ontario",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VoxelNet(\n",
       "  (voxel_feature_extractor): PillarFeatureNet(\n",
       "    (pfn_layers): ModuleList(\n",
       "      (0): PFNLayer(\n",
       "        (linear): DefaultArgLayer(in_features=10, out_features=64, bias=False)\n",
       "        (norm): DefaultArgLayer(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (middle_feature_extractor): PointPillarsScatter()\n",
       "  (rpn): RPNV2(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        (1): DefaultArgLayer(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (2): DefaultArgLayer(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "        (4): DefaultArgLayer(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (5): DefaultArgLayer(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (6): ReLU()\n",
       "        (7): DefaultArgLayer(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (8): DefaultArgLayer(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (9): ReLU()\n",
       "        (10): DefaultArgLayer(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (11): DefaultArgLayer(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (12): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        (1): DefaultArgLayer(64, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (2): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "        (4): DefaultArgLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (5): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (6): ReLU()\n",
       "        (7): DefaultArgLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (8): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (9): ReLU()\n",
       "        (10): DefaultArgLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (11): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (12): ReLU()\n",
       "        (13): DefaultArgLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (14): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (15): ReLU()\n",
       "        (16): DefaultArgLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (17): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (18): ReLU()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        (1): DefaultArgLayer(128, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (2): DefaultArgLayer(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "        (4): DefaultArgLayer(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (5): DefaultArgLayer(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (6): ReLU()\n",
       "        (7): DefaultArgLayer(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (8): DefaultArgLayer(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (9): ReLU()\n",
       "        (10): DefaultArgLayer(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (11): DefaultArgLayer(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (12): ReLU()\n",
       "        (13): DefaultArgLayer(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (14): DefaultArgLayer(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (15): ReLU()\n",
       "        (16): DefaultArgLayer(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (17): DefaultArgLayer(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (18): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (deblocks): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): DefaultArgLayer(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): DefaultArgLayer(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): DefaultArgLayer(256, 128, kernel_size=(4, 4), stride=(4, 4), bias=False)\n",
       "        (1): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (conv_cls): Conv2d(384, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv_box): Conv2d(384, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (rpn_acc): Accuracy()\n",
       "  (rpn_precision): Precision()\n",
       "  (rpn_recall): Recall()\n",
       "  (rpn_metrics): PrecisionRecall()\n",
       "  (rpn_cls_loss): Scalar()\n",
       "  (rpn_loc_loss): Scalar()\n",
       "  (rpn_total_loss): Scalar()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "surprising-tracker",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'box3d_lidar': tensor([[ 6.7495, -4.7776,  0.2637,  0.3864,  0.3830,  0.7659, -1.5742],\n",
      "        [ 9.5076, -7.3982,  0.1035,  0.3867,  0.3853,  0.7745, -1.5293],\n",
      "        [12.0970, -4.8765,  0.2988,  0.3835,  0.3754,  0.7661, -1.5879],\n",
      "        [ 8.3111, -7.4358,  0.1504,  0.3699,  0.3720,  0.7410,  1.4990],\n",
      "        [ 5.7080, -7.5948,  0.2344,  0.3796,  0.3737,  0.7455, -1.5410],\n",
      "        [14.4314, -7.0159,  0.2207,  0.3850,  0.3788,  0.7692, -1.5527],\n",
      "        [11.1903, -7.3256,  0.2227,  0.3837,  0.3791,  0.7678, -1.5547],\n",
      "        [12.7240, -7.2016,  0.2285,  0.3773,  0.3759,  0.7586, -1.5664],\n",
      "        [17.6695, -6.6163,  0.2383,  0.3776,  0.3738,  0.8115, -1.5117]],\n",
      "       device='cuda:0'), 'scores': tensor([0.8882, 0.8473, 0.8468, 0.8393, 0.8364, 0.8302, 0.8300, 0.8253, 0.8142],\n",
      "       device='cuda:0'), 'label_preds': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'), 'metadata': {'token': 'da08b54635664d7cbadadc3ca90a0db7'}}]\n"
     ]
    }
   ],
   "source": [
    "# example = next(iter(dataloader))\n",
    "example = next(iter(eval_dataloader))\n",
    "\n",
    "example = example_convert_to_torch(example, float_dtype)\n",
    "detection = net(example)\n",
    "print(detection)\n",
    "\n",
    "filtered_sample_tokens = eval_dataset.dataset.filtered_sample_tokens\n",
    "# filtered_sample_tokens = dataset.dataset.filtered_sample_tokens\n",
    "\n",
    "\n",
    "index = filtered_sample_tokens.index(detection[0]['metadata']['token'])\n",
    "det_boxes = detection[0]['box3d_lidar'].cpu().detach().numpy()\n",
    "det_labels = detection[0]['label_preds'].cpu().detach().numpy()\n",
    "det_scores = detection[0]['scores'].cpu().detach().numpy()\n",
    "\n",
    "det_boxes_cones = det_boxes[det_labels == 0]\n",
    "det_scores_cones = det_scores[det_labels == 0]\n",
    "det_labels_cones = det_labels[det_labels == 0]\n",
    "\n",
    "det_boxes_pedes = det_boxes[det_labels == 1]\n",
    "det_scores_pedes = det_scores[det_labels == 1]\n",
    "det_labels_pedes = det_labels[det_labels == 1]\n",
    "\n",
    "# gt_example = dataset.dataset.get_sensor_data(index)\n",
    "gt_example = eval_dataset.dataset.get_sensor_data(index)\n",
    "\n",
    "points = gt_example['lidar']['points']\n",
    "\n",
    "gt_boxes = gt_example['lidar']['annotations']['boxes']\n",
    "gt_labels = gt_example['lidar']['annotations']['names']\n",
    "gt_boxes_cones = gt_boxes[gt_labels == \"traffic_cone\"]\n",
    "gt_labels_cones = gt_labels[gt_labels == \"traffic_cone\"]\n",
    "\n",
    "gt_boxes_pedes = gt_boxes[gt_labels == \"pedestrian\"]\n",
    "gt_labels_pedes = gt_labels[gt_labels == \"pedestrian\"]\n",
    "\n",
    "gt_scores = np.ones(len(gt_labels_cones))\n",
    "c = points[:, 3].reshape(-1, 1)\n",
    "c = np.concatenate([c, c, c], axis=1)\n",
    "points = points[:, 0:3]\n",
    "pc = o3d.geometry.PointCloud()\n",
    "pc.points = o3d.utility.Vector3dVector(points)\n",
    "pc.colors = o3d.utility.Vector3dVector(c)\n",
    "mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=2.0,\n",
    "                                                               origin=[-0, -0, -0])\n",
    "geo = [pc, mesh_frame]\n",
    "rbbox_corners_cones = box_np_ops.center_to_corner_box3d(det_boxes_cones[:, :3],\n",
    "                                                  det_boxes_cones[:, 3:6],\n",
    "                                                  det_boxes_cones[:, 6],\n",
    "                                                  origin=(0.5, 0.5, 0.5), axis=2)\n",
    "rbbox_corners_pedes = box_np_ops.center_to_corner_box3d(det_boxes_pedes[:, :3],\n",
    "                                                  det_boxes_pedes[:, 3:6],\n",
    "                                                  det_boxes_pedes[:, 6],\n",
    "                                                  origin=(0.5, 0.5, 0.5), axis=2)\n",
    "gt_cones_rbbox_corners = box_np_ops.center_to_corner_box3d(gt_boxes_cones[:, :3],\n",
    "                                                     gt_boxes_cones[:, 3:6],\n",
    "                                                     gt_boxes_cones[:, 6],\n",
    "                                                     origin=(0.5, 0.5, 0.5), axis=2)\n",
    "gt_pedes_rbbox_corners = box_np_ops.center_to_corner_box3d(gt_boxes_pedes[:, :3],\n",
    "                                                     gt_boxes_pedes[:, 3:6],\n",
    "                                                     gt_boxes_pedes[:, 6],\n",
    "                                                     origin=(0.5, 0.5, 0.5), axis=2)\n",
    "for i in range(len(rbbox_corners_cones)):\n",
    "    geo.append(buildBBox(rbbox_corners_cones[i], color=[1, 0, 0]))\n",
    "for i in range(len(rbbox_corners_pedes)):\n",
    "    geo.append(buildBBox(rbbox_corners_pedes[i], color=[1, 0, 1]))\n",
    "for i in range(len(gt_cones_rbbox_corners)):\n",
    "    geo.append(buildBBox(gt_cones_rbbox_corners[i], color=[0, 1, 0]))\n",
    "for i in range(len(gt_pedes_rbbox_corners)):\n",
    "    geo.append(buildBBox(gt_pedes_rbbox_corners[i], color=[0, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "narrative-method",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries(geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "differential-belgium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VoxelNet(\n",
       "  (voxel_feature_extractor): PillarFeatureNet(\n",
       "    (pfn_layers): ModuleList(\n",
       "      (0): PFNLayer(\n",
       "        (linear): DefaultArgLayer(in_features=10, out_features=64, bias=False)\n",
       "        (norm): DefaultArgLayer(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (middle_feature_extractor): PointPillarsScatter()\n",
       "  (rpn): RPNV2(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        (1): DefaultArgLayer(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (2): DefaultArgLayer(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "        (4): DefaultArgLayer(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (5): DefaultArgLayer(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (6): ReLU()\n",
       "        (7): DefaultArgLayer(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (8): DefaultArgLayer(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (9): ReLU()\n",
       "        (10): DefaultArgLayer(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (11): DefaultArgLayer(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (12): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        (1): DefaultArgLayer(64, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (2): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "        (4): DefaultArgLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (5): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (6): ReLU()\n",
       "        (7): DefaultArgLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (8): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (9): ReLU()\n",
       "        (10): DefaultArgLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (11): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (12): ReLU()\n",
       "        (13): DefaultArgLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (14): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (15): ReLU()\n",
       "        (16): DefaultArgLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (17): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (18): ReLU()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        (1): DefaultArgLayer(128, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (2): DefaultArgLayer(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "        (4): DefaultArgLayer(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (5): DefaultArgLayer(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (6): ReLU()\n",
       "        (7): DefaultArgLayer(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (8): DefaultArgLayer(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (9): ReLU()\n",
       "        (10): DefaultArgLayer(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (11): DefaultArgLayer(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (12): ReLU()\n",
       "        (13): DefaultArgLayer(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (14): DefaultArgLayer(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (15): ReLU()\n",
       "        (16): DefaultArgLayer(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (17): DefaultArgLayer(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (18): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (deblocks): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): DefaultArgLayer(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): DefaultArgLayer(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): DefaultArgLayer(256, 128, kernel_size=(4, 4), stride=(4, 4), bias=False)\n",
       "        (1): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (conv_cls): Conv2d(384, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv_box): Conv2d(384, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (rpn_acc): Accuracy()\n",
       "  (rpn_precision): Precision()\n",
       "  (rpn_recall): Recall()\n",
       "  (rpn_metrics): PrecisionRecall()\n",
       "  (rpn_cls_loss): Scalar()\n",
       "  (rpn_loc_loss): Scalar()\n",
       "  (rpn_total_loss): Scalar()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

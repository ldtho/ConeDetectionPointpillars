{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "danish-sussex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4063, -0.2217]])\n",
      "tensor([[0.8418, 0.1938]], device='cuda:0')\n",
      "tensor([[-0.4063, -0.2217]])\n",
      "False\n",
      "tensor([[-0.4063, -0.2217]], device='cuda:0')\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "from numba.core.errors import NumbaDeprecationWarning,NumbaPendingDeprecationWarning, NumbaWarning\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', category=NumbaDeprecationWarning)\n",
    "warnings.simplefilter('ignore', category=NumbaWarning)\n",
    "import sys\n",
    "sys.path.append('/kaggle/code/ConeDetectionPointpillars')\n",
    "\n",
    "from second.data.CustomNuscDataset import * #to register dataset\n",
    "from models import * #to register model\n",
    "import torch\n",
    "from second.utils.log_tool import SimpleModelLog\n",
    "from second.builder import target_assigner_builder, voxel_builder\n",
    "from second.protos import pipeline_pb2\n",
    "from second.pytorch.builder import (box_coder_builder, input_reader_builder,\n",
    "                                    lr_scheduler_builder, optimizer_builder,\n",
    "                                    second_builder)\n",
    "from second.pytorch.core import box_torch_ops\n",
    "import torchplus\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from google.protobuf import text_format\n",
    "# from lyft_dataset_sdk.utils.geometry_utils import *\n",
    "from lyft_dataset_sdk.lyftdataset import Quaternion\n",
    "from apex import amp\n",
    "# from lyft_dataset_sdk.utils.data_classes import Box\n",
    "from nuscenes.utils.geometry_utils import *\n",
    "from nuscenes.utils.data_classes import Box\n",
    "from second.utils.progress_bar import ProgressBar\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "#commented second.core.non_max_suppression, nms_cpu, __init__.py, \n",
    "# pytorch.core.box_torch_ops line 524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "raised-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(model_cfg, measure_time=False):\n",
    "    voxel_generator = voxel_builder.build(model_cfg.voxel_generator)\n",
    "    bv_range = voxel_generator.point_cloud_range[[0, 1, 3, 4]]\n",
    "    box_coder = box_coder_builder.build(model_cfg.box_coder)\n",
    "    target_assigner_cfg = model_cfg.target_assigner\n",
    "    target_assigner = target_assigner_builder.build(target_assigner_cfg,\n",
    "                                                    bv_range, box_coder)\n",
    "    box_coder.custom_ndim = target_assigner._anchor_generators[0].custom_ndim\n",
    "    net = second_builder.build(\n",
    "        model_cfg, voxel_generator, target_assigner, measure_time=measure_time)\n",
    "    return net\n",
    "\n",
    "\n",
    "def merge_second_batch_multigpu(batch_list):\n",
    "    if isinstance(batch_list[0], list):\n",
    "        batch_list_c = []\n",
    "        for example in batch_list:\n",
    "            batch_list_c += example\n",
    "        batch_list = batch_list_c\n",
    "\n",
    "    example_merged = defaultdict(list)\n",
    "    for example in batch_list:\n",
    "        for k, v in example.items():\n",
    "            example_merged[k].append(v)\n",
    "    ret = {}\n",
    "    for key, elems in example_merged.items():\n",
    "        if key == 'metadata':\n",
    "            ret[key] = elems\n",
    "        elif key == \"calib\":\n",
    "            ret[key] = {}\n",
    "            for elem in elems:\n",
    "                for k1, v1 in elem.items():\n",
    "                    if k1 not in ret[key]:\n",
    "                        ret[key][k1] = [v1]\n",
    "                    else:\n",
    "                        ret[key][k1].append(v1)\n",
    "            for k1, v1 in ret[key].items():\n",
    "                ret[key][k1] = np.stack(v1, axis=0)\n",
    "        elif key == 'coordinates':\n",
    "            coors = []\n",
    "            for i, coor in enumerate(elems):\n",
    "                coor_pad = np.pad(\n",
    "                    coor, ((0, 0), (1, 0)), mode='constant', constant_values=i)\n",
    "                coors.append(coor_pad)\n",
    "            ret[key] = np.stack(coors, axis=0)\n",
    "        elif key in ['gt_names', 'gt_classes', 'gt_boxes']:\n",
    "            continue\n",
    "        else:\n",
    "            ret[key] = np.stack(elems, axis=0)\n",
    "\n",
    "    return ret\n",
    "\n",
    "def buildBBox(points,color = [1,0,0]):\n",
    "    #print(\"Let's draw a cubic using o3d.geometry.LineSet\")\n",
    "    # points = [[0, 0, 0], [1, 0, 0], [0, 1, 0], [1, 1, 0], [0, 0, 1], [1, 0, 1],\n",
    "    #           [0, 1, 1], [1, 1, 1]] x0y0z0, x0y0z1, x0y1z0, x0y1z1, x1y0z0, x1y0z1, x1y1z0, x1y1z1\n",
    "\n",
    "    points = points[[0,4,3,7,1,5,2,6],:]\n",
    "    lines = [[0, 1], [0, 2], [1, 3], [2, 3], [4, 5], [4, 6], [5, 7], [6, 7],\n",
    "             [0, 4], [1, 5], [2, 6], [3, 7]]\n",
    "    colors = [color for i in range(len(lines))]\n",
    "    line_set = o3d.geometry.LineSet()\n",
    "    line_set.points = o3d.utility.Vector3dVector(points)\n",
    "    line_set.lines = o3d.utility.Vector2iVector(lines)\n",
    "    line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "\n",
    "    return  line_set\n",
    "def merge_second_batch(batch_list):\n",
    "    if isinstance(batch_list[0], list):\n",
    "        batch_list_c = []\n",
    "        for example in batch_list:\n",
    "            batch_list_c += example\n",
    "        batch_list = batch_list_c\n",
    "\n",
    "    example_merged = defaultdict(list)\n",
    "    for example in batch_list:\n",
    "        for k, v in example.items():\n",
    "            example_merged[k].append(v)\n",
    "    ret = {}\n",
    "    for key, elems in example_merged.items():\n",
    "        if key in [\n",
    "            'voxels', 'num_points', 'num_gt', 'voxel_labels', 'gt_names', 'gt_classes', 'gt_boxes'\n",
    "        ]:\n",
    "            ret[key] = np.concatenate(elems, axis=0)\n",
    "        elif key == 'metadata':\n",
    "            ret[key] = elems\n",
    "        elif key == \"calib\":\n",
    "            ret[key] = {}\n",
    "            for elem in elems:\n",
    "                for k1, v1 in elem.items():\n",
    "                    if k1 not in ret[key]:\n",
    "                        ret[key][k1] = [v1]\n",
    "                    else:\n",
    "                        ret[key][k1].append(v1)\n",
    "            for k1, v1 in ret[key].items():\n",
    "                ret[key][k1] = np.stack(v1, axis=0)\n",
    "        elif key == 'coordinates':\n",
    "            coors = []\n",
    "            for i, coor in enumerate(elems):\n",
    "                coor_pad = np.pad(\n",
    "                    coor, ((0, 0), (1, 0)), mode='constant', constant_values=i)\n",
    "                coors.append(coor_pad)\n",
    "            ret[key] = np.concatenate(coors, axis=0)\n",
    "        elif key == 'metrics':\n",
    "            ret[key] = elems\n",
    "        else:\n",
    "            ret[key] = np.stack(elems, axis=0)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def _worker_init_fn(worker_id):\n",
    "    time_seed = np.array(time.time(), dtype=np.int32)\n",
    "    np.random.seed(time_seed + worker_id)\n",
    "\n",
    "\n",
    "def example_convert_to_torch(example, dtype=torch.float32,\n",
    "                             device=None) -> dict:\n",
    "    device = device or torch.device(\"cuda:0\")\n",
    "    example_torch = {}\n",
    "    float_names = [\n",
    "        \"voxels\", \"anchors\", \"reg_targets\", \"reg_weights\", \"bev_map\", \"importance\"\n",
    "    ]\n",
    "    for k, v in example.items():\n",
    "        if k in ['gt_names', 'gt_classes', 'gt_boxes', 'points']:\n",
    "            example_torch[k] = example[k]\n",
    "            continue\n",
    "\n",
    "        if k in float_names:\n",
    "            # slow when directly provide fp32 data with dtype=torch.half\n",
    "            example_torch[k] = torch.tensor(\n",
    "                v, dtype=torch.float32, device=device).to(dtype)\n",
    "        elif k in [\"coordinates\", \"labels\", \"num_points\"]:\n",
    "            example_torch[k] = torch.tensor(\n",
    "                v, dtype=torch.int32, device=device)\n",
    "        elif k in [\"anchors_mask\"]:\n",
    "            example_torch[k] = torch.tensor(\n",
    "                v, dtype=torch.uint8, device=device)\n",
    "        elif k == \"calib\":\n",
    "            calib = {}\n",
    "            for k1, v1 in v.items():\n",
    "                calib[k1] = torch.tensor(\n",
    "                    v1, dtype=dtype, device=device).to(dtype)\n",
    "            example_torch[k] = calib\n",
    "        elif k == \"num_voxels\":\n",
    "            example_torch[k] = torch.tensor(v)\n",
    "        else:\n",
    "            example_torch[k] = v\n",
    "    return example_torch\n",
    "\n",
    "\n",
    "def time_to_str(t, mode='min'):\n",
    "    if mode == 'min':\n",
    "        t = int(t) / 60\n",
    "        hr = t // 60\n",
    "        min = t % 60\n",
    "        return '%2d hr %02d m' % (hr, min)\n",
    "\n",
    "    elif mode == 'sec':\n",
    "        t = int(t)\n",
    "        min = t // 60\n",
    "        sec = t % 60\n",
    "        return '%2d min %02d sec' % (min, sec)\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "def get_paddings_indicator(actual_num, max_num, axis=0):\n",
    "    \"\"\"\n",
    "    Create boolean mask by actually number of a padded tensor.\n",
    "    :param actual_num:\n",
    "    :param max_num:\n",
    "    :param axis:\n",
    "    :return: [type]: [description]\n",
    "    \"\"\"\n",
    "    actual_num = torch.unsqueeze(actual_num, axis+1)\n",
    "    max_num_shape = [1] * len(actual_num.shape)\n",
    "    max_num_shape[axis+1] = -1\n",
    "    max_num = torch.arange(max_num, dtype=torch.int, device=actual_num.device).view(max_num_shape)\n",
    "    # tiled_actual_num : [N, M, 1]\n",
    "    # tiled_actual_num : [[3,3,3,3,3], [4,4,4,4,4], [2,2,2,2,2]]\n",
    "    # title_max_num : [[0,1,2,3,4], [0,1,2,3,4], [0,1,2,3,4]]\n",
    "    paddings_indicator = actual_num.int() > max_num\n",
    "    # paddings_indicator shape : [batch_size, max_num]\n",
    "    return paddings_indicator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "supported-triumph",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '/kaggle/code/ConeDetectionPointpillars/customNuscenes/configs/cones_pp_new_layers.config'\n",
    "# config_path = '/kaggle/code/ConeDetectionPointpillars/customNuscenes/configs/cones_pp_initial_v2.config'\n",
    "# config_path = '/kaggle/code/ConeDetectionPointpillars/customNuscenes/configs/cones_pp_initialak.config'\n",
    "model_dir = f'/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/cones_pp_new_layers.{time.time()}'\n",
    "result_path = None\n",
    "create_folder = False\n",
    "display_step = 50\n",
    "# pretrained_path=\"/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_9:53/voxelnet-5850.tckpt\"\n",
    "pretrained_path = None\n",
    "multi_gpu = False\n",
    "measure_time = False\n",
    "resume = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_dir = str(Path(model_dir).resolve())\n",
    "cur_time = time.localtime(time.time())\n",
    "cur_time = f'{cur_time.tm_year}-{cur_time.tm_mon}-{cur_time.tm_mday}_{cur_time.tm_hour}:{cur_time.tm_min}'\n",
    "if create_folder:\n",
    "    if Path(model_dir).exists():\n",
    "        model_dir = torchplus.train.create_folder(model_dir)\n",
    "model_dir = Path(model_dir)\n",
    "\n",
    "if not resume and model_dir.exists():\n",
    "    raise ValueError(\"model dir exists and you don't specify resume\")\n",
    "\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "if result_path is None:\n",
    "    result_path = model_dir / 'results'/ cur_time\n",
    "config_file_bkp = \"pipeline.config\"\n",
    "\n",
    "if isinstance(config_path, str):\n",
    "    config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "    with open(config_path, \"r\") as f:\n",
    "        proto_str = f.read()\n",
    "        text_format.Merge(proto_str, config)\n",
    "else:\n",
    "    config = config_path\n",
    "    proto_str = text_format.MessageToString(config, indent=2)\n",
    "\n",
    "with (model_dir / config_file_bkp).open('w') as f:\n",
    "    f.write(proto_str)\n",
    "# Read config file\n",
    "input_cfg = config.train_input_reader\n",
    "eval_input_cfg = config.eval_input_reader\n",
    "model_cfg = config.model.second  # model's config\n",
    "train_cfg = config.train_config  # training config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tender-peninsula",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "middle_class_name PointPillarsScatterMMS\n",
      "num_class 1\n",
      "rpn_layer_nums [3, 5, 5]\n",
      "rpn_layer_strides [2, 2, 2]\n",
      "rpn_num_filters [64, 128, 256]\n",
      "rpn_upsample_strides [1.0, 2.0, 4.0]\n",
      "rpn_num_upsample_filters [128, 128, 128]\n",
      "num_rpn_input_filters 64\n",
      "target_assigner.num_anchors_per_location 2\n",
      "encode_background_as_zeros True\n",
      "use_direction_classifier False\n",
      "use_groupnorm False\n",
      "num_groups 32\n",
      "num_filters, num_upsample_filters, upsample_strides, upsample_strides [64, 128, 256] [128, 128, 128] [1, 2, 4] [1, 2, 4]\n",
      "self.name voxelnetmms\n",
      "self._sin_error_factor 1.0\n",
      "self._num_class 1\n",
      "self._use_rotate_nms False\n",
      "self._multiclass_nms False\n",
      "self._nms_score_thresholds [0.019999999552965164]\n",
      "self._nms_pre_max_sizes [1000]\n",
      "self._nms_post_max_sizes [300]\n",
      "self._nms_iou_thresholds [0.30000001192092896]\n",
      "self._use_sigmoid_score True\n",
      "self._encode_background_as_zeros True\n",
      "self._use_direction_classifier False\n",
      "self._total_forward_time 0.0\n",
      "self._total_postprocess_time 0.0\n",
      "self._total_inference_count 0\n",
      "self._num_input_features 4\n",
      "self._box_coder <second.pytorch.core.box_coders.GroundBox3dCoderTorch object at 0x7fa629135280>\n",
      "self.target_assigner <second.core.target_assigner.TargetAssigner object at 0x7fa629135c10>\n",
      "self.voxel_generator <spconv.utils.VoxelGeneratorV2 object at 0x7fa629135160>\n",
      "self._pos_cls_weight 1.0\n",
      "self._neg_cls_weight 1.0\n",
      "self._encode_rad_error_by_sin True\n",
      "self._loss_norm_type LossNormType.NormByNumPositives\n",
      "self._dir_loss_ftor <second.pytorch.core.losses.WeightedSoftmaxClassificationLoss object at 0x7fa7182d1a00>\n",
      "self._diff_loc_loss_ftor <second.pytorch.core.losses.WeightedSmoothL1LocalizationLoss object at 0x7fa7182d1c40>\n",
      "self._dir_offset 0.0\n",
      "self._loc_loss_ftor <second.pytorch.core.losses.WeightedSmoothL1LocalizationLoss object at 0x7fa7182d12b0>\n",
      "self._cls_loss_ftor <second.pytorch.core.losses.SigmoidFocalClassificationLoss object at 0x7fa6291351f0>\n",
      "self._direction_loss_weight 0.20000000298023224\n",
      "self._cls_loss_weight 1.0\n",
      "self._loc_loss_weight 2.0\n",
      "self._batch_size 2\n",
      "self.measure_time False\n",
      "self._nms_class_agnostic False\n",
      "self._num_direction_bins 2\n",
      "self._dir_limit_offset 1.0\n",
      "self.voxel_feature_extractor PillarFeatureNetMMS(\n",
      "  (pfn_layers): ModuleList(\n",
      "    (0): PFNLayerMMS(\n",
      "      (linear): Linear(in_features=9, out_features=64, bias=False)\n",
      "      (norm): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(9, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (conv2): Conv2d(100, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (t_conv): ConvTranspose2d(100, 1, kernel_size=(1, 8), stride=(1, 7))\n",
      "      (conv3): Conv2d(64, 64, kernel_size=(1, 34), stride=(1, 1), dilation=(1, 3))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "self.middle_feature_extractor PointPillarsScatterMMS()\n",
      "self.rpn RPNV2MMS(\n",
      "  (block1): Sequential(\n",
      "    (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "    (1): DefaultArgLayer(64, 64, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "    (2): DefaultArgLayer(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "    (4): DefaultArgLayer(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (5): DefaultArgLayer(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): DefaultArgLayer(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (8): DefaultArgLayer(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (9): ReLU()\n",
      "    (10): DefaultArgLayer(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (11): DefaultArgLayer(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (12): ReLU()\n",
      "  )\n",
      "  (deconv1): Sequential(\n",
      "    (0): DefaultArgLayer(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "    (1): DefaultArgLayer(64, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "    (2): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "    (4): DefaultArgLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (5): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): DefaultArgLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (8): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (9): ReLU()\n",
      "    (10): DefaultArgLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (11): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (12): ReLU()\n",
      "    (13): DefaultArgLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (14): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (15): ReLU()\n",
      "    (16): DefaultArgLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (17): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (18): ReLU()\n",
      "  )\n",
      "  (deconv2): Sequential(\n",
      "    (0): DefaultArgLayer(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "    (1): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (block3): Sequential(\n",
      "    (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "    (1): DefaultArgLayer(128, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "    (2): DefaultArgLayer(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "    (4): DefaultArgLayer(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (5): DefaultArgLayer(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): DefaultArgLayer(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (8): DefaultArgLayer(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (9): ReLU()\n",
      "    (10): DefaultArgLayer(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (11): DefaultArgLayer(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (12): ReLU()\n",
      "    (13): DefaultArgLayer(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (14): DefaultArgLayer(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (15): ReLU()\n",
      "    (16): DefaultArgLayer(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (17): DefaultArgLayer(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (18): ReLU()\n",
      "  )\n",
      "  (deconv3): Sequential(\n",
      "    (0): DefaultArgLayer(256, 128, kernel_size=(4, 4), stride=(4, 4), bias=False)\n",
      "    (1): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv_cls): Conv2d(384, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv_box): Conv2d(384, 14, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "self.rpn_acc Accuracy()\n",
      "self.rpn_precision Precision()\n",
      "self.rpn_recall Recall()\n",
      "self.rpn_metrics PrecisionRecall()\n",
      "self.rpn_cls_loss Scalar()\n",
      "self.rpn_loc_loss Scalar()\n",
      "self.rpn_total_loss Scalar()\n",
      "num parameter:  72\n"
     ]
    }
   ],
   "source": [
    "# Build neural network\n",
    "net = build_network(model_cfg, measure_time).to(device)\n",
    "\n",
    "target_assigner = net.target_assigner\n",
    "voxel_generator = net.voxel_generator\n",
    "print(\"num parameter: \", len(list(net.parameters())))\n",
    "torchplus.train.try_restore_latest_checkpoints(model_dir, [net])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dietary-grain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False _amp_stash\n",
      "max_num_voxels: 140000\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    }
   ],
   "source": [
    "if pretrained_path is not None:\n",
    "    print('warning pretrain is loaded after restore, careful with resume')\n",
    "    model_dict = net.state_dict()\n",
    "    pretrained_dict = torch.load(pretrained_path)\n",
    "\n",
    "    new_pretrained_dict = {}\n",
    "    for k, v in pretrained_dict.items():\n",
    "        if k in model_dict and v.shape == model_dict[k].shape:\n",
    "            new_pretrained_dict[k] = v\n",
    "    print(\"Load pretrained parameters: \")\n",
    "    for k, v in new_pretrained_dict.items():\n",
    "        print(k, v.shape)\n",
    "    model_dict.update(new_pretrained_dict)\n",
    "    net.load_state_dict(model_dict)\n",
    "    net.clear_global_step()\n",
    "    net.clear_metrics()\n",
    "if multi_gpu:\n",
    "    net_parallel = torch.nn.DataParallel(net)\n",
    "else:\n",
    "    net_parallel = net\n",
    "optimizer_cfg = train_cfg.optimizer\n",
    "loss_scale = train_cfg.loss_scale_factor\n",
    "fastai_optimizer = optimizer_builder.build(\n",
    "    optimizer_cfg,\n",
    "    net,\n",
    "    mixed=False,\n",
    "    loss_scale=loss_scale)\n",
    "if loss_scale < 0:\n",
    "    loss_scale = \"dynamic\"\n",
    "if train_cfg.enable_mixed_precision:\n",
    "    max_num_voxels = input_cfg.preprocess.max_number_of_voxels * input_cfg.batch_size\n",
    "    print(\"max_num_voxels: %d\" % (max_num_voxels))\n",
    "\n",
    "    net, amp_optimizer = amp.initialize(net, fastai_optimizer,\n",
    "                                        opt_level=\"O1\",\n",
    "                                        keep_batchnorm_fp32=None,\n",
    "                                        loss_scale=loss_scale)\n",
    "    net.metrics_to_float()\n",
    "else:\n",
    "    amp_optimizer = fastai_optimizer\n",
    "torchplus.train.try_restore_latest_checkpoints(model_dir, [fastai_optimizer])\n",
    "lr_scheduler = lr_scheduler_builder.build(optimizer_cfg, amp_optimizer, train_cfg.steps)\n",
    "\n",
    "if train_cfg.enable_mixed_precision:\n",
    "    float_dtype = torch.float16\n",
    "else:\n",
    "    float_dtype = torch.float32\n",
    "\n",
    "if multi_gpu:\n",
    "    num_gpu = torch.cuda.device_count()\n",
    "    print(f\"MULTI_GPU: use {num_gpu} gpus\")\n",
    "    collate_fn = merge_second_batch_multigpu\n",
    "else:\n",
    "    collate_fn = merge_second_batch\n",
    "    num_gpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "monetary-taiwan",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_map_size [1, 200, 300]\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.311 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "dataset = input_reader_builder.build(\n",
    "    input_cfg,\n",
    "    model_cfg,\n",
    "    training=True,\n",
    "    voxel_generator=voxel_generator,\n",
    "    target_assigner=target_assigner,\n",
    "    multi_gpu=multi_gpu\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=input_cfg.batch_size * num_gpu,\n",
    "    shuffle=True,\n",
    "    num_workers=input_cfg.preprocess.num_workers * num_gpu,\n",
    "    pin_memory=False,\n",
    "    collate_fn=collate_fn,\n",
    "    worker_init_fn=_worker_init_fn,\n",
    "    drop_last=not multi_gpu\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-alliance",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "active-carry",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2021-03-09 12:15:27,874 - transforms - finding looplift candidates\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "120000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[1]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eleven-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [line for line in dataset[0]['reg_targets'] for i in line if i != 0]\n",
    "# res = []\n",
    "# for line in dataset[1]['reg_targets']:\n",
    "#     for i in line:\n",
    "#         if i != 0:\n",
    "#             print(line)\n",
    "#             res.append(line)\n",
    "#             break\n",
    "# #     break\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "likely-recipe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119504"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in dataset[1]['labels'] if i == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-belarus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "young-matrix",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# eval_dataset = input_reader_builder.build(\n",
    "#     eval_input_cfg,\n",
    "#     model_cfg,\n",
    "#     training=False,\n",
    "#     voxel_generator=voxel_generator,\n",
    "#     target_assigner=target_assigner)\n",
    "# eval_dataloader = torch.utils.data.DataLoader(\n",
    "#     eval_dataset,\n",
    "#     batch_size=input_cfg.batch_size,\n",
    "#     shuffle=True,\n",
    "#     num_workers=eval_input_cfg.preprocess.num_workers,\n",
    "#     pin_memory=False,\n",
    "#     collate_fn=merge_second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-lancaster",
   "metadata": {},
   "source": [
    "## Working on layer adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "tested-progress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: {\n",
      "  second: {\n",
      "    network_class_name: \"VoxelNetMMS\"\n",
      "    voxel_generator {\n",
      "      full_empty_part_with_mean: false\n",
      "      point_cloud_range : [-30, -20, -2.6, 30, 20, 2.2]\n",
      "      voxel_size : [0.1, 0.1, 4]\n",
      "      max_number_of_points_per_voxel : 100\n",
      "    }\n",
      "    voxel_feature_extractor: {\n",
      "      module_class_name: \"PillarFeatureNetMMS\"\n",
      "      num_filters: [64]\n",
      "      with_distance: false\n",
      "      num_input_features: 4\n",
      "    }\n",
      "    middle_feature_extractor: {\n",
      "      module_class_name: \"PointPillarsScatterMMS\"\n",
      "      downsample_factor: 1\n",
      "      num_input_features: 64\n",
      "    }\n",
      "    rpn: {\n",
      "      module_class_name: \"RPNV2MMS\"\n",
      "      layer_nums: [3, 5, 5]\n",
      "      layer_strides: [2, 2, 2]\n",
      "      num_filters: [64, 128, 256]\n",
      "      upsample_strides: [1, 2, 4]\n",
      "      num_upsample_filters: [128, 128, 128]\n",
      "      use_groupnorm: false\n",
      "      num_groups: 32\n",
      "      num_input_features: 64\n",
      "    }\n",
      "    loss: {\n",
      "      classification_loss: {\n",
      "        weighted_sigmoid_focal: {\n",
      "          alpha: 0.25\n",
      "          gamma: 2.0\n",
      "          anchorwise_output: true\n",
      "        }\n",
      "      }\n",
      "      localization_loss: {\n",
      "        weighted_smooth_l1: {\n",
      "          sigma: 3.0\n",
      "          code_weight: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "        }\n",
      "      }\n",
      "      classification_weight: 1.0\n",
      "      localization_weight: 2.0\n",
      "    }\n",
      "    num_point_features: 4 # model's num point feature should be independent of dataset\n",
      "    # Outputs\n",
      "    use_sigmoid_score: true\n",
      "    encode_background_as_zeros: true #false\n",
      "    encode_rad_error_by_sin: true\n",
      "    sin_error_factor: 1.0\n",
      "\n",
      "    use_direction_classifier: false # this can help for orientation benchmark\n",
      "    direction_loss_weight: 0.2 # enough.\n",
      "    num_direction_bins: 2\n",
      "    direction_limit_offset: 1\n",
      "\n",
      "    # Loss\n",
      "    pos_class_weight: 1.0\n",
      "    neg_class_weight: 1.0\n",
      "\n",
      "    loss_norm_type: NormByNumPositives\n",
      "    # Postprocess\n",
      "    post_center_limit_range: [-30, -20, -2.6, 30, 20, 2.2]\n",
      "    nms_class_agnostic: false # only valid in multi-class nms\n",
      "\n",
      "    box_coder: {\n",
      "      ground_box3d_coder: {\n",
      "        linear_dim: false\n",
      "        encode_angle_vector: false\n",
      "      }\n",
      "    }\n",
      "    target_assigner: {\n",
      "\n",
      "      class_settings: {\n",
      "        class_name: \"traffic_cone\"\n",
      "        anchor_generator_range: {\n",
      "          sizes: [0.39694519, 0.40359262, 1.06232151] # wlh\n",
      "          anchor_ranges: [-30, -20, -2.6, 30, 20, 2.2]\n",
      "          rotations: [0, 1.57] # DON'T modify this unless you are very familiar with my code.\n",
      "          # custom_values: [0, 0] # velocity vector base value\n",
      "        }\n",
      "#        anchor_generator_stride: {\n",
      "#          sizes: [0.36, 0.36, 0.8] # wlh\n",
      "#          strides: [0.14, 0.14, 0.0] # if generate only 1 z_center, z_stride will be ignored\n",
      "#          offsets: [0.07, -20.09, -1.465] # origin_offset + strides / 2\n",
      "#          rotations: [0, 1.57] # 0, pi/2\n",
      "#        }\n",
      "        matched_threshold : 0.4\n",
      "        unmatched_threshold : 0.3\n",
      "        use_rotate_nms: false\n",
      "        use_multi_class_nms: false\n",
      "        nms_pre_max_size: 1000\n",
      "        nms_post_max_size: 300\n",
      "        nms_score_threshold: 0.02\n",
      "        nms_iou_threshold: 0.3\n",
      "        region_similarity_calculator: {\n",
      "          distance_similarity: {\n",
      "            distance_norm: 1.414 # match range\n",
      "            with_rotation: false\n",
      "            rotation_alpha: 0.0 # rot error contribution\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "\n",
      "#      class_settings: {\n",
      "#        class_name: \"pedestrian\"\n",
      "#        anchor_generator_range: {\n",
      "#          sizes: [0.66344886, 0.7256437, 1.75748069] # wlh\n",
      "#          anchor_ranges: [-30, -20, -2.6, 30, 20, 2.2]\n",
      "#          rotations: [0, 1.57] # DON'T modify this unless you are very familiar with my code.\n",
      "#          # custom_values: [0, 0] # velocity vector base value\n",
      "#        }\n",
      "#        matched_threshold : 0.5\n",
      "#        unmatched_threshold : 0.35\n",
      "#        use_rotate_nms: false\n",
      "#        use_multi_class_nms: false\n",
      "#        nms_pre_max_size: 1000\n",
      "#        nms_post_max_size: 300\n",
      "#        nms_score_threshold: 0.02\n",
      "#        nms_iou_threshold: 0.3\n",
      "#        region_similarity_calculator: {\n",
      "#          distance_similarity: {\n",
      "#            distance_norm: 1.414 # match range\n",
      "#            with_rotation: false\n",
      "#            rotation_alpha: 0.0 # rot error contribution\n",
      "#          }\n",
      "#        }\n",
      "#      }\n",
      "\n",
      "#      class_settings: {\n",
      "#        class_name: \"animal\"\n",
      "#        anchor_generator_range: {\n",
      "#          sizes: [0.36, 0.73, 0.51] # wlh\n",
      "#          anchor_ranges: [-30, -20, -2.6, 30, 20, 2.2]\n",
      "#          rotations: [0, 1.57] # DON'T modify this unless you are very familiar with my code.\n",
      "#          # custom_values: [0, 0] # velocity vector base value\n",
      "#        }\n",
      "#        matched_threshold : 0.5\n",
      "#        unmatched_threshold : 0.35\n",
      "#        use_rotate_nms: false\n",
      "#        use_multi_class_nms: false\n",
      "#        nms_pre_max_size: 1000\n",
      "#        nms_post_max_size: 300\n",
      "#        nms_score_threshold: 0.02\n",
      "#        nms_iou_threshold: 0.3\n",
      "#        region_similarity_calculator: {\n",
      "#          distance_similarity: {\n",
      "#            distance_norm: 1.414 # match range\n",
      "#            with_rotation: false\n",
      "#            rotation_alpha: 0.0 # rot error contribution\n",
      "#          }\n",
      "#        }\n",
      "#      }\n",
      "\n",
      "\n",
      "      sample_positive_fraction : -1\n",
      "      sample_size : 512\n",
      "      assign_per_class: true\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "train_input_reader: {\n",
      "  dataset: {\n",
      "    dataset_class_name: \"CustomNuscDatasetMMS\"\n",
      "    kitti_info_path: \"/media/starlet/LdTho/data/sets/nuscenes/v1.0-mini/infos_train.pkl\"\n",
      "    kitti_root_path: \"/media/starlet/LdTho/data/sets/nuscenes/v1.0-mini\"\n",
      "  }\n",
      "  batch_size: 2\n",
      "  preprocess: {\n",
      "    num_workers: 2\n",
      "    shuffle_points: false\n",
      "    max_number_of_voxels: 70000\n",
      "\n",
      "    groundtruth_localization_noise_std: [0.0, 0.0, 0.0]\n",
      "    groundtruth_rotation_uniform_noise: [-0.0, 0.0]\n",
      "    global_rotation_uniform_noise: [-0.3925, 0.3925]\n",
      "    global_scaling_uniform_noise: [0.95, 1.05]\n",
      "    global_random_rotation_range_per_object: [0, 0] # pi/4 ~ 3pi/4\n",
      "    global_translate_noise_std: [0.2, 0.2, 0.2]\n",
      "    anchor_area_threshold: -1 # very slow if enable when using FHD map (1600x1200x40).\n",
      "    remove_points_after_sample: true\n",
      "    groundtruth_points_drop_percentage: 0.0\n",
      "    groundtruth_drop_max_keep_points: 15\n",
      "    remove_unknown_examples: false\n",
      "    sample_importance: 1.0\n",
      "    random_flip_x: true\n",
      "    random_flip_y: true\n",
      "    remove_environment: false #false\n",
      "    database_sampler {\n",
      "#      rate: 1.0\n",
      "#      database_info_path: \"/media/starlet/LdTho/data/sets/nuscenes/v1.0-mini/nusc_custom_dbinfos_train.pkl\"\n",
      "#      sample_groups {\n",
      "#        name_to_max_num {\n",
      "#          key: \"traffic_cone\"\n",
      "#          value: 30\n",
      "#        }\n",
      "#      }\n",
      "#      database_prep_steps {\n",
      "#        filter_by_min_num_points {\n",
      "#          min_num_point_pairs {\n",
      "#            key: \"traffic_cone\"\n",
      "#            value: 2\n",
      "#          }\n",
      "#        }\n",
      "#      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "train_config: {\n",
      "  optimizer: {\n",
      "    adam_optimizer: {\n",
      "      learning_rate: {\n",
      "        one_cycle: {\n",
      "          lr_max: 1e-3\n",
      "          moms: [0.95, 0.85]\n",
      "          div_factor: 10.0\n",
      "          pct_start: 0.4\n",
      "        }\n",
      "      }\n",
      "      weight_decay: 0.01\n",
      "    }\n",
      "    fixed_weight_decay: true\n",
      "    use_moving_average: false\n",
      "  }\n",
      "#  optimizer: {\n",
      "#    adam_optimizer: {\n",
      "#      learning_rate: {\n",
      "#        exponential_decay: {\n",
      "#          initial_learning_rate: 0.0002\n",
      "#          decay_length: 0.1\n",
      "#          decay_factor: 0.8\n",
      "#          staircase: True\n",
      "#        }\n",
      "#      }\n",
      "#      weight_decay: 0.0001\n",
      "#    }\n",
      "#    fixed_weight_decay: false\n",
      "#    use_moving_average: false\n",
      "#  }\n",
      "  steps: 35000 # 120000 * 4 * 2 / 22680 ~ 40 epoch\n",
      "  steps_per_eval: 1000 #500 # 1238 * 5\n",
      "  save_checkpoints_secs : 1800 # half hour\n",
      "  save_summary_steps : 10\n",
      "  enable_mixed_precision: true\n",
      "  loss_scale_factor: -1\n",
      "  clear_metrics_every_epoch: true\n",
      "}\n",
      "\n",
      "eval_input_reader: {\n",
      "  dataset: {\n",
      "    dataset_class_name: \"CustomNuscEvalDataset\"\n",
      "    kitti_info_path: \" \"\n",
      "    kitti_root_path: \"/media/starlet/LdTho/data/sets/nuscenes/v1.0-mini\"\n",
      "  }\n",
      "  batch_size: 2\n",
      "  preprocess: {\n",
      "    max_number_of_voxels: 120000\n",
      "    shuffle_points: true\n",
      "    num_workers: 2\n",
      "    anchor_area_threshold: -1\n",
      "    remove_environment: false #false\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_logging = SimpleModelLog(model_dir)\n",
    "model_logging.open()\n",
    "model_logging.log_text(proto_str + \"\\n\", 0, tag=\"config\")\n",
    "\n",
    "start_step = net.get_global_step()\n",
    "total_step = train_cfg.steps\n",
    "t = time.time()\n",
    "steps_per_eval = train_cfg.steps_per_eval\n",
    "clear_metrics_every_epoch = train_cfg.clear_metrics_every_epoch\n",
    "\n",
    "amp_optimizer.zero_grad()\n",
    "step_times = []\n",
    "step = start_step\n",
    "run = True\n",
    "ave_valid_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "rotary-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-cheese",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "scientific-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler.step(net.get_global_step())\n",
    "example.pop(\"metrics\")\n",
    "example_torch = example_convert_to_torch(example, float_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "compliant-brighton",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'voxels': tensor([[[ -0.8984,  -3.1348,  -0.1470,  -0.4490],\n",
       "          [ -0.8101,  -3.1230,  -0.1470,  -0.4490],\n",
       "          [ -0.8281,  -3.1289,  -0.1483,  -0.4529],\n",
       "          ...,\n",
       "          [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,   0.0000]],\n",
       " \n",
       "         [[ -0.8779,  -3.3066,  -0.1440,  -0.4529],\n",
       "          [ -0.8970,  -3.3086,  -0.1445,  -0.4529],\n",
       "          [ -0.8662,  -3.3066,  -0.1414,  -0.4568],\n",
       "          ...,\n",
       "          [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,   0.0000]],\n",
       " \n",
       "         [[ -0.8569,  -3.4863,  -0.1375,  -0.4412],\n",
       "          [ -0.8765,  -3.4922,  -0.1398,  -0.4412],\n",
       "          [ -0.8975,  -3.4941,  -0.1403,  -0.4412],\n",
       "          ...,\n",
       "          [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,   0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[  2.9336,  -9.3438,   1.0117,  -0.1863],\n",
       "          [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "          ...,\n",
       "          [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,   0.0000]],\n",
       " \n",
       "         [[  3.1094, -10.2344,   0.9126,  -0.1863],\n",
       "          [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "          ...,\n",
       "          [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,   0.0000]],\n",
       " \n",
       "         [[  2.6934,  -9.5469,   0.9907,  -0.1666],\n",
       "          [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "          ...,\n",
       "          [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,   0.0000]]], device='cuda:0',\n",
       "        dtype=torch.float16),\n",
       " 'num_points': tensor([17, 17, 18,  ...,  1,  1,  1], device='cuda:0', dtype=torch.int32),\n",
       " 'coordinates': tensor([[  0,   0, 168, 291],\n",
       "         [  0,   0, 166, 291],\n",
       "         [  0,   0, 165, 291],\n",
       "         ...,\n",
       "         [  1,   0, 106, 329],\n",
       "         [  1,   0,  97, 331],\n",
       "         [  1,   0, 104, 326]], device='cuda:0', dtype=torch.int32),\n",
       " 'num_voxels': tensor([[31719],\n",
       "         [29945]]),\n",
       " 'anchors': tensor([[[-30.0000, -20.0000,  -2.5996,  ...,   0.4036,   1.0625,   0.0000],\n",
       "          [-29.7969, -20.0000,  -2.5996,  ...,   0.4036,   1.0625,   0.0000],\n",
       "          [-29.5938, -20.0000,  -2.5996,  ...,   0.4036,   1.0625,   0.0000],\n",
       "          ...,\n",
       "          [ 29.5938,  20.0000,  -2.5996,  ...,   0.4036,   1.0625,   1.5703],\n",
       "          [ 29.7969,  20.0000,  -2.5996,  ...,   0.4036,   1.0625,   1.5703],\n",
       "          [ 30.0000,  20.0000,  -2.5996,  ...,   0.4036,   1.0625,   1.5703]],\n",
       " \n",
       "         [[-30.0000, -20.0000,  -2.5996,  ...,   0.4036,   1.0625,   0.0000],\n",
       "          [-29.7969, -20.0000,  -2.5996,  ...,   0.4036,   1.0625,   0.0000],\n",
       "          [-29.5938, -20.0000,  -2.5996,  ...,   0.4036,   1.0625,   0.0000],\n",
       "          ...,\n",
       "          [ 29.5938,  20.0000,  -2.5996,  ...,   0.4036,   1.0625,   1.5703],\n",
       "          [ 29.7969,  20.0000,  -2.5996,  ...,   0.4036,   1.0625,   1.5703],\n",
       "          [ 30.0000,  20.0000,  -2.5996,  ...,   0.4036,   1.0625,   1.5703]]],\n",
       "        device='cuda:0', dtype=torch.float16),\n",
       " 'gt_names': array(['traffic_cone', 'traffic_cone', 'traffic_cone', 'traffic_cone',\n",
       "        'traffic_cone', 'traffic_cone', 'traffic_cone', 'traffic_cone',\n",
       "        'traffic_cone', 'traffic_cone', 'traffic_cone', 'traffic_cone',\n",
       "        'traffic_cone'], dtype='<U12'),\n",
       " 'labels': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0', dtype=torch.int32),\n",
       " 'reg_targets': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16),\n",
       " 'importance': tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16),\n",
       " 'metadata': [{'token': '9813c23a5f1448b09bb7910fea9baf20'},\n",
       "  {'token': 'c235638ed66145988d17f9d0601923f2'}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "banner-fossil",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 : voxels\n",
    "# 1 : num_points\n",
    "# 2 : coordinates\n",
    "# 3 : num_voxels\n",
    "# 4 : anchors\n",
    "# 5 : gt_names\n",
    "# 6 : labels\n",
    "# 7 : reg_targets\n",
    "# 8 : importance\n",
    "# 9 : metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "democratic-incident",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -0.8984,  -3.1348,  -0.1470,  -0.4490],\n",
       "         [ -0.8101,  -3.1230,  -0.1470,  -0.4490],\n",
       "         [ -0.8281,  -3.1289,  -0.1483,  -0.4529],\n",
       "         ...,\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000]],\n",
       "\n",
       "        [[ -0.8779,  -3.3066,  -0.1440,  -0.4529],\n",
       "         [ -0.8970,  -3.3086,  -0.1445,  -0.4529],\n",
       "         [ -0.8662,  -3.3066,  -0.1414,  -0.4568],\n",
       "         ...,\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000]],\n",
       "\n",
       "        [[ -0.8569,  -3.4863,  -0.1375,  -0.4412],\n",
       "         [ -0.8765,  -3.4922,  -0.1398,  -0.4412],\n",
       "         [ -0.8975,  -3.4941,  -0.1403,  -0.4412],\n",
       "         ...,\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  2.9336,  -9.3438,   1.0117,  -0.1863],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         ...,\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000]],\n",
       "\n",
       "        [[  3.1094, -10.2344,   0.9126,  -0.1863],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         ...,\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000]],\n",
       "\n",
       "        [[  2.6934,  -9.5469,   0.9907,  -0.1666],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         ...,\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000]]], device='cuda:0',\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_torch['voxels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "directed-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traning input from example\n",
    "batch_size = example_torch['anchors'].shape[0]\n",
    "example_tuple = list(example_torch.values())\n",
    "pillar_x = example_tuple[0][:,:,0].unsqueeze(0).unsqueeze(0)\n",
    "pillar_y = example_tuple[0][:,:,1].unsqueeze(0).unsqueeze(0)\n",
    "pillar_z = example_tuple[0][:,:,2].unsqueeze(0).unsqueeze(0)\n",
    "pillar_i = example_tuple[0][:,:,3].unsqueeze(0).unsqueeze(0)\n",
    "num_points_per_pillar = example_tuple[1].float().unsqueeze(0)\n",
    "coors_x = example_tuple[2][:, 3].float()\n",
    "coors_y = example_tuple[2][:, 2].float()\n",
    "vx, vy = voxel_generator.voxel_size[0], voxel_generator.voxel_size[1]\n",
    "x_offset = vx/2 + voxel_generator.point_cloud_range[0]\n",
    "y_offset = vy/2 + voxel_generator.point_cloud_range[1]\n",
    "x_sub = coors_x.unsqueeze(1)*vx + x_offset\n",
    "y_sub = coors_y.unsqueeze(1)*vy + y_offset\n",
    "ones = torch.ones([1,voxel_generator._max_num_points], dtype = torch.float32, device = pillar_x.device)\n",
    "x_sub_shaped = torch.mm(x_sub, ones)\n",
    "y_sub_shaped = torch.mm(y_sub, ones)\n",
    "num_points_for_a_pillar = pillar_x.size()[3]\n",
    "mask = get_paddings_indicator(num_points_per_pillar, num_points_for_a_pillar, axis=0)\n",
    "mask = mask.permute(0,2,1)\n",
    "mask = mask.unsqueeze(1)\n",
    "mask = mask.type_as(pillar_x)\n",
    "coors = example_tuple[2]\n",
    "anchors = example_tuple[4]\n",
    "labels = example_tuple[6]\n",
    "reg_targets = example_tuple[7]\n",
    "input_example = [pillar_x, pillar_y, pillar_z, pillar_i, num_points_per_pillar,\n",
    "                 x_sub_shaped, y_sub_shaped, mask, coors, anchors, labels, reg_targets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-canyon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-language",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-circle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "refined-seafood",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.batch_size 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(46.9408, device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[3.1858e-04],\n",
       "          [1.6206e-04],\n",
       "          [3.1351e-04],\n",
       "          ...,\n",
       "          [1.7156e-04],\n",
       "          [3.1240e-04],\n",
       "          [7.4337e-05]],\n",
       " \n",
       "         [[2.4903e-04],\n",
       "          [1.1983e-04],\n",
       "          [1.7042e-04],\n",
       "          ...,\n",
       "          [1.1608e-04],\n",
       "          [2.1138e-04],\n",
       "          [5.0299e-05]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       " tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " tensor(35.6954, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " tensor([[[[ 0.2023, -0.1907],\n",
       "           [ 0.1919, -0.3977],\n",
       "           [-0.0126, -0.2805],\n",
       "           ...,\n",
       "           [ 0.1382, -0.4050],\n",
       "           [ 0.0255, -0.3113],\n",
       "           [ 0.0682, -0.2454]],\n",
       " \n",
       "          [[ 0.0235, -0.1427],\n",
       "           [ 0.0707, -0.5205],\n",
       "           [ 0.5454, -0.1544],\n",
       "           ...,\n",
       "           [ 0.1924, -0.4280],\n",
       "           [ 0.3550, -0.3665],\n",
       "           [-0.0323, -0.6074]],\n",
       " \n",
       "          [[ 0.0935, -0.1646],\n",
       "           [-0.0478, -0.1333],\n",
       "           [ 0.2388,  0.0992],\n",
       "           ...,\n",
       "           [ 0.0623, -0.0840],\n",
       "           [ 0.2742, -0.0961],\n",
       "           [ 0.1677, -0.6084]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.4148, -0.0098],\n",
       "           [ 0.1606, -0.5459],\n",
       "           [ 0.3542, -0.2556],\n",
       "           ...,\n",
       "           [ 0.2590, -0.4524],\n",
       "           [ 0.3386, -0.3938],\n",
       "           [-0.0365, -0.4456]],\n",
       " \n",
       "          [[ 0.2812, -0.3120],\n",
       "           [ 0.0272, -0.1434],\n",
       "           [ 0.3218,  0.1943],\n",
       "           ...,\n",
       "           [ 0.2471, -0.2593],\n",
       "           [ 0.3911, -0.1418],\n",
       "           [ 0.1194, -0.2334]],\n",
       " \n",
       "          [[ 0.1161, -0.4495],\n",
       "           [ 0.4646, -0.2422],\n",
       "           [ 0.0599, -0.0405],\n",
       "           ...,\n",
       "           [ 0.2559, -0.5972],\n",
       "           [ 0.1954, -0.1604],\n",
       "           [ 0.1893, -0.5869]]],\n",
       " \n",
       " \n",
       "         [[[ 0.2942, -0.1433],\n",
       "           [ 0.0580, -0.3372],\n",
       "           [ 0.0887, -0.3354],\n",
       "           ...,\n",
       "           [ 0.1382, -0.4050],\n",
       "           [ 0.0255, -0.3113],\n",
       "           [ 0.0682, -0.2454]],\n",
       " \n",
       "          [[ 0.0260, -0.1064],\n",
       "           [-0.0049, -0.5679],\n",
       "           [ 0.4143, -0.1707],\n",
       "           ...,\n",
       "           [ 0.1924, -0.4280],\n",
       "           [ 0.3550, -0.3665],\n",
       "           [-0.0323, -0.6074]],\n",
       " \n",
       "          [[ 0.0506, -0.1553],\n",
       "           [-0.0263, -0.0274],\n",
       "           [ 0.2454,  0.1215],\n",
       "           ...,\n",
       "           [ 0.0623, -0.0840],\n",
       "           [ 0.2742, -0.0961],\n",
       "           [ 0.1677, -0.6084]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0139, -0.3071],\n",
       "           [ 0.1190, -0.5850],\n",
       "           [ 0.3679, -0.4333],\n",
       "           ...,\n",
       "           [ 0.2590, -0.4524],\n",
       "           [ 0.3386, -0.3938],\n",
       "           [-0.0365, -0.4456]],\n",
       " \n",
       "          [[ 0.1356, -0.4573],\n",
       "           [-0.0362, -0.3401],\n",
       "           [ 0.2202, -0.0847],\n",
       "           ...,\n",
       "           [ 0.2471, -0.2593],\n",
       "           [ 0.3911, -0.1418],\n",
       "           [ 0.1194, -0.2334]],\n",
       " \n",
       "          [[-0.0077, -0.5176],\n",
       "           [ 0.2314, -0.5317],\n",
       "           [ 0.1572, -0.1382],\n",
       "           ...,\n",
       "           [ 0.2559, -0.5972],\n",
       "           [ 0.1954, -0.1604],\n",
       "           [ 0.1893, -0.5869]]]], device='cuda:0', dtype=torch.float16,\n",
       "        grad_fn=<CopyBackwards>),\n",
       " tensor(35.7480, device='cuda:0', grad_fn=<MulBackward0>),\n",
       " tensor(11.1928, device='cuda:0', grad_fn=<MulBackward0>),\n",
       " tensor([[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]], device='cuda:0'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output_tuple = (loss, cls_loss, loc_loss, cls_pos_loss, cls_neg_loss,\n",
    "#                 cls_preds, dir_loss, cls_loss_reduced, loc_loss_reduced, cared)\n",
    "ret_dict = net(input_example)\n",
    "ret_dict\n",
    "\n",
    "# below is ret_dict but the real one is a list.\n",
    "# ret_dict = {\n",
    "#     \"box_preds\": box_preds,\n",
    "#     \"cls_preds\": cls_preds,\n",
    "#     \"dir_cls_preds\": dir_cls_preds\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "administrative-amendment",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels_cpu = labels.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "rubber-footage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_cpu[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "acting-chocolate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(labels_cpu[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "beneficial-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_preds = ret_dict[5]\n",
    "loss = ret_dict[0].mean()\n",
    "cls_loss_reduced = ret_dict[6].mean()\n",
    "loc_loss_reduced = ret_dict[7].mean()\n",
    "cls_pos_loss = ret_dict[3]\n",
    "cls_neg_loss = ret_dict[4]\n",
    "loc_loss = ret_dict[2]\n",
    "cls_loss = ret_dict[1]\n",
    "cared = ret_dict[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "through-perspective",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "# loss.backward()\n",
    "# torch.nn.utils.clip_grad_norm_(net.parameters(), 10.0)\n",
    "# mixed_optimizer.step()\n",
    "# mixed_optimizer.zero_grad()\n",
    "# net.update_global_step()\n",
    "# net_metrics = net.update_metrics(cls_loss_reduced,\n",
    "#                                  loc_loss_reduced, cls_preds,\n",
    "#                                  labels, cared)\n",
    "# num_pos = int((labels > 0)[0].float().sum().cpu().numpy())\n",
    "# num_neg = int((labels == 0)[0].float().sum().cpu().numpy())\n",
    "# # if 'anchors_mask' not in example_torch:\n",
    "# #     num_anchors = example_torch['anchors'].shape[1]\n",
    "# # else:\n",
    "# #     num_anchors = int(example_torch['anchors_mask'][0].sum())\n",
    "# num_anchors = int(example_tuple[7][0].sum())\n",
    "# global_step = net.get_global_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "closing-interpretation",
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_optimizer.step()\n",
    "amp_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-flush",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "rental-lounge",
   "metadata": {},
   "source": [
    "## Test predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-chapel",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net(input_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-defense",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_assigner.matched_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-visit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_convert_to_torch(example, dtype=torch.float32,\n",
    "                             device=None) -> dict:\n",
    "    device = device or torch.device(\"cuda:0\")\n",
    "    example_torch = {}\n",
    "    float_names = [\n",
    "        \"voxels\", \"anchors\", \"reg_targets\", \"reg_weights\", \"bev_map\", \"importance\"\n",
    "    ]\n",
    "    for k, v in example.items():\n",
    "        if k in ['gt_names', 'gt_classes', 'gt_boxes', 'points']:\n",
    "            example_torch[k] = example[k]\n",
    "            continue\n",
    "\n",
    "        if k in float_names:\n",
    "            # slow when directly provide fp32 data with dtype=torch.half\n",
    "            example_torch[k] = torch.tensor(\n",
    "                v, dtype=torch.float32, device=device).to(dtype)\n",
    "        elif k in [\"coordinates\", \"labels\", \"num_points\"]:\n",
    "            example_torch[k] = torch.tensor(\n",
    "                v, dtype=torch.int32, device=device)\n",
    "        elif k in [\"anchors_mask\"]:\n",
    "            example_torch[k] = torch.tensor(\n",
    "                v, dtype=torch.uint8, device=device)\n",
    "        elif k == \"calib\":\n",
    "            calib = {}\n",
    "            for k1, v1 in v.items():\n",
    "                calib[k1] = torch.tensor(\n",
    "                    v1, dtype=dtype, device=device).to(dtype)\n",
    "            example_torch[k] = calib\n",
    "        elif k == \"num_voxels\":\n",
    "            example_torch[k] = torch.tensor(v)\n",
    "        else:\n",
    "            example_torch[k] = v\n",
    "    return example_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "pillar_x = np.ones([1, 1, 70000, 100], dtype=np.float32)\n",
    "pillar_y = np.ones([1, 1, 70000, 100], dtype=np.float32)\n",
    "pillar_z = np.ones([1, 1, 70000, 100], dtype=np.float32)\n",
    "pillar_i = np.ones([1, 1, 70000, 100], dtype=np.float32)\n",
    "num_points_per_pillar = np.ones([1, 70000], dtype=np.float32)\n",
    "x_sub_shaped = np.ones([1, 1, 70000, 100], dtype=np.float32)\n",
    "y_sub_shaped = np.ones([1, 1, 70000, 100], dtype=np.float32)\n",
    "mask = np.ones([1, 1, 70000, 100], dtype=np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-accessory",
   "metadata": {},
   "outputs": [],
   "source": [
    "pillar_x = torch.tensor(pillar_x, dtype = torch.float32, device = torch.device('cuda'))\n",
    "pillar_y = torch.tensor(pillar_y, dtype = torch.float32, device = torch.device('cuda'))\n",
    "pillar_z = torch.tensor(pillar_z, dtype = torch.float32, device = torch.device('cuda'))\n",
    "pillar_i = torch.tensor(pillar_i, dtype = torch.float32, device = torch.device('cuda'))\n",
    "num_points_per_pillar = torch.tensor(num_points_per_pillar, dtype = torch.float32, device = torch.device('cuda'))\n",
    "x_sub_shaped = torch.tensor(x_sub_shaped, dtype = torch.float32, device = torch.device('cuda'))\n",
    "y_sub_shaped = torch.tensor(y_sub_shaped, dtype = torch.float32, device = torch.device('cuda'))\n",
    "mask = torch.tensor(mask, dtype = torch.float32, device = torch.device('cuda'))\n",
    "pfe_inputs = [pillar_x, pillar_y, pillar_z, pillar_i, num_points_per_pillar,\n",
    "                  x_sub_shaped, y_sub_shaped, mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-reporter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net(pfe_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-sydney",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-chess",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-contamination",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "electric-papua",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-registrar",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_dir = model_dir / cur_time\n",
    "model_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-burlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-affairs",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model_logging = SimpleModelLog(model_dir)\n",
    "model_logging.open()\n",
    "model_logging.log_text(proto_str + \"\\n\", 0, tag=\"config\")\n",
    "\n",
    "start_step = net.get_global_step()\n",
    "total_step = train_cfg.steps\n",
    "t = time.time()\n",
    "steps_per_eval = train_cfg.steps_per_eval\n",
    "clear_metrics_every_epoch = train_cfg.clear_metrics_every_epoch\n",
    "\n",
    "amp_optimizer.zero_grad()\n",
    "step_times = []\n",
    "step = start_step\n",
    "run = True\n",
    "ave_valid_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-bhutan",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "self._batch_size# Ver2\n",
    "try:\n",
    "    start_tic = time.time()\n",
    "    print(\"num samples: %d\" % (len(dataset)))\n",
    "    while run == True:\n",
    "        if clear_metrics_every_epoch:\n",
    "            net.clear_metrics()\n",
    "        for example in tqdm_notebook(dataloader):\n",
    "            lr_scheduler.step(net.get_global_step())\n",
    "            example.pop(\"metrics\")\n",
    "            example_torch = example_convert_to_torch(example, float_dtype)\n",
    "            # traning input from example\n",
    "            batch_size = example_torch['anchors'].shape[0]\n",
    "            example_tuple = list(example_torch.values())\n",
    "            pillar_x = example_tuple[0][:,:,0].unsqueeze(0).unsqueeze(0)\n",
    "            pillar_y = example_tuple[0][:,:,1].unsqueeze(0).unsqueeze(0)\n",
    "            pillar_z = example_tuple[0][:,:,2].unsqueeze(0).unsqueeze(0)\n",
    "            pillar_i = example_tuple[0][:,:,3].unsqueeze(0).unsqueeze(0)\n",
    "            num_points_per_pillar = example_tuple[1].float().unsqueeze(0)\n",
    "            coors_x = example_tuple[2][:, 3].float()\n",
    "            coors_y = example_tuple[2][:, 2].float()\n",
    "            vx, vy = voxel_generator.voxel_size[0], voxel_generator.voxel_size[1]\n",
    "            x_offset = vx/2 + voxel_generator.point_cloud_range[0]\n",
    "            y_offset = vy/2 + voxel_generator.point_cloud_range[1]\n",
    "            x_sub = coors_x.unsqueeze(1)*vx + x_offset\n",
    "            y_sub = coors_y.unsqueeze(1)*vy + y_offset\n",
    "            ones = torch.ones([1,voxel_generator._max_num_points], dtype = torch.float32, device = pillar_x.device)\n",
    "            x_sub_shaped = torch.mm(x_sub, ones)\n",
    "            y_sub_shaped = torch.mm(y_sub, ones)\n",
    "            num_points_for_a_pillar = pillar_x.size()[3]\n",
    "            mask = get_paddings_indicator(num_points_per_pillar, num_points_for_a_pillar, axis=0)\n",
    "            mask = mask.permute(0,2,1)\n",
    "            mask = mask.unsqueeze(1)\n",
    "            mask = mask.type_as(pillar_x)\n",
    "            coors = example_tuple[2]\n",
    "            anchors = example_tuple[4]\n",
    "            labels = example_tuple[6]\n",
    "            reg_targets = example_tuple[7]\n",
    "            input_example = [pillar_x, pillar_y, pillar_z, pillar_i, num_points_per_pillar,\n",
    "                             x_sub_shaped, y_sub_shaped, mask, coors, anchors, labels, reg_targets]\n",
    "\n",
    "            ret_dict = net_parallel(input_example)\n",
    "            #ret_dict = (0 loss, 1 cls_loss, 2 loc_loss, 3 cls_pos_loss, 4 cls_neg_loss,\n",
    "            #            5 cls_preds, 6 cls_loss_reduced, 7 loc_loss_reduced, 8 cared)\n",
    "            cls_preds = ret_dict[5]\n",
    "            loss = ret_dict[0].mean()\n",
    "            cls_loss_reduced = ret_dict[6].mean()\n",
    "            loc_loss_reduced = ret_dict[7].mean()\n",
    "            cls_pos_loss = ret_dict[3]\n",
    "            cls_neg_loss = ret_dict[4]\n",
    "            loc_loss = ret_dict[2]\n",
    "            cls_loss = ret_dict[1]\n",
    "            cared = ret_dict[8]\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), 10.0)\n",
    "            amp_optimizer.step()\n",
    "            amp_optimizer.zero_grad()\n",
    "            net.update_global_step()\n",
    "            net_metrics = net.update_metrics(cls_loss_reduced,\n",
    "                                            loc_loss_reduced, \n",
    "                                            cls_preds,\n",
    "                                            labels, cared)\n",
    "            step_time = (time.time() - t)\n",
    "            t = time.time()\n",
    "            metrics = {}\n",
    "            num_pos = int((labels > 0)[0].float().sum().cpu().numpy())\n",
    "            num_neg = int((labels == 0))\n",
    "            num_anchors = int(example_tuple[7][0].sum())\n",
    "            global_step = net.get_global_step()\n",
    "            if global_step % display_step ==0:\n",
    "                loc_loss_elem = [\n",
    "                    float(loc_loss[:,:,i].sum().detach().cpu().numpy()/batch_size) for i \n",
    "                    in range (loc_loss.shape[-1])]\n",
    "                metrics[\"step\"] = global_step\n",
    "                metrics[\"steptime\"] = step_time\n",
    "                metrics.update(net_metrics)\n",
    "                metrics['loss'] = {}\n",
    "                metrics[\"loss\"][\"loc_elem\"]\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            #####Continued###\n",
    "            loss = ret_dict[\"loss\"].mean()\n",
    "            cls_loss_reduced = ret_dict[\"cls_loss_reduced\"].mean()\n",
    "            loc_loss_reduced = ret_dict[\"loc_loss_reduced\"].mean()\n",
    "\n",
    "            if train_cfg.enable_mixed_precision:\n",
    "                if net.get_global_step() < 100:\n",
    "                    loss *= 1e-3\n",
    "                with amp.scale_loss(loss, amp_optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), 10.0)\n",
    "            amp_optimizer.step()\n",
    "            amp_optimizer.zero_grad()\n",
    "            net.update_global_step()\n",
    "\n",
    "            cls_preds = ret_dict[\"cls_preds\"]\n",
    "            labels = example_torch[\"labels\"]\n",
    "            cared = ret_dict[\"cared\"]\n",
    "\n",
    "            net_metrics = net.update_metrics(cls_loss_reduced,\n",
    "                                             loc_loss_reduced, cls_preds,\n",
    "                                             labels, cared)\n",
    "            step_time = (time.time() - t)\n",
    "            step_times.append(step_time)\n",
    "            t = time.time()\n",
    "            metrics = {}\n",
    "            global_step = net.get_global_step()\n",
    "\n",
    "            if global_step % display_step == 0:\n",
    "                net.eval()\n",
    "                det = net(example_torch)\n",
    "                print(det[0]['label_preds'])\n",
    "                print(det[0]['scores'])\n",
    "                net.train()\n",
    "                eta = time.time() - start_tic\n",
    "                if measure_time:\n",
    "                    for name, val in net.get_avg_time_dict().items():\n",
    "                        print(f\"avg {name} time = {val * 1000:.3f} ms\")\n",
    "\n",
    "                metrics[\"step\"] = global_step\n",
    "                metrics['epoch'] = global_step / len(dataloader)\n",
    "                metrics['steptime'] = np.mean(step_times)\n",
    "                metrics['valid'] = ave_valid_loss\n",
    "                step_times = []\n",
    "\n",
    "                metrics[\"loss\"] = net_metrics['loss']['cls_loss'] + net_metrics['loss']['loc_loss']\n",
    "                metrics[\"cls_loss\"] = net_metrics['loss']['cls_loss']\n",
    "                metrics[\"loc_loss\"] = net_metrics['loss']['loc_loss']\n",
    "\n",
    "                if model_cfg.use_direction_classifier:\n",
    "                    dir_loss_reduced = ret_dict[\"dir_loss_reduced\"].mean()\n",
    "                    metrics[\"dir_rt\"] = float(\n",
    "                        dir_loss_reduced.detach().cpu().numpy())\n",
    "\n",
    "                metrics['lr'] = float(amp_optimizer.lr)\n",
    "                metrics['eta'] = time_to_str(eta)\n",
    "                model_logging.log_metrics(metrics, global_step)\n",
    "\n",
    "                net.clear_metrics()\n",
    "            if global_step % steps_per_eval == 0:\n",
    "                torchplus.train.save_models(model_dir, [net, amp_optimizer],\n",
    "                                            net.get_global_step())\n",
    "                model_logging.log_text(f\"Model saved: {model_dir}, {net.get_global_step()}\", global_step)\n",
    "                net.eval()\n",
    "                result_path_step = result_path / f\"step_{net.get_global_step()}\"\n",
    "                result_path_step.mkdir(parents=True, exist_ok=True)\n",
    "                model_logging.log_text(\"########################\", global_step)\n",
    "                model_logging.log_text(\" EVALUATE\", global_step)\n",
    "                model_logging.log_text(\"########################\", global_step)\n",
    "                model_logging.log_text(\"Generating eval predictions...\", global_step)\n",
    "                t = time.time()\n",
    "                detections = []\n",
    "                prog_bar = ProgressBar()\n",
    "                net.clear_timer()\n",
    "                cnt = 0\n",
    "                for example in tqdm_notebook(iter(eval_dataloader)):\n",
    "                    example = example_convert_to_torch(example, float_dtype)\n",
    "                    detections += net(example)\n",
    "                sec_per_ex = len(eval_dataset) / (time.time() - t)\n",
    "                model_logging.log_text(\n",
    "                    f'generate eval predictions finished({sec_per_ex:.2f}/s). Start eval:',\n",
    "                    global_step)\n",
    "                result_dict = eval_dataset.dataset.evaluation(\n",
    "                    detections, str(result_path_step)\n",
    "                )\n",
    "                for k, v in result_dict['results'].items():\n",
    "                    model_logging.log_text(f\"Evaluation {k}\", global_step)\n",
    "                    model_logging.log_text(v, global_step)\n",
    "                model_logging.log_metrics(result_dict[\"detail\"], global_step)\n",
    "                with open(result_path_step/\"result.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(detections,f)\n",
    "                net.train()\n",
    "\n",
    "            step += 1\n",
    "            if step >= total_step:\n",
    "                break\n",
    "        if step >= total_step:\n",
    "            break\n",
    "except Exception as e:\n",
    "    model_logging.log_text(str(e), step)\n",
    "    model_logging.log_text(json.dumps(example['metadata'], indent=2), step)\n",
    "    torchplus.train.save_models(model_dir, [net, amp_optimizer], step)\n",
    "    raise e\n",
    "finally:\n",
    "    model_logging.close()\n",
    "torchplus.train.save_models(model_dir, [net, amp_optimizer], step)\n",
    "# python /kaggle/code/ConeDetectionPointpillars/second/data/nusc_eval.py --root_path=\"/media/starlet/LdTho/data/sets/nuscenes/v1.0-trainval\" --version=\"v1.0-trainval\" --eval_version=detection_cvpr_2019 --res_path=\"/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/results/step_20/result_nusc.json\" --eval_set=val --output_dir=\"/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/results/step_20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-mambo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver 1\n",
    "# try:\n",
    "#     start_tic = time.time()\n",
    "#     print(\"num samples: %d\" % (len(dataset)))\n",
    "#     while run == True:\n",
    "#         if clear_metrics_every_epoch:\n",
    "#             net.clear_metrics()\n",
    "#         for example in tqdm_notebook(dataloader):\n",
    "#             lr_scheduler.step(net.get_global_step())\n",
    "#             example.pop(\"metrics\")\n",
    "#             example_torch = example_convert_to_torch(example, float_dtype)\n",
    "\n",
    "#             ret_dict = net_parallel(example_torch)\n",
    "#             loss = ret_dict[\"loss\"].mean()\n",
    "#             cls_loss_reduced = ret_dict[\"cls_loss_reduced\"].mean()\n",
    "#             loc_loss_reduced = ret_dict[\"loc_loss_reduced\"].mean()\n",
    "\n",
    "#             if train_cfg.enable_mixed_precision:\n",
    "#                 if net.get_global_step() < 100:\n",
    "#                     loss *= 1e-3\n",
    "#                 with amp.scale_loss(loss, amp_optimizer) as scaled_loss:\n",
    "#                     scaled_loss.backward()\n",
    "#             else:\n",
    "#                 loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(net.parameters(), 10.0)\n",
    "#             amp_optimizer.step()\n",
    "#             amp_optimizer.zero_grad()\n",
    "#             net.update_global_step()\n",
    "\n",
    "#             cls_preds = ret_dict[\"cls_preds\"]\n",
    "#             labels = example_torch[\"labels\"]\n",
    "#             cared = ret_dict[\"cared\"]\n",
    "\n",
    "#             net_metrics = net.update_metrics(cls_loss_reduced,\n",
    "#                                              loc_loss_reduced, cls_preds,\n",
    "#                                              labels, cared)\n",
    "#             step_time = (time.time() - t)\n",
    "#             step_times.append(step_time)\n",
    "#             t = time.time()\n",
    "#             metrics = {}\n",
    "#             global_step = net.get_global_step()\n",
    "\n",
    "#             if global_step % display_step == 0:\n",
    "#                 net.eval()\n",
    "#                 det = net(example_torch)\n",
    "#                 print(det[0]['label_preds'])\n",
    "#                 print(det[0]['scores'])      \n",
    "#                 net.train()\n",
    "#                 eta = time.time() - start_tic\n",
    "#                 if measure_time:\n",
    "#                     for name, val in net.get_avg_time_dict().items():\n",
    "#                         print(f\"avg {name} time = {val * 1000:.3f} ms\")\n",
    "\n",
    "#                 metrics[\"step\"] = global_step\n",
    "#                 metrics['epoch'] = global_step / len(dataloader)\n",
    "#                 metrics['steptime'] = np.mean(step_times)\n",
    "#                 metrics['valid'] = ave_valid_loss\n",
    "#                 step_times = []\n",
    "\n",
    "#                 metrics[\"loss\"] = net_metrics['loss']['cls_loss'] + net_metrics['loss']['loc_loss']\n",
    "#                 metrics[\"cls_loss\"] = net_metrics['loss']['cls_loss']\n",
    "#                 metrics[\"loc_loss\"] = net_metrics['loss']['loc_loss']\n",
    "\n",
    "#                 if model_cfg.use_direction_classifier:\n",
    "#                     dir_loss_reduced = ret_dict[\"dir_loss_reduced\"].mean()\n",
    "#                     metrics[\"dir_rt\"] = float(\n",
    "#                         dir_loss_reduced.detach().cpu().numpy())\n",
    "\n",
    "#                 metrics['lr'] = float(amp_optimizer.lr)\n",
    "#                 metrics['eta'] = time_to_str(eta)\n",
    "#                 model_logging.log_metrics(metrics, global_step)\n",
    "\n",
    "#                 net.clear_metrics()\n",
    "#             if global_step % steps_per_eval == 0:\n",
    "#                 torchplus.train.save_models(model_dir, [net, amp_optimizer],\n",
    "#                                             net.get_global_step())\n",
    "#                 model_logging.log_text(f\"Model saved: {model_dir}, {net.get_global_step()}\", global_step)\n",
    "#                 net.eval()\n",
    "#                 result_path_step = result_path / f\"step_{net.get_global_step()}\"\n",
    "#                 result_path_step.mkdir(parents=True, exist_ok=True)\n",
    "#                 model_logging.log_text(\"########################\", global_step)\n",
    "#                 model_logging.log_text(\" EVALUATE\",global_step)\n",
    "#                 model_logging.log_text(\"########################\", global_step)\n",
    "#                 model_logging.log_text(\"Generating eval images...\", global_step)\n",
    "#                 t = time.time()\n",
    "#                 detections = []\n",
    "#                 prog_bar = ProgressBar()\n",
    "#                 net.clear_timer()\n",
    "#                 cnt = 0\n",
    "#                 for example in iter(dataloader):\n",
    "#                     cnt += 1\n",
    "#                     detection = example_convert_to_torch(example, float_dtype)\n",
    "#                     detection = net(detection)\n",
    "# #                     filtered_sample_tokens = eval_dataset.dataset.filtered_sample_tokens\n",
    "#                     filtered_sample_tokens = dataset.dataset.filtered_sample_tokens\n",
    "#                     index = filtered_sample_tokens.index(detection[0]['metadata']['token'])\n",
    "#                     det_boxes = detection[0]['box3d_lidar'].cpu().detach().numpy()\n",
    "#                     det_labels = detection[0]['label_preds'].cpu().detach().numpy()\n",
    "#                     det_scores = detection[0]['scores'].cpu().detach().numpy()\n",
    "                    \n",
    "#                     det_boxes_cones = det_boxes[det_labels == 0]\n",
    "#                     det_scores_cones = det_scores[det_labels == 0]\n",
    "#                     det_labels_cones = det_labels[det_labels == 0]\n",
    "                    \n",
    "#                     det_boxes_pedes = det_boxes[det_labels == 1]\n",
    "#                     det_scores_pedes = det_scores[det_labels == 1]\n",
    "#                     det_labels_pedes = det_labels[det_labels == 1]\n",
    "# #                     gt_example = eval_dataset.dataset.get_sensor_data(index)\n",
    "#                     gt_example = dataset.dataset.get_sensor_data(index)\n",
    "#                     points = gt_example['lidar']['points']\n",
    "        \n",
    "#                     gt_boxes = gt_example['lidar']['annotations']['boxes']\n",
    "#                     gt_labels = gt_example['lidar']['annotations']['names']\n",
    "#                     gt_boxes_cones = gt_boxes[gt_labels == \"traffic_cone\"]\n",
    "#                     gt_labels_cones = gt_labels[gt_labels == \"traffic_cone\"]\n",
    "\n",
    "#                     gt_boxes_pedes = gt_boxes[gt_labels == \"pedestrian\"]\n",
    "#                     gt_labels_pedes = gt_labels[gt_labels == \"pedestrian\"]\n",
    "\n",
    "#                     gt_scores = np.ones(len(gt_labels_cones))\n",
    "#                     c = points[:, 3].reshape(-1, 1)\n",
    "#                     c = np.concatenate([c, c, c], axis=1)\n",
    "#                     points = points[:, 0:3]\n",
    "#                     pc = o3d.geometry.PointCloud()\n",
    "#                     pc.points = o3d.utility.Vector3dVector(points)\n",
    "#                     pc.colors = o3d.utility.Vector3dVector(c)\n",
    "#                     mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=2.0,\n",
    "#                                                                                    origin=[-0, -0, -0])\n",
    "#                     geo = [pc, mesh_frame]\n",
    "#                     rbbox_corners_cones = box_np_ops.center_to_corner_box3d(det_boxes_cones[:, :3],\n",
    "#                                                                       det_boxes_cones[:, 3:6],\n",
    "#                                                                       det_boxes_cones[:, 6],\n",
    "#                                                                       origin=(0.5, 0.5, 0.5), axis=2)\n",
    "#                     rbbox_corners_pedes = box_np_ops.center_to_corner_box3d(det_boxes_pedes[:, :3],\n",
    "#                                                                       det_boxes_pedes[:, 3:6],\n",
    "#                                                                       det_boxes_pedes[:, 6],\n",
    "#                                                                       origin=(0.5, 0.5, 0.5), axis=2)\n",
    "#                     gt_cones_rbbox_corners = box_np_ops.center_to_corner_box3d(gt_boxes_cones[:, :3],\n",
    "#                                                                          gt_boxes_cones[:, 3:6],\n",
    "#                                                                          gt_boxes_cones[:, 6],\n",
    "#                                                                          origin=(0.5, 0.5, 0.5), axis=2)\n",
    "#                     gt_pedes_rbbox_corners = box_np_ops.center_to_corner_box3d(gt_boxes_pedes[:, :3],\n",
    "#                                                                          gt_boxes_pedes[:, 3:6],\n",
    "#                                                                          gt_boxes_pedes[:, 6],\n",
    "#                                                                          origin=(0.5, 0.5, 0.5), axis=2)\n",
    "#                     for i in range(len(rbbox_corners_cones)):\n",
    "#                         geo.append(buildBBox(rbbox_corners_cones[i], color=[1, 0, 0]))\n",
    "#                     for i in range(len(rbbox_corners_pedes)):\n",
    "#                         geo.append(buildBBox(rbbox_corners_pedes[i], color=[1, 1, 0]))\n",
    "#                     for i in range(len(gt_cones_rbbox_corners)):\n",
    "#                         geo.append(buildBBox(gt_cones_rbbox_corners[i], color=[0, 1, 0]))\n",
    "#                     for i in range(len(gt_pedes_rbbox_corners)):\n",
    "#                         geo.append(buildBBox(gt_pedes_rbbox_corners[i], color=[0, 0, 1]))\n",
    "# #                     o3d.visualization.draw_geometries(geo)\n",
    "#                     vis = o3d.visualization.Visualizer()\n",
    "#                     vis.create_window(visible=True)\n",
    "#                     for i in range(len(geo)):\n",
    "#                         vis.add_geometry(geo[i])\n",
    "#                         vis.update_geometry(geo[i])\n",
    "#                     vis.poll_events()\n",
    "#                     vis.update_renderer()\n",
    "#                     img_dir = str(Path(f'{model_dir}/images/{global_step}').resolve())\n",
    "#                     img_dir = Path(img_dir)\n",
    "#                     img_dir.mkdir(parents=True, exist_ok=True)\n",
    "#                     vis.capture_screen_image(f'{model_dir}/images/{global_step}/eval_{cnt}.png')\n",
    "#                     model_logging.log_text(f\"eval images saved at {model_dir}/images/{global_step}/\", global_step)\n",
    "#                     vis.destroy_window()\n",
    "#                     if cnt >= 5:\n",
    "#                         break\n",
    "#                 if 1 in detection[0]['label_preds']:\n",
    "#                     print(detection[0]['label_preds'])\n",
    "#                     print(detection[0]['scores'])\n",
    "#                 sec_per_ex = len(eval_dataset) / (time.time() - t)\n",
    "#                 model_logging.log_text(\n",
    "#                     f'generate eval images finished({sec_per_ex:.2f}/s). continue training:',\n",
    "#                     global_step)\n",
    "#                 net.train()\n",
    "                \n",
    "#             step += 1\n",
    "#             if step >= total_step:\n",
    "#                 break\n",
    "#         if step >= total_step:\n",
    "#             break\n",
    "# except Exception as e:\n",
    "#     model_logging.log_text(str(e), step)\n",
    "#     model_logging.log_text(json.dumps(example['metadata'], indent=2), step)\n",
    "#     torchplus.train.save_models(model_dir, [net, amp_optimizer], step)\n",
    "#     raise e\n",
    "# finally:\n",
    "#     model_logging.close()\n",
    "# torchplus.train.save_models(model_dir, [net, amp_optimizer], step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for example in tqdm(eval_dataloader):\n",
    "    example = example_convert_to_torch(example, float_dtype)\n",
    "    det = net(example)\n",
    "#     if len(det['label_preds']) > 0:\n",
    "    print(det)\n",
    "    detections += det\n",
    "    c += 1\n",
    "    if c == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-companion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-branch",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randint(1,100,(1,))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset.dataset.evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-leadership",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/media/starlet/LdTho/data/sets/nuscenes/v1.0-trainval/nusc_custom_dbinfos_train.pkl', 'rb') as f:\n",
    "    cus_info_file = pickle.load(f)\n",
    "with open('/media/starlet/LdTho/data/sets/nuscenes/v1.0-trainval/infos_train.pkl', 'rb') as f:\n",
    "    info_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "cus_info_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-diamond",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-blocking",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[1]['num_points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-rally",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-research",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "modular-clock",
   "metadata": {},
   "source": [
    "## Visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-absorption",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-desert",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_labels in [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(iter(dataloader))\n",
    "det = net(example_convert_to_torch(example, float_dtype))\n",
    "filtered_sample_tokens = dataset.dataset.filtered_sample_tokens\n",
    "index = filtered_sample_tokens.index(det[0]['metadata']['token'])\n",
    "det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-amber",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_boxes = det[0]['box3d_lidar'].cpu().detach().numpy()\n",
    "det_labels = det[0]['label_preds'].cpu().detach().numpy()\n",
    "det_scores = det[0]['scores'].cpu().detach().numpy()\n",
    "token = det[0]['metadata']\n",
    "example = dataset.dataset.get_sensor_data(index)\n",
    "points = example['lidar']['points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = example['lidar']['annotations']['boxes']\n",
    "labels = example['lidar']['annotations']['names']\n",
    "scores = np.ones(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-expansion",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = det_boxes\n",
    "pc = o3d.geometry.PointCloud()\n",
    "c = points[:, 3].reshape(-1, 1)\n",
    "c = np.concatenate([c, c, c], axis=1)\n",
    "pc.points = o3d.utility.Vector3dVector(points)\n",
    "pc.colors = o3d.utility.Vector3dVector(c)\n",
    "mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size = 2.0, origin = [-0,-0,-0])\n",
    "geo = [pc, mesh_frame]\n",
    "rbbox_corners = box_np_ops.center_to_corner_box3d(boxes[:,:3],\n",
    "                                                 boxes[:,3:6],\n",
    "                                                 boxes[:,6],\n",
    "                                                 origin=(0.5,0.5,0.5), axis = 2)\n",
    "\n",
    "for i in range(boxes.shape[0]):\n",
    "    geo.append(buildBBox(rbbox_corners[i], color = [1,0,0]))\n",
    "print(geo)\n",
    "o3d.visualization.draw_geometries(geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-radio",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "# example = next(iter(dataloader))\n",
    "example = next(iter(eval_dataloader))\n",
    "\n",
    "detection = example_convert_to_torch(example, float_dtype)\n",
    "detection = net(detection)\n",
    "print(detection)\n",
    "filtered_sample_tokens = eval_dataset.dataset.filtered_sample_tokens\n",
    "# filtered_sample_tokens = dataset.dataset.filtered_sample_tokens\n",
    "index = filtered_sample_tokens.index(detection[0]['metadata']['token'])\n",
    "det_boxes = detection[0]['box3d_lidar'].cpu().detach().numpy()\n",
    "det_labels = detection[0]['label_preds'].cpu().detach().numpy()\n",
    "det_scores = detection[0]['scores'].cpu().detach().numpy()\n",
    "\n",
    "det_boxes_cones = det_boxes[det_labels == 0]\n",
    "det_scores_cones = det_scores[det_labels == 0]\n",
    "det_labels_cones = det_labels[det_labels == 0]\n",
    "\n",
    "det_boxes_pedes = det_boxes[det_labels == 1]\n",
    "det_scores_pedes = det_scores[det_labels == 1]\n",
    "det_labels_pedes = det_labels[det_labels == 1]\n",
    "gt_example = eval_dataset.dataset.get_sensor_data(index)\n",
    "# gt_example = dataset.dataset.get_sensor_data(index)\n",
    "points = gt_example['lidar']['points']\n",
    "pc_range = model_cfg.voxel_generator.point_cloud_range\n",
    "points = np.array([p for p in points if (pc_range[0] < p[0] < pc_range[3]) & (pc_range[1] < p[1] < pc_range[4]) & (pc_range[2] < p[2] < pc_range[5])])\n",
    "\n",
    "gt_boxes = gt_example['lidar']['annotations']['boxes']\n",
    "gt_labels = gt_example['lidar']['annotations']['names']\n",
    "gt_boxes_cones = gt_boxes[gt_labels == \"traffic_cone\"]\n",
    "gt_labels_cones = gt_labels[gt_labels == \"traffic_cone\"]\n",
    "\n",
    "gt_boxes_pedes = gt_boxes[gt_labels == \"pedestrian\"]\n",
    "gt_labels_pedes = gt_labels[gt_labels == \"pedestrian\"]\n",
    "\n",
    "gt_scores = np.ones(len(gt_labels_cones))\n",
    "c = points[:, 3].reshape(-1, 1)\n",
    "c = np.concatenate([c, c, c], axis=1)\n",
    "points = points[:, 0:3]\n",
    "\n",
    "pc = o3d.geometry.PointCloud()\n",
    "pc.points = o3d.utility.Vector3dVector(points)\n",
    "pc.colors = o3d.utility.Vector3dVector(c)\n",
    "mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=2.0,\n",
    "                                                               origin=[-0, -0, -0])\n",
    "geo = [pc, mesh_frame]\n",
    "rbbox_corners_cones = box_np_ops.center_to_corner_box3d(det_boxes_cones[:, :3],\n",
    "                                                  det_boxes_cones[:, 3:6],\n",
    "                                                  det_boxes_cones[:, 6],\n",
    "                                                  origin=(0.5, 0.5, 0.5), axis=2)\n",
    "rbbox_corners_pedes = box_np_ops.center_to_corner_box3d(det_boxes_pedes[:, :3],\n",
    "                                                  det_boxes_pedes[:, 3:6],\n",
    "                                                  det_boxes_pedes[:, 6],\n",
    "                                                  origin=(0.5, 0.5, 0.5), axis=2)\n",
    "gt_cones_rbbox_corners = box_np_ops.center_to_corner_box3d(gt_boxes_cones[:, :3],\n",
    "                                                     gt_boxes_cones[:, 3:6],\n",
    "                                                     gt_boxes_cones[:, 6],\n",
    "                                                     origin=(0.5, 0.5, 0.5), axis=2)\n",
    "gt_pedes_rbbox_corners = box_np_ops.center_to_corner_box3d(gt_boxes_pedes[:, :3],\n",
    "                                                     gt_boxes_pedes[:, 3:6],\n",
    "                                                     gt_boxes_pedes[:, 6],\n",
    "                                                     origin=(0.5, 0.5, 0.5), axis=2)\n",
    "for i in range(len(rbbox_corners_cones)):\n",
    "    geo.append(buildBBox(rbbox_corners_cones[i], color=[1, 0, 0]))\n",
    "for i in range(len(rbbox_corners_pedes)):\n",
    "    geo.append(buildBBox(rbbox_corners_pedes[i], color=[1, 0, 1]))\n",
    "for i in range(len(gt_cones_rbbox_corners)):\n",
    "    geo.append(buildBBox(gt_cones_rbbox_corners[i], color=[0, 1, 0]))\n",
    "for i in range(len(gt_pedes_rbbox_corners)):\n",
    "    geo.append(buildBBox(gt_pedes_rbbox_corners[i], color=[0, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries(geo)\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-india",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_range = model_cfg.voxel_generator.point_cloud_range\n",
    "pc_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = [p for p in points if (pc_range[0] < p[0] < pc_range[3]) & (pc_range[1] < p[1] < pc_range[4]) & (pc_range[2] < p[2] < pc_range[5])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-least",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-supervisor",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _second_det_to_nusc_box(detection):\n",
    "    from nuscenes.utils.data_classes import Box\n",
    "    import pyquaternion\n",
    "    box3d = detection[\"box3d_lidar\"].detach().cpu().numpy()\n",
    "    scores = detection[\"scores\"].detach().cpu().numpy()\n",
    "    labels = detection[\"label_preds\"].detach().cpu().numpy()\n",
    "    print(labels)\n",
    "    box3d[:, 6] = -box3d[:, 6] - np.pi / 2\n",
    "    box_list = []\n",
    "    for i in range(box3d.shape[0]):\n",
    "        quat = pyquaternion.Quaternion(axis=[0, 0, 1], radians=box3d[i, 6])\n",
    "        velocity = (np.nan, np.nan, np.nan)\n",
    "        if box3d.shape[1] == 9:\n",
    "            velocity = (*box3d[i, 7:9], 0.0)\n",
    "            # velo_val = np.linalg.norm(box3d[i, 7:9])\n",
    "            # velo_ori = box3d[i, 6]\n",
    "            # velocity = (velo_val * np.cos(velo_ori), velo_val * np.sin(velo_ori), 0.0)\n",
    "        box = Box(\n",
    "            box3d[i, :3],\n",
    "            box3d[i, 3:6],\n",
    "            quat,\n",
    "            label=labels[i],\n",
    "            score=scores[i],\n",
    "            velocity=velocity)\n",
    "        box_list.append(box)\n",
    "    return box_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-handling",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_list = _second_det_to_nusc_box(detection[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation_nusc\n",
    "import json\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-brief",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/eval_dev_test\"\n",
    "mapped_class_names = {0: \"traffic_cone\", 1: \"pedestrian\"}\n",
    "nusc_annos = {}\n",
    "for det in detection:\n",
    "    annos = []\n",
    "    boxes = _second_det_to_nusc_box(det)\n",
    "    for i, box in enumerate(boxes):\n",
    "        name = mapped_class_names[box.label]\n",
    "        velocity = np.nan\n",
    "        nusc_anno = {\n",
    "            \"sample_token\": det[\"metadata\"][\"token\"],\n",
    "            \"translation\": box.center.tolist(),\n",
    "            \"size\": box.wlh.tolist(),\n",
    "            \"rotation\": box.orientation.elements.tolist(),\n",
    "            \"velocity\": velocity,\n",
    "            \"detection_name\":name,\n",
    "            \"detection_score\": box.score,\n",
    "            \"attribute_name\":DefaultAttribute [name]\n",
    "        }\n",
    "        annos.append(nusc_anno)\n",
    "    nusc_annos[det[\"metadata\"][\"token\"]] = annos\n",
    "nusc_submissions = {\n",
    "    \"meta\": {\n",
    "        \"use_camera\": False,\n",
    "        \"use_lidar\": False,\n",
    "        \"use_radar\": False,\n",
    "        \"use_map\": False,\n",
    "        \"use_external\": False\n",
    "    },\n",
    "    \"result\": nusc_annos\n",
    "}\n",
    "res_path = Path(output_dir)/\"result_nusc.json\"\n",
    "with open(res_path,\"wb\") as f:\n",
    "    json.dump(nusc_submissions,f)\n",
    "eval_main_file = Path(__file__).resolve().parent / \"nusc_eval.py\"\n",
    "cmd = f\"python {str(eval_main_file)} --root_path=/media/starlet/LdTho/data/sets/nuscenes/v1.0-trainval\"\n",
    "cmd += f\" --version=trainval --eval_version=cvpr_2019\"\n",
    "cmd += f\" --res_path=\\\"{str(res_path)}\\\" --eval_set=val\"\n",
    "cmd += f\" --output_dir=\\\"{output_dir}\\\"\"\n",
    "subprocess.check_output(cmd, shell = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "nusc_submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-hostel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-break",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-opinion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-implementation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-settle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-reliance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-hybrid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-while",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-audience",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-tongue",
   "metadata": {},
   "outputs": [],
   "source": [
    "NameMappingInverse = {\n",
    "    \"barrier\": \"movable_object.barrier\",\n",
    "    \"bicycle\": \"vehicle.bicycle\",\n",
    "    \"bus\": \"vehicle.bus.rigid\",\n",
    "    \"car\": \"vehicle.car\",\n",
    "    \"construction_vehicle\": \"vehicle.construction\",\n",
    "    \"motorcycle\": \"vehicle.motorcycle\",\n",
    "    \"pedestrian\": \"human.pedestrian.adult\",\n",
    "    \"traffic_cone\": \"movable_object.trafficcone\",\n",
    "    \"trailer\": \"vehicle.trailer\",\n",
    "    \"truck\": \"vehicle.truck\",\n",
    "}\n",
    "def visualize_evaluation(config_path, model_dir, pretrained_path, multi_gpu=False):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if isinstance(config_path, str):\n",
    "        config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "        with open(config_path, \"r\") as f:\n",
    "            proto_str = f.read()\n",
    "            text_format.Merge(proto_str, config)\n",
    "    else:\n",
    "        config = config_path\n",
    "        proto_str = text_format.MessageToString(config, indent=2)\n",
    "\n",
    "\n",
    "    # Read config file\n",
    "    input_cfg = config.train_input_reader\n",
    "    eval_input_cfg = config.eval_input_reader\n",
    "    model_cfg = config.model.second  # model's config\n",
    "    train_cfg = config.train_config\n",
    "\n",
    "    # Build neural network\n",
    "    net = build_network(model_cfg).to(device)\n",
    "\n",
    "    # Build Model\n",
    "    target_assigner = net.target_assigner\n",
    "    voxel_generator = net.voxel_generator\n",
    "    print(\"num parameter: \", len(list(net.parameters())))\n",
    "    torchplus.train.try_restore_latest_checkpoints(model_dir, [net])\n",
    "\n",
    "    if pretrained_path is not None:\n",
    "        print('warning pretrain is loaded after restore, careful with resume')\n",
    "        model_dict = net.state_dict()\n",
    "        pretrained_dict = torch.load(pretrained_path)\n",
    "\n",
    "        new_pretrained_dict = {}\n",
    "        for k, v in pretrained_dict.items():\n",
    "            if k in model_dict and v.shape == model_dict[k].shape:\n",
    "                new_pretrained_dict[k] = v\n",
    "        print(\"Load pretrained parameters: \")\n",
    "        for k, v in new_pretrained_dict.items():\n",
    "            print(k, v.shape)\n",
    "        model_dict.update(new_pretrained_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "        net.clear_global_step()\n",
    "        net.clear_metrics()\n",
    "    if multi_gpu:\n",
    "        net_parallel = torch.nn.DataParallel(net)\n",
    "    else:\n",
    "        net_parallel = net\n",
    "\n",
    "    optimizer_cfg = train_cfg.optimizer\n",
    "    loss_scale = train_cfg.loss_scale_factor\n",
    "    fastai_optimizer = optimizer_builder.build(\n",
    "        optimizer_cfg,\n",
    "        net,\n",
    "        mixed=False,\n",
    "        loss_scale=loss_scale)\n",
    "    if loss_scale < 0:\n",
    "        loss_scale = \"dynamic\"\n",
    "    if train_cfg.enable_mixed_precision:\n",
    "        max_num_voxels = input_cfg.preprocess.max_number_of_voxels * input_cfg.batch_size\n",
    "        print(\"max_num_voxels: %d\" % (max_num_voxels))\n",
    "\n",
    "        net, amp_optimizer = amp.initialize(net, fastai_optimizer,\n",
    "                                            opt_level=\"O1\",\n",
    "                                            keep_batchnorm_fp32=None,\n",
    "                                            loss_scale=loss_scale)\n",
    "        net.metrics_to_float()\n",
    "    else:\n",
    "        amp_optimizer = fastai_optimizer\n",
    "    torchplus.train.try_restore_latest_checkpoints(model_dir, [fastai_optimizer])\n",
    "    lr_scheduler = lr_scheduler_builder.build(optimizer_cfg, amp_optimizer, train_cfg.steps)\n",
    "    if train_cfg.enable_mixed_precision:\n",
    "        float_dtype = torch.float16\n",
    "    else:\n",
    "        float_dtype = torch.float32\n",
    "\n",
    "    if multi_gpu:\n",
    "        num_gpu = torch.cuda.device_count()\n",
    "        print(f\"MULTI_GPU: use {num_gpu} gpus\")\n",
    "        collate_fn = merge_second_batch_multigpu\n",
    "    else:\n",
    "        collate_fn = merge_second_batch\n",
    "        num_gpu = 1\n",
    "\n",
    "    eval_dataset = input_reader_builder.build(\n",
    "        eval_input_cfg,\n",
    "        model_cfg,\n",
    "        training=False,\n",
    "        voxel_generator=voxel_generator,\n",
    "        target_assigner=target_assigner)\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=input_cfg.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=eval_input_cfg.preprocess.num_workers,\n",
    "        pin_memory=False,\n",
    "        collate_fn=merge_second_batch)\n",
    "\n",
    "    # Start visualizing\n",
    "    net.eval()\n",
    "    # example = next(iter(dataloader))\n",
    "    for example in iter(eval_dataloader):\n",
    "        detection = example_convert_to_torch(example, float_dtype)\n",
    "        detection = net(detection)\n",
    "        print(detection)\n",
    "        filtered_sample_tokens = eval_dataset.dataset.filtered_sample_tokens\n",
    "        # filtered_sample_tokens = dataset.dataset.filtered_sample_tokens\n",
    "        index = filtered_sample_tokens.index(detection[0]['metadata']['token'])\n",
    "\n",
    "        gt_example = eval_dataset.dataset.get_sensor_data(index)\n",
    "        # gt_example = dataset.dataset.get_sensor_data(index)\n",
    "        points = gt_example['lidar']['points']\n",
    "        pc_range = model_cfg.voxel_generator.point_cloud_range\n",
    "        points = np.array(\n",
    "            [p for p in points if (pc_range[0] < p[0] < pc_range[3]) & (pc_range[1] < p[1] < pc_range[4]) & (\n",
    "                    pc_range[2] < p[2] < pc_range[5])])\n",
    "\n",
    "        gt_boxes = gt_example['lidar']['annotations']['boxes']\n",
    "        gt_labels = gt_example['lidar']['annotations']['names']\n",
    "        c = points[:, 3].reshape(-1, 1)\n",
    "        c = np.concatenate([c, c, c], axis=1)\n",
    "        points = points[:, 0:3]\n",
    "        pc = o3d.geometry.PointCloud()\n",
    "        pc.points = o3d.utility.Vector3dVector(points)\n",
    "        pc.colors = o3d.utility.Vector3dVector(c)\n",
    "        mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=2.0,\n",
    "                                                                       origin=[-0, -0, -0])\n",
    "        geo = [pc, mesh_frame]\n",
    "        geo = add_prediction_per_class(eval_dataset.dataset.nusc,\n",
    "                                       detection, gt_boxes, gt_labels,\n",
    "                                       target_assigner.classes, geo)\n",
    "        o3d.visualization.draw_geometries(geo)\n",
    "\n",
    "    net.train()\n",
    "\n",
    "color = {\n",
    "    \"traffic_cone\": (1,0,0),\n",
    "    \"gt_traffic_cone\": (0,1,0),\n",
    "    \"pedestrian\": (1,1,0),\n",
    "    \"gt_pedestrian\": (0,0,1)\n",
    "}\n",
    "\n",
    "def add_prediction_per_class(nusc, detection, gt_boxes, gt_labels, class_names, geometries):\n",
    "    det_boxes = detection[0]['box3d_lidar'].cpu().detach().numpy()\n",
    "    det_labels = detection[0]['label_preds'].cpu().detach().numpy()\n",
    "    det_scores = detection[0]['scores'].cpu().detach().numpy()\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        mask = np.logical_and(det_labels == i, det_scores > 0.5)\n",
    "        class_det_boxes = det_boxes[mask]\n",
    "        class_det_scores = det_scores[mask]\n",
    "        class_det_labels = det_labels[mask]\n",
    "        print(len(class_det_boxes),len(class_det_scores),len(class_det_labels))\n",
    "        print(class_det_scores)\n",
    "        class_gt_boxes = gt_boxes[gt_labels == class_name]\n",
    "        class_gt_labels = gt_labels[gt_labels == class_name]\n",
    "\n",
    "        rbbox_corners = box_np_ops.center_to_corner_box3d(class_det_boxes[:, :3],\n",
    "                                                          class_det_boxes[:, 3:6],\n",
    "                                                          class_det_boxes[:, 6],\n",
    "                                                          origin=(0.5, 0.5, 0.5), axis=2)\n",
    "        gt_rbbox_corners = box_np_ops.center_to_corner_box3d(class_gt_boxes[:, :3],\n",
    "                                                             class_gt_boxes[:, 3:6],\n",
    "                                                             class_gt_boxes[:, 6],\n",
    "                                                             origin=(0.5, 0.5, 0.5), axis=2)\n",
    "        for j in range(len(rbbox_corners)):\n",
    "            geometries.append(buildBBox(rbbox_corners[j],\n",
    "                                        color=color[class_name]))\n",
    "        for j in range(len(gt_rbbox_corners)):\n",
    "            geometries.append(buildBBox(gt_rbbox_corners[j], \n",
    "                                        color=color[f'gt_{class_name}']))\n",
    "    return geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-elimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_evaluation('/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/cones_pp_v4/pipeline.config',\n",
    "                     '/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/cones_pp_v4/2021-2-11_9:58', \n",
    "                     '/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/cones_pp_v4/2021-2-11_9:58/voxelnet-7000.tckpt',\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_boxes = detection[0]['box3d_lidar'].cpu().detach().numpy()\n",
    "det_labels = detection[0]['label_preds'].cpu().detach().numpy()\n",
    "det_scores = detection[0]['scores'].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_labels == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_scores > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-registrar",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logical_and(det_scores > 0.5,det_labels == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bitwise_and(det_scores > 0.5,det_labels == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-spray",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_boxes[np.logical_and(det_scores > 0.5,det_labels == 0)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

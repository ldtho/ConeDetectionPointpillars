{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d \n",
    "from numba.core.errors import NumbaDeprecationWarning,NumbaPendingDeprecationWarning, NumbaWarning\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', category=NumbaDeprecationWarning)\n",
    "warnings.simplefilter('ignore', category=NumbaWarning)\n",
    "import sys\n",
    "sys.path.append('/kaggle/code/ConeDetectionPointpillars')\n",
    "\n",
    "from second.data.CustomNuscDataset import * #to register dataset\n",
    "from models import * #to register model\n",
    "import torch\n",
    "from second.utils.log_tool import SimpleModelLog\n",
    "from second.builder import target_assigner_builder, voxel_builder\n",
    "from second.protos import pipeline_pb2\n",
    "from second.pytorch.builder import (box_coder_builder, input_reader_builder,\n",
    "                                    lr_scheduler_builder, optimizer_builder,\n",
    "                                    second_builder)\n",
    "from second.pytorch.core import box_torch_ops\n",
    "import torchplus\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from google.protobuf import text_format\n",
    "# from lyft_dataset_sdk.utils.geometry_utils import *\n",
    "from lyft_dataset_sdk.lyftdataset import Quaternion\n",
    "# from lyft_dataset_sdk.utils.data_classes import Box\n",
    "from nuscenes.utils.geometry_utils import *\n",
    "from nuscenes.utils.data_classes import Box\n",
    "\n",
    "#commented second.core.non_max_suppression, nms_cpu, __init__.py, \n",
    "# pytorch.core.box_torch_ops line 524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "rotation_points_single_angle() missing 2 required positional arguments: 'points' and 'angle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d5384199e218>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbox_np_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbox_np_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotation_points_single_angle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: rotation_points_single_angle() missing 2 required positional arguments: 'points' and 'angle'"
     ]
    }
   ],
   "source": [
    "from second.core import box_np_ops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# t1 = torch.randn(1,2)\n",
    "# t2 = torch.randn(1,2).to(dev)\n",
    "# print(t1)  # tensor([[-0.2678,  1.9252]])\n",
    "# print(t2)  # tensor([[ 0.5117, -3.6247]], device='cuda:0')\n",
    "# t1.to(dev) \n",
    "# print(t1)  # tensor([[-0.2678,  1.9252]]) \n",
    "# print(t1.is_cuda) # False\n",
    "# t1 = t1.to(dev)\n",
    "# print(t1)  # tensor([[-0.2678,  1.9252]], device='cuda:0') \n",
    "# print(t1.is_cuda) # True\n",
    "\n",
    "# class M(nn.Module):\n",
    "#     def __init__(self):        \n",
    "#         super().__init__()        \n",
    "#         self.l1 = nn.Linear(1,2)\n",
    "\n",
    "#     def forward(self, x):                      \n",
    "#         x = self.l1(x)\n",
    "#         return x\n",
    "# model = M()   # not on cuda\n",
    "# model.to(dev) # is on cuda (all parameters)\n",
    "# print(next(model.parameters()).is_cuda) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = './configs/cones_pp_initial.config'\n",
    "from google.protobuf import text_format\n",
    "import sys\n",
    "sys.path.append('/kaggle/code/ConeDetectionPointpillars')\n",
    "from second.protos import pipeline_pb2\n",
    "\n",
    "config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "with open(config_path, \"r\") as f:\n",
    "    proto_str = text_format.Merge(f.read(), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model {\n",
      "  second {\n",
      "    network_class_name: \"VoxelNet\"\n",
      "    voxel_generator {\n",
      "      voxel_size: 0.10000000149011612\n",
      "      voxel_size: 0.10000000149011612\n",
      "      voxel_size: 4.0\n",
      "      point_cloud_range: -120.0\n",
      "      point_cloud_range: -60.0\n",
      "      point_cloud_range: -4.0\n",
      "      point_cloud_range: 80.0\n",
      "      point_cloud_range: 60.0\n",
      "      point_cloud_range: 5.0\n",
      "      max_number_of_points_per_voxel: 15\n",
      "    }\n",
      "    voxel_feature_extractor {\n",
      "      module_class_name: \"PillarFeatureNet\"\n",
      "      num_filters: 64\n",
      "      num_input_features: 5\n",
      "    }\n",
      "    middle_feature_extractor {\n",
      "      module_class_name: \"PointPillarsScatter\"\n",
      "      num_input_features: 64\n",
      "      downsample_factor: 1\n",
      "    }\n",
      "    rpn {\n",
      "      module_class_name: \"RPNV2\"\n",
      "      layer_nums: 3\n",
      "      layer_nums: 5\n",
      "      layer_nums: 5\n",
      "      layer_strides: 2\n",
      "      layer_strides: 2\n",
      "      layer_strides: 2\n",
      "      num_filters: 64\n",
      "      num_filters: 128\n",
      "      num_filters: 256\n",
      "      upsample_strides: 1.0\n",
      "      upsample_strides: 2.0\n",
      "      upsample_strides: 4.0\n",
      "      num_upsample_filters: 128\n",
      "      num_upsample_filters: 128\n",
      "      num_upsample_filters: 128\n",
      "      num_groups: 32\n",
      "      num_input_features: 64\n",
      "    }\n",
      "    num_point_features: 5\n",
      "    use_sigmoid_score: true\n",
      "    loss {\n",
      "      localization_loss {\n",
      "        weighted_smooth_l1 {\n",
      "          sigma: 3.0\n",
      "          code_weight: 1.0\n",
      "          code_weight: 1.0\n",
      "          code_weight: 1.0\n",
      "          code_weight: 1.0\n",
      "          code_weight: 1.0\n",
      "          code_weight: 1.0\n",
      "          code_weight: 1.0\n",
      "        }\n",
      "      }\n",
      "      classification_loss {\n",
      "        weighted_sigmoid_focal {\n",
      "          anchorwise_output: true\n",
      "          gamma: 2.0\n",
      "          alpha: 0.25\n",
      "        }\n",
      "      }\n",
      "      classification_weight: 1.0\n",
      "      localization_weight: 2.0\n",
      "    }\n",
      "    encode_rad_error_by_sin: true\n",
      "    encode_background_as_zeros: true\n",
      "    direction_loss_weight: 0.20000000298023224\n",
      "    pos_class_weight: 1.0\n",
      "    neg_class_weight: 1.0\n",
      "    loss_norm_type: NormByNumPositives\n",
      "    box_coder {\n",
      "      ground_box3d_coder {\n",
      "      }\n",
      "    }\n",
      "    target_assigner {\n",
      "      class_settings {\n",
      "        anchor_generator_range {\n",
      "          sizes: 0.36000001430511475\n",
      "          sizes: 0.36000001430511475\n",
      "          sizes: 0.5099999904632568\n",
      "          anchor_ranges: -100.0\n",
      "          anchor_ranges: -100.0\n",
      "          anchor_ranges: -5.0\n",
      "          anchor_ranges: 100.0\n",
      "          anchor_ranges: 100.0\n",
      "          anchor_ranges: 3.0\n",
      "          rotations: 0.0\n",
      "          rotations: 1.5700000524520874\n",
      "        }\n",
      "        region_similarity_calculator {\n",
      "          distance_similarity {\n",
      "            distance_norm: 1.4140000343322754\n",
      "          }\n",
      "        }\n",
      "        nms_pre_max_size: 1000\n",
      "        nms_post_max_size: 300\n",
      "        nms_score_threshold: 0.019999999552965164\n",
      "        nms_iou_threshold: 0.30000001192092896\n",
      "        matched_threshold: 0.5\n",
      "        unmatched_threshold: 0.3499999940395355\n",
      "        class_name: \"traffic_cone\"\n",
      "      }\n",
      "      sample_positive_fraction: -1.0\n",
      "      sample_size: 512\n",
      "      assign_per_class: true\n",
      "    }\n",
      "    post_center_limit_range: 0.0\n",
      "    post_center_limit_range: -40.0\n",
      "    post_center_limit_range: -2.200000047683716\n",
      "    post_center_limit_range: 70.4000015258789\n",
      "    post_center_limit_range: 40.0\n",
      "    post_center_limit_range: 0.800000011920929\n",
      "    sin_error_factor: 1.0\n",
      "    num_direction_bins: 2\n",
      "    direction_limit_offset: 1.0\n",
      "  }\n",
      "}\n",
      "train_input_reader {\n",
      "  batch_size: 1\n",
      "  dataset {\n",
      "    kitti_info_path: \" \"\n",
      "    kitti_root_path: \"/media/starlet/LdTho/data/sets/nuscenes/v1.0-trainval\"\n",
      "    dataset_class_name: \"CustomNuscDataset\"\n",
      "  }\n",
      "  preprocess {\n",
      "    shuffle_points: true\n",
      "    max_number_of_voxels: 70000\n",
      "    groundtruth_localization_noise_std: 0.0\n",
      "    groundtruth_localization_noise_std: 0.0\n",
      "    groundtruth_localization_noise_std: 0.0\n",
      "    groundtruth_rotation_uniform_noise: -0.0\n",
      "    groundtruth_rotation_uniform_noise: 0.0\n",
      "    global_rotation_uniform_noise: -0.39250001311302185\n",
      "    global_rotation_uniform_noise: 0.39250001311302185\n",
      "    global_scaling_uniform_noise: 0.949999988079071\n",
      "    global_scaling_uniform_noise: 1.0499999523162842\n",
      "    global_translate_noise_std: 0.20000000298023224\n",
      "    global_translate_noise_std: 0.20000000298023224\n",
      "    global_translate_noise_std: 0.20000000298023224\n",
      "    num_workers: 3\n",
      "    anchor_area_threshold: -1.0\n",
      "    remove_points_after_sample: true\n",
      "    groundtruth_drop_max_keep_points: 15\n",
      "    global_random_rotation_range_per_object: 0.0\n",
      "    global_random_rotation_range_per_object: 0.0\n",
      "    database_sampler {\n",
      "    }\n",
      "    random_flip_x: true\n",
      "    random_flip_y: true\n",
      "    sample_importance: 1.0\n",
      "  }\n",
      "}\n",
      "train_config {\n",
      "  optimizer {\n",
      "    adam_optimizer {\n",
      "      learning_rate {\n",
      "        one_cycle {\n",
      "          lr_max: 0.0010000000474974513\n",
      "          moms: 0.949999988079071\n",
      "          moms: 0.8500000238418579\n",
      "          div_factor: 10.0\n",
      "          pct_start: 0.4000000059604645\n",
      "        }\n",
      "      }\n",
      "      weight_decay: 0.009999999776482582\n",
      "    }\n",
      "    fixed_weight_decay: true\n",
      "  }\n",
      "  steps: 15000\n",
      "  steps_per_eval: 16000\n",
      "  save_checkpoints_secs: 1800\n",
      "  save_summary_steps: 10\n",
      "  enable_mixed_precision: true\n",
      "  loss_scale_factor: -1.0\n",
      "  clear_metrics_every_epoch: true\n",
      "}\n",
      "eval_input_reader {\n",
      "  batch_size: 1\n",
      "  dataset {\n",
      "    kitti_info_path: \" \"\n",
      "    kitti_root_path: \" \"\n",
      "    dataset_class_name: \"CustomNuscTestDataset\"\n",
      "  }\n",
      "  preprocess {\n",
      "    shuffle_points: true\n",
      "    max_number_of_voxels: 120000\n",
      "    num_workers: 3\n",
      "    anchor_area_threshold: -1.0\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(proto_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = './configs/cones_pp_initial.config'\n",
    "model_dir = f'./outputs/{time.time()}'\n",
    "create_folder = False\n",
    "measure_time = False\n",
    "resume = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## def run_train\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_dir = str(Path(model_dir).resolve())\n",
    "if create_folder:\n",
    "    if Path(model_dir).exists():\n",
    "        model_dir = torchplus.train.create_folder(model_dir)\n",
    "model_dir = Path(model_dir)\n",
    "\n",
    "\n",
    "# !sudo rm -R /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs\n",
    "\n",
    "\n",
    "if not resume and model_dir.exists():\n",
    "    raise ValueError(\"model dir exists and you don't specify resume\")\n",
    "\n",
    "model_dir.mkdir(parents=True, exist_ok= True)\n",
    "config_file_bkp = \"pipeline.config\"\n",
    "\n",
    "if isinstance(config_path, str):\n",
    "    config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "    with open(config_path, \"r\") as f:\n",
    "        proto_str = f.read()\n",
    "        text_format.Merge(proto_str, config)\n",
    "else:\n",
    "    config = config_path\n",
    "    proto_str = text_format.MessageToString(config, indent=2)\n",
    "\n",
    "with (model_dir/config_file_bkp).open('w') as f:\n",
    "    f.write(proto_str)\n",
    "# Read config file\n",
    "input_cfg = config.train_input_reader\n",
    "eval_input_cfg = config.eval_input_reader\n",
    "model_cfg = config.model.second # model's config\n",
    "train_cfg = config.train_config # training config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(model_cfg, measure_time=False):\n",
    "    voxel_generator = voxel_builder.build(model_cfg.voxel_generator)\n",
    "    bv_range = voxel_generator.point_cloud_range[[0, 1, 3, 4]]\n",
    "    box_coder = box_coder_builder.build(model_cfg.box_coder)\n",
    "    target_assigner_cfg = model_cfg.target_assigner\n",
    "    target_assigner = target_assigner_builder.build(target_assigner_cfg,\n",
    "                                                    bv_range, box_coder)\n",
    "    box_coder.custom_ndim = target_assigner._anchor_generators[0].custom_ndim\n",
    "    net = second_builder.build(\n",
    "        model_cfg, voxel_generator, target_assigner, measure_time=measure_time)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = build_network(model_cfg, measure_time).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VoxelNet(\n",
       "  (voxel_feature_extractor): PillarFeatureNet(\n",
       "    (pfn_layers): ModuleList(\n",
       "      (0): PFNLayer(\n",
       "        (linear): DefaultArgLayer(in_features=10, out_features=64, bias=False)\n",
       "        (norm): DefaultArgLayer(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (middle_feature_extractor): PointPillarsScatter()\n",
       "  (rpn): RPNV2(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        (1): DefaultArgLayer(64, 64, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (2): DefaultArgLayer(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "        (4): DefaultArgLayer(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (5): DefaultArgLayer(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (6): ReLU()\n",
       "        (7): DefaultArgLayer(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (8): DefaultArgLayer(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (9): ReLU()\n",
       "        (10): DefaultArgLayer(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (11): DefaultArgLayer(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (12): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        (1): DefaultArgLayer(64, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (2): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "        (4): DefaultArgLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (5): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (6): ReLU()\n",
       "        (7): DefaultArgLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (8): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (9): ReLU()\n",
       "        (10): DefaultArgLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (11): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (12): ReLU()\n",
       "        (13): DefaultArgLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (14): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (15): ReLU()\n",
       "        (16): DefaultArgLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (17): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (18): ReLU()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        (1): DefaultArgLayer(128, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (2): DefaultArgLayer(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "        (4): DefaultArgLayer(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (5): DefaultArgLayer(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (6): ReLU()\n",
       "        (7): DefaultArgLayer(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (8): DefaultArgLayer(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (9): ReLU()\n",
       "        (10): DefaultArgLayer(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (11): DefaultArgLayer(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (12): ReLU()\n",
       "        (13): DefaultArgLayer(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (14): DefaultArgLayer(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (15): ReLU()\n",
       "        (16): DefaultArgLayer(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (17): DefaultArgLayer(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (18): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (deblocks): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): DefaultArgLayer(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): DefaultArgLayer(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): DefaultArgLayer(256, 128, kernel_size=(4, 4), stride=(4, 4), bias=False)\n",
       "        (1): DefaultArgLayer(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (conv_cls): Conv2d(384, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv_box): Conv2d(384, 14, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (rpn_acc): Accuracy()\n",
       "  (rpn_precision): Precision()\n",
       "  (rpn_recall): Recall()\n",
       "  (rpn_metrics): PrecisionRecall()\n",
       "  (rpn_cls_loss): Scalar()\n",
       "  (rpn_loc_loss): Scalar()\n",
       "  (rpn_total_loss): Scalar()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_assigner = net.target_assigner\n",
    "voxel_generator = net.voxel_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num parameter:  64\n"
     ]
    }
   ],
   "source": [
    "print(\"num parameter: \", len(list(net.parameters())))\n",
    "torchplus.train.try_restore_latest_checkpoints(model_dir, [net])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_parallel = net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False _amp_stash\n"
     ]
    }
   ],
   "source": [
    "optimizer_cfg = train_cfg.optimizer\n",
    "loss_scale = train_cfg.loss_scale_factor\n",
    "fastai_optimizer = optimizer_builder.build(\n",
    "    optimizer_cfg,\n",
    "    net,\n",
    "    mixed=False,\n",
    "    loss_scale=loss_scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PillarFeatureNet(\n",
      "  (pfn_layers): ModuleList(\n",
      "    (0): PFNLayer(\n",
      "      (linear): DefaultArgLayer(in_features=10, out_features=64, bias=False)\n",
      "      (norm): DefaultArgLayer(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net.voxel_feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_num_voxels: 70000\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    }
   ],
   "source": [
    "if loss_scale < 0:\n",
    "    loss_scale = \"dynamic\"\n",
    "if train_cfg.enable_mixed_precision:\n",
    "    max_num_voxels = input_cfg.preprocess.max_number_of_voxels * input_cfg.batch_size\n",
    "    print(\"max_num_voxels: %d\" % (max_num_voxels))\n",
    "    from apex import amp\n",
    "    net, amp_optimizer = amp.initialize(net, fastai_optimizer, opt_level=\"O1\",\n",
    "                                        keep_batchnorm_fp32=None, loss_scale=loss_scale)\n",
    "    net.metrics_to_float()\n",
    "else:\n",
    "    amp_optimizer = fastai_optimizer\n",
    "torchplus.train.try_restore_latest_checkpoints(model_dir, [fastai_optimizer])\n",
    "lr_scheduler = lr_scheduler_builder.build(optimizer_cfg, amp_optimizer, train_cfg.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_second_batch(batch_list):\n",
    "    if isinstance(batch_list[0], list):\n",
    "        batch_list_c = []\n",
    "        for example in batch_list:\n",
    "            batch_list_c += example\n",
    "        batch_list = batch_list_c\n",
    "\n",
    "    example_merged = defaultdict(list)\n",
    "    for example in batch_list:\n",
    "        for k, v in example.items():\n",
    "            example_merged[k].append(v)\n",
    "    ret = {}\n",
    "    for key, elems in example_merged.items():\n",
    "        if key in [\n",
    "            'voxels', 'num_points', 'num_gt', 'voxel_labels', 'gt_names', 'gt_classes', 'gt_boxes'\n",
    "        ]:\n",
    "            ret[key] = np.concatenate(elems, axis=0)\n",
    "        elif key == 'metadata':\n",
    "            ret[key] = elems\n",
    "        elif key == \"calib\":\n",
    "            ret[key] = {}\n",
    "            for elem in elems:\n",
    "                for k1, v1 in elem.items():\n",
    "                    if k1 not in ret[key]:\n",
    "                        ret[key][k1] = [v1]\n",
    "                    else:\n",
    "                        ret[key][k1].append(v1)\n",
    "            for k1, v1 in ret[key].items():\n",
    "                ret[key][k1] = np.stack(v1, axis=0)\n",
    "        elif key == 'coordinates':\n",
    "            coors = []\n",
    "            for i, coor in enumerate(elems):\n",
    "                coor_pad = np.pad(\n",
    "                    coor, ((0, 0), (1, 0)), mode='constant', constant_values=i)\n",
    "                coors.append(coor_pad)\n",
    "            ret[key] = np.concatenate(coors, axis=0)\n",
    "        elif key == 'metrics':\n",
    "            ret[key] = elems\n",
    "        else:\n",
    "            ret[key] = np.stack(elems, axis=0)\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if train_cfg.enable_mixed_precision:\n",
    "    float_dtype = torch.float16\n",
    "else:\n",
    "    float_dtype = torch.float32\n",
    "multi_gpu = False\n",
    "if multi_gpu:\n",
    "    num_gpu = torch.cuda.device_count()\n",
    "    print(f\"MULTI_GPU: use {num_gpu} gpus\")\n",
    "    collate_fn = merge_second_batch_multigpu\n",
    "else:\n",
    "    collate_fn = merge_second_batch\n",
    "    num_gpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_map_size [1, 600, 1000]\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-trainval...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "64386 instance,\n",
      "12 sensor,\n",
      "10200 calibrated_sensor,\n",
      "2631083 ego_pose,\n",
      "68 log,\n",
      "850 scene,\n",
      "34149 sample,\n",
      "2631083 sample_data,\n",
      "1166187 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 22.726 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 5.3 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "dataset = input_reader_builder.build(\n",
    "    input_cfg,\n",
    "    model_cfg,\n",
    "    training=True,\n",
    "    voxel_generator=voxel_generator,\n",
    "    target_assigner=target_assigner,\n",
    "    multi_gpu=multi_gpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2021-01-20 22:47:38,190 - transforms - finding looplift candidates\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'voxels': array([[[  3.14525949,  -3.96943501,  -0.33711653,  -0.19019608,\n",
       "            0.        ],\n",
       "         [  3.15678115,  -3.93691871,  -0.33244276,  -0.20980392,\n",
       "            0.        ],\n",
       "         [  3.17456788,  -3.91114932,  -0.33074373,  -0.1745098 ,\n",
       "            0.        ],\n",
       "         ...,\n",
       "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "            0.        ],\n",
       "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "            0.        ],\n",
       "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "            0.        ]],\n",
       " \n",
       "        [[ -9.4763381 ,  -0.47624819,  -0.46163136,  -0.44901961,\n",
       "            0.        ],\n",
       "         [ -9.48667664,  -0.42482655,  -0.46366235,  -0.44509804,\n",
       "            0.        ],\n",
       "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "            0.        ],\n",
       "         ...,\n",
       "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "            0.        ],\n",
       "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "            0.        ],\n",
       "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "            0.        ]],\n",
       " \n",
       "        [[  3.21228856, -11.31615849,  -0.15628595,  -0.42941176,\n",
       "            0.        ],\n",
       "         [  3.27945358, -11.3031182 ,  -0.15626074,  -0.42941176,\n",
       "            0.        ],\n",
       "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "            0.        ],\n",
       "         ...,\n",
       "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "            0.        ],\n",
       "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "            0.        ],\n",
       "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "            0.        ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-12.57326794,  11.38537394,   2.65794734,  -0.46862745,\n",
       "            0.        ],\n",
       "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "            0.        ],\n",
       "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "            0.        ],\n",
       "         ...,\n",
       "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "            0.        ],\n",
       "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "            0.        ],\n",
       "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "            0.        ]],\n",
       " \n",
       "        [[-22.20056142,  -5.14858097,  -0.15719274,  -0.4254902 ,\n",
       "            0.        ],\n",
       "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "            0.        ],\n",
       "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "            0.        ],\n",
       "         ...,\n",
       "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "            0.        ],\n",
       "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "            0.        ],\n",
       "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "            0.        ]],\n",
       " \n",
       "        [[  8.28906918,  -3.00090593,  -0.18125716,  -0.35490196,\n",
       "            0.        ],\n",
       "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "            0.        ],\n",
       "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "            0.        ],\n",
       "         ...,\n",
       "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "            0.        ],\n",
       "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "            0.        ],\n",
       "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "            0.        ]]]),\n",
       " 'num_points': array([4, 2, 2, ..., 1, 1, 1], dtype=int32),\n",
       " 'coordinates': array([[   0,  560, 1231],\n",
       "        [   0,  595, 1105],\n",
       "        [   0,  486, 1232],\n",
       "        ...,\n",
       "        [   1,  713, 1074],\n",
       "        [   0,  548,  977],\n",
       "        [   0,  569, 1282]], dtype=int32),\n",
       " 'num_voxels': array([12694]),\n",
       " 'metrics': {'voxel_gene_time': 0.002565622329711914,\n",
       "  'prep_time': 0.5952579975128174},\n",
       " 'anchors': array([[-100.    , -100.    ,   -5.    , ...,    0.36  ,    0.51  ,\n",
       "            0.    ],\n",
       "        [ -99.7998, -100.    ,   -5.    , ...,    0.36  ,    0.51  ,\n",
       "            0.    ],\n",
       "        [ -99.5996, -100.    ,   -5.    , ...,    0.36  ,    0.51  ,\n",
       "            0.    ],\n",
       "        ...,\n",
       "        [  99.5996,  100.    ,   -5.    , ...,    0.36  ,    0.51  ,\n",
       "            1.57  ],\n",
       "        [  99.7998,  100.    ,   -5.    , ...,    0.36  ,    0.51  ,\n",
       "            1.57  ],\n",
       "        [ 100.    ,  100.    ,   -5.    , ...,    0.36  ,    0.51  ,\n",
       "            1.57  ]], dtype=float32),\n",
       " 'gt_names': array(['traffic_cone', 'traffic_cone', 'traffic_cone'], dtype='<U12'),\n",
       " 'labels': array([0, 0, 0, ..., 0, 0, 0], dtype=int32),\n",
       " 'reg_targets': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " 'importance': array([1., 1., 1., ..., 1., 1., 1.], dtype=float32),\n",
       " 'metadata': {'token': 'e93e98b63d3b40209056d129dc53ceee'}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _worker_init_fn(worker_id):\n",
    "    time_seed = np.array(time.time(), dtype=np.int32)\n",
    "    np.random.seed(time_seed + worker_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=input_cfg.batch_size * num_gpu,\n",
    "    shuffle=True,\n",
    "    num_workers=input_cfg.preprocess.num_workers * num_gpu,\n",
    "    pin_memory=False,\n",
    "    collate_fn=collate_fn,\n",
    "    worker_init_fn=_worker_init_fn,\n",
    "    drop_last=not multi_gpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.num_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: {\n",
      "  second: {\n",
      "    network_class_name: \"VoxelNet\"\n",
      "    voxel_generator {\n",
      "      full_empty_part_with_mean: false\n",
      "      point_cloud_range : [-120, -60, -4 , 80, 60, 5] # will use [0,1,3,4] means [-100,-100,100,1000]\n",
      "      voxel_size : [0.1, 0.1, 4]\n",
      "      max_number_of_points_per_voxel : 15\n",
      "      #block_filtering: true # filter voxels by block height\n",
      "      #block_factor: 1 # height calc width: voxel_size * block_factor * block_size= (0.05 * 1 * 8)\n",
      "      #block_size: 8\n",
      "      #height_threshold: 0.2 # locations with height < height_threshold will be removed.\n",
      "    }\n",
      "    voxel_feature_extractor: {\n",
      "      module_class_name: \"PillarFeatureNet\"\n",
      "      num_filters: [64]\n",
      "      with_distance: false\n",
      "      num_input_features: 5\n",
      "    }\n",
      "    middle_feature_extractor: {\n",
      "      module_class_name: \"PointPillarsScatter\"\n",
      "      downsample_factor: 1\n",
      "      num_input_features: 64\n",
      "    }\n",
      "    rpn: {\n",
      "      module_class_name: \"RPNV2\"\n",
      "      layer_nums: [3, 5, 5]\n",
      "      layer_strides: [2, 2, 2]\n",
      "      num_filters: [64, 128, 256]\n",
      "      upsample_strides: [1, 2, 4]\n",
      "      num_upsample_filters: [128, 128, 128]\n",
      "      use_groupnorm: false\n",
      "      num_groups: 32\n",
      "      num_input_features: 64\n",
      "    }\n",
      "    loss: {\n",
      "      classification_loss: {\n",
      "        weighted_sigmoid_focal: {\n",
      "          alpha: 0.25\n",
      "          gamma: 2.0\n",
      "          anchorwise_output: true\n",
      "        }\n",
      "      }\n",
      "      localization_loss: {\n",
      "        weighted_smooth_l1: {\n",
      "          sigma: 3.0\n",
      "          code_weight: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "        }\n",
      "      }\n",
      "      classification_weight: 1.0\n",
      "      localization_weight: 2.0\n",
      "    }\n",
      "    num_point_features: 5 # model's num point feature should be independent of dataset\n",
      "    # Outputs\n",
      "    use_sigmoid_score: true\n",
      "    encode_background_as_zeros: true\n",
      "    encode_rad_error_by_sin: true\n",
      "    sin_error_factor: 1.0\n",
      "\n",
      "    use_direction_classifier: false # this can help for orientation benchmark\n",
      "    direction_loss_weight: 0.2 # enough.\n",
      "    num_direction_bins: 2\n",
      "    direction_limit_offset: 1\n",
      "\n",
      "    # Loss\n",
      "    pos_class_weight: 1.0\n",
      "    neg_class_weight: 1.0\n",
      "\n",
      "    loss_norm_type: NormByNumPositives\n",
      "    # Postprocess\n",
      "    post_center_limit_range: [0, -40, -2.2, 70.4, 40, 0.8]\n",
      "    nms_class_agnostic: false # only valid in multi-class nms\n",
      "\n",
      "    box_coder: {\n",
      "      ground_box3d_coder: {\n",
      "        linear_dim: false\n",
      "        encode_angle_vector: false\n",
      "      }\n",
      "    }\n",
      "    target_assigner: {\n",
      "      class_settings: {\n",
      "        class_name: \"traffic_cone\"\n",
      "        anchor_generator_range: {\n",
      "          sizes: [0.36, 0.36, 0.51] # wlh\n",
      "          anchor_ranges: [-100, -100, -5 , 100, 100, 3]\n",
      "          rotations: [0, 1.57] # DON'T modify this unless you are very familiar with my code.\n",
      "          # custom_values: [0, 0] # velocity vector base value\n",
      "        }\n",
      "        matched_threshold : 0.5\n",
      "        unmatched_threshold : 0.35\n",
      "        use_rotate_nms: false\n",
      "        use_multi_class_nms: false\n",
      "        nms_pre_max_size: 1000\n",
      "        nms_post_max_size: 300\n",
      "        nms_score_threshold: 0.02\n",
      "        nms_iou_threshold: 0.3\n",
      "        region_similarity_calculator: {\n",
      "          distance_similarity: {\n",
      "            distance_norm: 1.414 # match range\n",
      "            with_rotation: false\n",
      "            rotation_alpha: 0.0 # rot error contribution\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "\n",
      "      sample_positive_fraction : -1\n",
      "      sample_size : 512\n",
      "      assign_per_class: true\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "train_input_reader: {\n",
      "  dataset: {\n",
      "    dataset_class_name: \"CustomNuscDataset\"\n",
      "    kitti_info_path: \" \"\n",
      "    kitti_root_path: \"/media/starlet/LdTho/data/sets/nuscenes/v1.0-trainval\"\n",
      "  }\n",
      "  batch_size: 1\n",
      "  preprocess: {\n",
      "    num_workers: 3\n",
      "    shuffle_points: true\n",
      "    max_number_of_voxels: 70000\n",
      "\n",
      "    groundtruth_localization_noise_std: [0.0, 0.0, 0.0]\n",
      "    groundtruth_rotation_uniform_noise: [-0.0, 0.0]\n",
      "    global_rotation_uniform_noise: [-0.3925, 0.3925]\n",
      "    global_scaling_uniform_noise: [0.95, 1.05]\n",
      "    global_random_rotation_range_per_object: [0, 0] # pi/4 ~ 3pi/4\n",
      "    global_translate_noise_std: [0.2, 0.2, 0.2]\n",
      "    anchor_area_threshold: -1 # very slow if enable when using FHD map (1600x1200x40).\n",
      "    remove_points_after_sample: true\n",
      "    groundtruth_points_drop_percentage: 0.0\n",
      "    groundtruth_drop_max_keep_points: 15\n",
      "    remove_unknown_examples: false\n",
      "    sample_importance: 1.0\n",
      "    random_flip_x: true\n",
      "    random_flip_y: true\n",
      "    remove_environment: false\n",
      "    database_sampler {\n",
      "      database_info_path: \"\"\n",
      "    }\n",
      "  }\n",
      "\n",
      "}\n",
      "\n",
      "train_config: {\n",
      "  optimizer: {\n",
      "    adam_optimizer: {\n",
      "      learning_rate: {\n",
      "        one_cycle: {\n",
      "          lr_max: 1e-3\n",
      "          moms: [0.95, 0.85]\n",
      "          div_factor: 10.0\n",
      "          pct_start: 0.4\n",
      "        }\n",
      "      }\n",
      "      weight_decay: 0.01\n",
      "    }\n",
      "    fixed_weight_decay: true\n",
      "    use_moving_average: false\n",
      "  }\n",
      "  steps: 15000 # 120000 * 4 * 2 / 22680 ~ 40 epoch\n",
      "  steps_per_eval: 16000 # 1238 * 5\n",
      "  save_checkpoints_secs : 1800 # half hour\n",
      "  save_summary_steps : 10\n",
      "  enable_mixed_precision: true\n",
      "  loss_scale_factor: -1\n",
      "  clear_metrics_every_epoch: true\n",
      "}\n",
      "\n",
      "eval_input_reader: {\n",
      "  dataset: {\n",
      "    dataset_class_name: \"CustomNuscTestDataset\"\n",
      "    kitti_info_path: \" \"\n",
      "    kitti_root_path: \" \"\n",
      "  }\n",
      "  batch_size: 1\n",
      "  preprocess: {\n",
      "    max_number_of_voxels: 120000\n",
      "    shuffle_points: true\n",
      "    num_workers: 3\n",
      "    anchor_area_threshold: -1\n",
      "    remove_environment: false\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_logging = SimpleModelLog(model_dir)\n",
    "model_logging.open()\n",
    "model_logging.log_text(proto_str + \"\\n\", 0, tag='config')\n",
    "start_step = net.get_global_step()\n",
    "total_step = train_cfg.steps\n",
    "t = time.time()\n",
    "steps_per_eval = train_cfg.steps_per_eval\n",
    "clear_metrics_every_epoch = train_cfg.clear_metrics_every_epoch\n",
    "\n",
    "amp_optimizer.zero_grad()\n",
    "step_times = []\n",
    "step = start_step\n",
    "\n",
    "ave_valid_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_convert_to_torch(example, dtype=torch.float32,\n",
    "                             device=None) -> dict:\n",
    "    device = device or torch.device(\"cuda\")\n",
    "    example_torch = {}\n",
    "    float_names = [\n",
    "        \"voxels\", \"anchors\", \"reg_targets\", \"reg_weights\", \"bev_map\", \"importance\"\n",
    "    ]\n",
    "    for k, v in example.items():\n",
    "        if k in ['gt_names', 'gt_classes', 'gt_boxes','points']:\n",
    "            example_torch[k] = example[k]\n",
    "            continue\n",
    "\n",
    "        if k in float_names:\n",
    "            # slow when directly provide fp32 data with dtype=torch.half\n",
    "            example_torch[k] = torch.tensor(\n",
    "                v, dtype=torch.float32, device=device).to(dtype)\n",
    "        elif k in [\"coordinates\", \"labels\", \"num_points\"]:\n",
    "            example_torch[k] = torch.tensor(\n",
    "                v, dtype=torch.int32, device=device)\n",
    "        elif k in [\"anchors_mask\"]:\n",
    "            example_torch[k] = torch.tensor(\n",
    "                v, dtype=torch.uint8, device=device)\n",
    "        elif k == \"calib\":\n",
    "            calib = {}\n",
    "            for k1, v1 in v.items():\n",
    "                calib[k1] = torch.tensor(\n",
    "                    v1, dtype=dtype, device=device).to(dtype)\n",
    "            example_torch[k] = calib\n",
    "        elif k == \"num_voxels\":\n",
    "            example_torch[k] = torch.tensor(v)\n",
    "        else:\n",
    "            example_torch[k] = v\n",
    "    return example_torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_str(t, mode='min'):\n",
    "    if mode == 'min':\n",
    "        t = int(t) / 60\n",
    "        hr = t // 60\n",
    "        min = t % 60\n",
    "        return '%2d hr %02d m' % (hr, min)\n",
    "\n",
    "    elif mode == 'sec':\n",
    "        t = int(t)\n",
    "        min = t // 60\n",
    "        sec = t % 60\n",
    "        return '%2d min %02d sec' % (min, sec)\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples: 14856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-1993cac64d66>:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for example in tqdm_notebook(dataloader):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c7e504245743a7aaaf474fdb514fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14856 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2021-01-20 22:47:41,429 - transforms - finding looplift candidates\n",
      "INFO - 2021-01-20 22:47:43,251 - transforms - finding looplift candidates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2021-01-20 22:47:55,250 - transforms - finding looplift candidates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=50, epoch=0.003366, steptime=2.356, valid=0.0, loss=430.0, cls_loss=402.1, loc_loss=27.96, lr=0.0001001, eta= 0 hr 01 m\n",
      "step=100, epoch=0.006731, steptime=2.289, valid=0.0, loss=110.4, cls_loss=84.14, loc_loss=26.28, lr=0.0001006, eta= 0 hr 03 m\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0\n",
      "step=150, epoch=0.0101, steptime=2.355, valid=0.0, loss=52.78, cls_loss=29.65, loc_loss=23.14, lr=0.0001014, eta= 0 hr 05 m\n",
      "step=200, epoch=0.01346, steptime=2.303, valid=0.0, loss=24.7, cls_loss=6.65, loc_loss=18.05, lr=0.0001024, eta= 0 hr 07 m\n",
      "step=250, epoch=0.01683, steptime=2.317, valid=0.0, loss=18.33, cls_loss=3.818, loc_loss=14.52, lr=0.0001038, eta= 0 hr 09 m\n",
      "step=300, epoch=0.02019, steptime=2.311, valid=0.0, loss=12.1, cls_loss=1.744, loc_loss=10.36, lr=0.0001055, eta= 0 hr 11 m\n",
      "step=350, epoch=0.02356, steptime=2.326, valid=0.0, loss=11.3, cls_loss=1.437, loc_loss=9.858, lr=0.0001075, eta= 0 hr 13 m\n",
      "step=400, epoch=0.02693, steptime=2.329, valid=0.0, loss=13.8, cls_loss=2.432, loc_loss=11.36, lr=0.0001098, eta= 0 hr 15 m\n",
      "step=450, epoch=0.03029, steptime=2.326, valid=0.0, loss=10.77, cls_loss=1.074, loc_loss=9.7, lr=0.0001124, eta= 0 hr 17 m\n",
      "step=500, epoch=0.03366, steptime=2.333, valid=0.0, loss=9.637, cls_loss=0.9471, loc_loss=8.69, lr=0.0001153, eta= 0 hr 19 m\n",
      "step=550, epoch=0.03702, steptime=2.334, valid=0.0, loss=9.827, cls_loss=0.9796, loc_loss=8.848, lr=0.0001185, eta= 0 hr 21 m\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0\n",
      "step=600, epoch=0.04039, steptime=2.331, valid=0.0, loss=10.28, cls_loss=0.9381, loc_loss=9.346, lr=0.000122, eta= 0 hr 23 m\n",
      "step=650, epoch=0.04375, steptime=2.338, valid=0.0, loss=8.911, cls_loss=0.9611, loc_loss=7.95, lr=0.0001257, eta= 0 hr 25 m\n",
      "step=700, epoch=0.04712, steptime=2.333, valid=0.0, loss=8.815, cls_loss=0.9045, loc_loss=7.91, lr=0.0001298, eta= 0 hr 27 m\n",
      "step=750, epoch=0.05048, steptime=2.331, valid=0.0, loss=8.497, cls_loss=0.8739, loc_loss=7.623, lr=0.0001342, eta= 0 hr 29 m\n",
      "step=800, epoch=0.05385, steptime=2.336, valid=0.0, loss=8.461, cls_loss=0.8594, loc_loss=7.602, lr=0.0001388, eta= 0 hr 31 m\n",
      "step=850, epoch=0.05722, steptime=2.332, valid=0.0, loss=9.822, cls_loss=0.9111, loc_loss=8.911, lr=0.0001437, eta= 0 hr 32 m\n",
      "step=900, epoch=0.06058, steptime=2.334, valid=0.0, loss=8.916, cls_loss=0.8913, loc_loss=8.025, lr=0.0001489, eta= 0 hr 34 m\n",
      "step=950, epoch=0.06395, steptime=2.334, valid=0.0, loss=8.285, cls_loss=0.8433, loc_loss=7.441, lr=0.0001544, eta= 0 hr 36 m\n",
      "step=1000, epoch=0.06731, steptime=2.335, valid=0.0, loss=9.577, cls_loss=1.026, loc_loss=8.551, lr=0.0001602, eta= 0 hr 38 m\n",
      "step=1050, epoch=0.07068, steptime=2.332, valid=0.0, loss=8.489, cls_loss=0.9435, loc_loss=7.545, lr=0.0001662, eta= 0 hr 40 m\n",
      "step=1100, epoch=0.07404, steptime=2.332, valid=0.0, loss=7.723, cls_loss=0.8884, loc_loss=6.834, lr=0.0001725, eta= 0 hr 42 m\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0\n",
      "step=1150, epoch=0.07741, steptime=2.333, valid=0.0, loss=7.989, cls_loss=0.8721, loc_loss=7.117, lr=0.000179, eta= 0 hr 44 m\n",
      "step=1200, epoch=0.08078, steptime=2.331, valid=0.0, loss=8.094, cls_loss=0.8684, loc_loss=7.225, lr=0.0001858, eta= 0 hr 46 m\n",
      "step=1250, epoch=0.08414, steptime=2.333, valid=0.0, loss=8.253, cls_loss=0.9085, loc_loss=7.344, lr=0.0001928, eta= 0 hr 48 m\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0\n",
      "step=1300, epoch=0.08751, steptime=2.329, valid=0.0, loss=7.905, cls_loss=0.8572, loc_loss=7.048, lr=0.0002001, eta= 0 hr 50 m\n",
      "step=1350, epoch=0.09087, steptime=2.332, valid=0.0, loss=7.773, cls_loss=0.9234, loc_loss=6.85, lr=0.0002077, eta= 0 hr 52 m\n",
      "step=1400, epoch=0.09424, steptime=2.364, valid=0.0, loss=8.551, cls_loss=0.9006, loc_loss=7.65, lr=0.0002154, eta= 0 hr 54 m\n",
      "step=1450, epoch=0.0976, steptime=2.365, valid=0.0, loss=7.848, cls_loss=0.8611, loc_loss=6.987, lr=0.0002234, eta= 0 hr 56 m\n",
      "step=1500, epoch=0.101, steptime=2.357, valid=0.0, loss=8.006, cls_loss=0.9163, loc_loss=7.09, lr=0.0002316, eta= 0 hr 58 m\n",
      "step=1550, epoch=0.1043, steptime=2.364, valid=0.0, loss=7.654, cls_loss=0.8994, loc_loss=6.755, lr=0.0002401, eta= 1 hr 00 m\n",
      "step=1600, epoch=0.1077, steptime=2.368, valid=0.0, loss=7.56, cls_loss=0.8572, loc_loss=6.703, lr=0.0002487, eta= 1 hr 02 m\n",
      "step=1650, epoch=0.1111, steptime=2.357, valid=0.0, loss=7.729, cls_loss=1.275, loc_loss=6.455, lr=0.0002576, eta= 1 hr 04 m\n",
      "step=1700, epoch=0.1144, steptime=2.361, valid=0.0, loss=7.992, cls_loss=1.105, loc_loss=6.887, lr=0.0002666, eta= 1 hr 06 m\n",
      "step=1750, epoch=0.1178, steptime=2.362, valid=0.0, loss=7.771, cls_loss=0.9738, loc_loss=6.797, lr=0.0002759, eta= 1 hr 08 m\n",
      "step=1800, epoch=0.1212, steptime=2.356, valid=0.0, loss=7.409, cls_loss=0.9085, loc_loss=6.501, lr=0.0002853, eta= 1 hr 10 m\n",
      "step=1850, epoch=0.1245, steptime=2.366, valid=0.0, loss=7.687, cls_loss=0.9656, loc_loss=6.721, lr=0.0002949, eta= 1 hr 12 m\n",
      "step=1900, epoch=0.1279, steptime=2.359, valid=0.0, loss=7.005, cls_loss=0.9172, loc_loss=6.088, lr=0.0003047, eta= 1 hr 14 m\n",
      "step=1950, epoch=0.1313, steptime=2.362, valid=0.0, loss=7.107, cls_loss=0.9289, loc_loss=6.178, lr=0.0003147, eta= 1 hr 16 m\n",
      "step=2000, epoch=0.1346, steptime=2.358, valid=0.0, loss=7.414, cls_loss=0.8951, loc_loss=6.519, lr=0.0003248, eta= 1 hr 18 m\n",
      "step=2050, epoch=0.138, steptime=2.356, valid=0.0, loss=7.179, cls_loss=0.9161, loc_loss=6.262, lr=0.0003351, eta= 1 hr 19 m\n",
      "step=2100, epoch=0.1414, steptime=2.352, valid=0.0, loss=7.93, cls_loss=1.21, loc_loss=6.72, lr=0.0003455, eta= 1 hr 21 m\n",
      "step=2150, epoch=0.1447, steptime=2.362, valid=0.0, loss=7.152, cls_loss=0.915, loc_loss=6.237, lr=0.0003561, eta= 1 hr 23 m\n",
      "step=2200, epoch=0.1481, steptime=2.359, valid=0.0, loss=7.335, cls_loss=0.9112, loc_loss=6.423, lr=0.0003668, eta= 1 hr 25 m\n",
      "step=2250, epoch=0.1515, steptime=2.394, valid=0.0, loss=8.051, cls_loss=0.9593, loc_loss=7.091, lr=0.0003776, eta= 1 hr 27 m\n",
      "step=2300, epoch=0.1548, steptime=2.422, valid=0.0, loss=7.839, cls_loss=1.159, loc_loss=6.681, lr=0.0003885, eta= 1 hr 29 m\n",
      "step=2350, epoch=0.1582, steptime=2.369, valid=0.0, loss=8.074, cls_loss=0.9853, loc_loss=7.089, lr=0.0003996, eta= 1 hr 31 m\n",
      "step=2400, epoch=0.1616, steptime=2.382, valid=0.0, loss=7.623, cls_loss=0.9501, loc_loss=6.673, lr=0.0004107, eta= 1 hr 33 m\n",
      "step=2450, epoch=0.1649, steptime=2.474, valid=0.0, loss=7.209, cls_loss=0.9519, loc_loss=6.257, lr=0.000422, eta= 1 hr 35 m\n",
      "step=2500, epoch=0.1683, steptime=2.363, valid=0.0, loss=7.299, cls_loss=0.9329, loc_loss=6.366, lr=0.0004333, eta= 1 hr 37 m\n",
      "step=2550, epoch=0.1716, steptime=2.368, valid=0.0, loss=7.596, cls_loss=1.312, loc_loss=6.283, lr=0.0004447, eta= 1 hr 39 m\n",
      "step=2600, epoch=0.175, steptime=2.375, valid=0.0, loss=7.055, cls_loss=0.9081, loc_loss=6.147, lr=0.0004562, eta= 1 hr 41 m\n",
      "step=2650, epoch=0.1784, steptime=2.386, valid=0.0, loss=6.872, cls_loss=0.8748, loc_loss=5.997, lr=0.0004678, eta= 1 hr 43 m\n",
      "step=2700, epoch=0.1817, steptime=2.384, valid=0.0, loss=7.359, cls_loss=0.9312, loc_loss=6.428, lr=0.0004794, eta= 1 hr 45 m\n",
      "step=2750, epoch=0.1851, steptime=2.358, valid=0.0, loss=7.085, cls_loss=0.9089, loc_loss=6.176, lr=0.000491, eta= 1 hr 47 m\n",
      "step=2800, epoch=0.1885, steptime=2.326, valid=0.0, loss=7.03, cls_loss=0.9175, loc_loss=6.113, lr=0.0005027, eta= 1 hr 49 m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-1993cac64d66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamp_optimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spconv12cuda11/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spconv12cuda11/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "display_step=50\n",
    "clear_metrics_every_epoch = True\n",
    "try:\n",
    "    start_tic = time.time()\n",
    "    print(\"num samples: %d\" % (len(dataset)))\n",
    "    while True:\n",
    "        if clear_metrics_every_epoch:\n",
    "            net.clear_metrics()\n",
    "        for example in tqdm_notebook(dataloader):\n",
    "            #             print(example)\n",
    "            lr_scheduler.step(net.get_global_step())\n",
    "            example.pop(\"metrics\")\n",
    "            example_torch = example_convert_to_torch(example, float_dtype)\n",
    "\n",
    "\n",
    "            ret_dict = net_parallel(example_torch)\n",
    "            loss = ret_dict[\"loss\"].mean()\n",
    "            cls_loss_reduced = ret_dict[\"cls_loss_reduced\"].mean()\n",
    "            loc_loss_reduced = ret_dict[\"loc_loss_reduced\"].mean()\n",
    "\n",
    "\n",
    "            if train_cfg.enable_mixed_precision:\n",
    "                if net.get_global_step() < 100:\n",
    "                    loss *= 1e-3\n",
    "                with amp.scale_loss(loss, amp_optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), 10.0)\n",
    "            amp_optimizer.step()\n",
    "            amp_optimizer.zero_grad()\n",
    "            net.update_global_step()\n",
    "\n",
    "            cls_preds = ret_dict[\"cls_preds\"]\n",
    "            labels = example_torch[\"labels\"]\n",
    "            cared = ret_dict[\"cared\"]\n",
    "\n",
    "            net_metrics = net.update_metrics(cls_loss_reduced,\n",
    "                                             loc_loss_reduced, cls_preds,\n",
    "                                             labels, cared)\n",
    "            step_time = (time.time() - t)\n",
    "            step_times.append(step_time)\n",
    "            t = time.time()\n",
    "            metrics = {}\n",
    "            global_step = net.get_global_step()\n",
    "\n",
    "            if global_step % display_step == 0:\n",
    "                eta = time.time() - start_tic\n",
    "                if measure_time:\n",
    "                    for name, val in net.get_avg_time_dict().items():\n",
    "                        print(f\"avg {name} time = {val * 1000:.3f} ms\")\n",
    "\n",
    "                metrics[\"step\"] = global_step\n",
    "                metrics['epoch'] = global_step / len(dataloader)\n",
    "                metrics['steptime'] = np.mean(step_times)\n",
    "                # metrics[\"runtime\"].update(time_metrics[0])\n",
    "                metrics['valid'] = ave_valid_loss\n",
    "                step_times = []\n",
    "\n",
    "                metrics[\"loss\"] = net_metrics['loss']['cls_loss'] + net_metrics['loss']['loc_loss']\n",
    "                metrics[\"cls_loss\"] = net_metrics['loss']['cls_loss']\n",
    "                metrics[\"loc_loss\"] = net_metrics['loss']['loc_loss']\n",
    "\n",
    "                if model_cfg.use_direction_classifier:\n",
    "                    dir_loss_reduced = ret_dict[\"dir_loss_reduced\"].mean()\n",
    "                    metrics[\"dir_rt\"] = float(\n",
    "                        dir_loss_reduced.detach().cpu().numpy())\n",
    "\n",
    "                metrics['lr'] = float(amp_optimizer.lr)\n",
    "                metrics['eta'] = time_to_str(eta)\n",
    "                model_logging.log_metrics(metrics, global_step)\n",
    "\n",
    "                net.clear_metrics()\n",
    "            if global_step % steps_per_eval == 0:\n",
    "                torchplus.train.save_models(model_dir, [net, amp_optimizer],\n",
    "                                            net.get_global_step())\n",
    "            step += 1\n",
    "            if step >= total_step:\n",
    "                break\n",
    "        if step >= total_step:\n",
    "            break\n",
    "except Exception as e:\n",
    "    example['metadata'] = 'None'\n",
    "    model_logging.log_text(str(e), step)\n",
    "    model_logging.log_text(json.dumps(example['metadata'], indent=2), step)\n",
    "    torchplus.train.save_models(model_dir, [net, amp_optimizer], step)\n",
    "    raise e\n",
    "finally:\n",
    "    model_logging.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchplus.train.save_models(model_dir, [net, amp_optimizer],\n",
    "                                            net.get_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611143208.1997967')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = CustomNuscDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset[0]['lidar']['annotations']['boxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_x, max_x,min_y, max_y,min_z, max_z= 999999,-999999,999999,-999999,999999,-999999\n",
    "from tqdm import tqdm_notebook\n",
    "for i in tqdm_notebook(range(len(dset))):\n",
    "    for box in dset[i]['lidar']['annotations']['boxes']:\n",
    "        min_x = round(min(min_x, box[0]),2)\n",
    "        max_x = round(max(max_x, box[0]),2)\n",
    "        min_y = round(min(min_y, box[1]),2)\n",
    "        max_y = round(max(max_y, box[1]),2)\n",
    "        min_z = round(min(min_z, box[2]),2)\n",
    "        max_z = round(max(max_z, box[2]),2)\n",
    "    print(min_x, max_x,min_y, max_y,min_z, max_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min_x, max_x,min_y, max_y,min_z, max_z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

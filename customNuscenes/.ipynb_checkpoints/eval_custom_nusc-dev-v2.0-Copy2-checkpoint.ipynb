{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "simple-viewer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3382,  0.6436]])\n",
      "tensor([[-1.2450, -1.3614]], device='cuda:0')\n",
      "tensor([[-0.3382,  0.6436]])\n",
      "False\n",
      "tensor([[-0.3382,  0.6436]], device='cuda:0')\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "from numba.core.errors import NumbaDeprecationWarning,NumbaPendingDeprecationWarning, NumbaWarning\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', category=NumbaDeprecationWarning)\n",
    "warnings.simplefilter('ignore', category=NumbaWarning)\n",
    "import sys\n",
    "sys.path.append('/kaggle/code/ConeDetectionPointpillars')\n",
    "\n",
    "from second.data.CustomNuscDataset import * #to register dataset\n",
    "from models import * #to register model\n",
    "import torch\n",
    "from second.utils.log_tool import SimpleModelLog\n",
    "from second.builder import target_assigner_builder, voxel_builder\n",
    "from second.protos import pipeline_pb2\n",
    "from second.pytorch.builder import (box_coder_builder, input_reader_builder,\n",
    "                                    lr_scheduler_builder, optimizer_builder,\n",
    "                                    second_builder)\n",
    "from second.pytorch.core import box_torch_ops\n",
    "import torchplus\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from google.protobuf import text_format\n",
    "# from lyft_dataset_sdk.utils.geometry_utils import *\n",
    "from lyft_dataset_sdk.lyftdataset import Quaternion\n",
    "from apex import amp\n",
    "# from lyft_dataset_sdk.utils.data_classes import Box\n",
    "from nuscenes.utils.geometry_utils import *\n",
    "from nuscenes.utils.data_classes import Box\n",
    "from second.utils.progress_bar import ProgressBar\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "#commented second.core.non_max_suppression, nms_cpu, __init__.py, \n",
    "# pytorch.core.box_torch_ops line 524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "national-elder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(model_cfg, measure_time=False):\n",
    "    voxel_generator = voxel_builder.build(model_cfg.voxel_generator)\n",
    "    bv_range = voxel_generator.point_cloud_range[[0, 1, 3, 4]]\n",
    "    box_coder = box_coder_builder.build(model_cfg.box_coder)\n",
    "    target_assigner_cfg = model_cfg.target_assigner\n",
    "    target_assigner = target_assigner_builder.build(target_assigner_cfg,\n",
    "                                                    bv_range, box_coder)\n",
    "    box_coder.custom_ndim = target_assigner._anchor_generators[0].custom_ndim\n",
    "    net = second_builder.build(\n",
    "        model_cfg, voxel_generator, target_assigner, measure_time=measure_time)\n",
    "    return net\n",
    "\n",
    "\n",
    "def merge_second_batch_multigpu(batch_list):\n",
    "    if isinstance(batch_list[0], list):\n",
    "        batch_list_c = []\n",
    "        for example in batch_list:\n",
    "            batch_list_c += example\n",
    "        batch_list = batch_list_c\n",
    "\n",
    "    example_merged = defaultdict(list)\n",
    "    for example in batch_list:\n",
    "        for k, v in example.items():\n",
    "            example_merged[k].append(v)\n",
    "    ret = {}\n",
    "    for key, elems in example_merged.items():\n",
    "        if key == 'metadata':\n",
    "            ret[key] = elems\n",
    "        elif key == \"calib\":\n",
    "            ret[key] = {}\n",
    "            for elem in elems:\n",
    "                for k1, v1 in elem.items():\n",
    "                    if k1 not in ret[key]:\n",
    "                        ret[key][k1] = [v1]\n",
    "                    else:\n",
    "                        ret[key][k1].append(v1)\n",
    "            for k1, v1 in ret[key].items():\n",
    "                ret[key][k1] = np.stack(v1, axis=0)\n",
    "        elif key == 'coordinates':\n",
    "            coors = []\n",
    "            for i, coor in enumerate(elems):\n",
    "                coor_pad = np.pad(\n",
    "                    coor, ((0, 0), (1, 0)), mode='constant', constant_values=i)\n",
    "                coors.append(coor_pad)\n",
    "            ret[key] = np.stack(coors, axis=0)\n",
    "        elif key in ['gt_names', 'gt_classes', 'gt_boxes']:\n",
    "            continue\n",
    "        else:\n",
    "            ret[key] = np.stack(elems, axis=0)\n",
    "\n",
    "    return ret\n",
    "\n",
    "def buildBBox(points,color = [1,0,0]):\n",
    "    #print(\"Let's draw a cubic using o3d.geometry.LineSet\")\n",
    "    # points = [[0, 0, 0], [1, 0, 0], [0, 1, 0], [1, 1, 0], [0, 0, 1], [1, 0, 1],\n",
    "    #           [0, 1, 1], [1, 1, 1]] x0y0z0, x0y0z1, x0y1z0, x0y1z1, x1y0z0, x1y0z1, x1y1z0, x1y1z1\n",
    "\n",
    "    points = points[[0,4,3,7,1,5,2,6],:]\n",
    "    lines = [[0, 1], [0, 2], [1, 3], [2, 3], [4, 5], [4, 6], [5, 7], [6, 7],\n",
    "             [0, 4], [1, 5], [2, 6], [3, 7]]\n",
    "    colors = [color for i in range(len(lines))]\n",
    "    line_set = o3d.geometry.LineSet()\n",
    "    line_set.points = o3d.utility.Vector3dVector(points)\n",
    "    line_set.lines = o3d.utility.Vector2iVector(lines)\n",
    "    line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "\n",
    "    return  line_set\n",
    "def merge_second_batch(batch_list):\n",
    "    if isinstance(batch_list[0], list):\n",
    "        batch_list_c = []\n",
    "        for example in batch_list:\n",
    "            batch_list_c += example\n",
    "        batch_list = batch_list_c\n",
    "\n",
    "    example_merged = defaultdict(list)\n",
    "    for example in batch_list:\n",
    "        for k, v in example.items():\n",
    "            example_merged[k].append(v)\n",
    "    ret = {}\n",
    "    for key, elems in example_merged.items():\n",
    "        if key in [\n",
    "            'voxels', 'num_points', 'num_gt', 'voxel_labels', 'gt_names', 'gt_classes', 'gt_boxes'\n",
    "        ]:\n",
    "            ret[key] = np.concatenate(elems, axis=0)\n",
    "        elif key == 'metadata':\n",
    "            ret[key] = elems\n",
    "        elif key == \"calib\":\n",
    "            ret[key] = {}\n",
    "            for elem in elems:\n",
    "                for k1, v1 in elem.items():\n",
    "                    if k1 not in ret[key]:\n",
    "                        ret[key][k1] = [v1]\n",
    "                    else:\n",
    "                        ret[key][k1].append(v1)\n",
    "            for k1, v1 in ret[key].items():\n",
    "                ret[key][k1] = np.stack(v1, axis=0)\n",
    "        elif key == 'coordinates':\n",
    "            coors = []\n",
    "            for i, coor in enumerate(elems):\n",
    "                coor_pad = np.pad(\n",
    "                    coor, ((0, 0), (1, 0)), mode='constant', constant_values=i)\n",
    "                coors.append(coor_pad)\n",
    "            ret[key] = np.concatenate(coors, axis=0)\n",
    "        elif key == 'metrics':\n",
    "            ret[key] = elems\n",
    "        else:\n",
    "            ret[key] = np.stack(elems, axis=0)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def _worker_init_fn(worker_id):\n",
    "    time_seed = np.array(time.time(), dtype=np.int32)\n",
    "    np.random.seed(time_seed + worker_id)\n",
    "\n",
    "\n",
    "def example_convert_to_torch(example, dtype=torch.float32,\n",
    "                             device=None) -> dict:\n",
    "    device = device or torch.device(\"cuda:0\")\n",
    "    example_torch = {}\n",
    "    float_names = [\n",
    "        \"voxels\", \"anchors\", \"reg_targets\", \"reg_weights\", \"bev_map\", \"importance\"\n",
    "    ]\n",
    "    for k, v in example.items():\n",
    "        if k in ['gt_names', 'gt_classes', 'gt_boxes', 'points']:\n",
    "            example_torch[k] = example[k]\n",
    "            continue\n",
    "\n",
    "        if k in float_names:\n",
    "            # slow when directly provide fp32 data with dtype=torch.half\n",
    "            example_torch[k] = torch.tensor(\n",
    "                v, dtype=torch.float32, device=device).to(dtype)\n",
    "        elif k in [\"coordinates\", \"labels\", \"num_points\"]:\n",
    "            example_torch[k] = torch.tensor(\n",
    "                v, dtype=torch.int32, device=device)\n",
    "        elif k in [\"anchors_mask\"]:\n",
    "            example_torch[k] = torch.tensor(\n",
    "                v, dtype=torch.uint8, device=device)\n",
    "        elif k == \"calib\":\n",
    "            calib = {}\n",
    "            for k1, v1 in v.items():\n",
    "                calib[k1] = torch.tensor(\n",
    "                    v1, dtype=dtype, device=device).to(dtype)\n",
    "            example_torch[k] = calib\n",
    "        elif k == \"num_voxels\":\n",
    "            example_torch[k] = torch.tensor(v)\n",
    "        else:\n",
    "            example_torch[k] = v\n",
    "    return example_torch\n",
    "\n",
    "\n",
    "def time_to_str(t, mode='min'):\n",
    "    if mode == 'min':\n",
    "        t = int(t) / 60\n",
    "        hr = t // 60\n",
    "        min = t % 60\n",
    "        return '%2d hr %02d m' % (hr, min)\n",
    "\n",
    "    elif mode == 'sec':\n",
    "        t = int(t)\n",
    "        min = t // 60\n",
    "        sec = t % 60\n",
    "        return '%2d min %02d sec' % (min, sec)\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '/kaggle/code/ConeDetectionPointpillars/customNuscenes/configs/cones_pp_v4.config'\n",
    "# config_path = '/kaggle/code/ConeDetectionPointpillars/customNuscenes/configs/cones_pp_initial_v2.config'\n",
    "# config_path = '/kaggle/code/ConeDetectionPointpillars/customNuscenes/configs/cones_pp_initialak.config'\n",
    "model_dir = f'/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/cones_pp_v4'\n",
    "result_path = None\n",
    "create_folder = False\n",
    "display_step = 50\n",
    "# pretrained_path=\"/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_9:53/voxelnet-5850.tckpt\"\n",
    "pretrained_path = \"/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/cones_pp_v4/2021-02-11_09:50/voxelnet-15000.tckpt\"\n",
    "multi_gpu = False\n",
    "measure_time = False\n",
    "resume = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_dir = str(Path(model_dir).resolve())\n",
    "cur_time = time.localtime(time.time())\n",
    "cur_time = f'{cur_time.tm_year}-{cur_time.tm_mon}-{cur_time.tm_mday}_{cur_time.tm_hour}:{cur_time.tm_min}'\n",
    "if create_folder:\n",
    "    if Path(model_dir).exists():\n",
    "        model_dir = torchplus.train.create_folder(model_dir)\n",
    "model_dir = Path(model_dir)\n",
    "\n",
    "if not resume and model_dir.exists():\n",
    "    raise ValueError(\"model dir exists and you don't specify resume\")\n",
    "\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "if result_path is None:\n",
    "    result_path = model_dir / 'results'/ cur_time\n",
    "config_file_bkp = \"pipeline.config\"\n",
    "\n",
    "if isinstance(config_path, str):\n",
    "    config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "    with open(config_path, \"r\") as f:\n",
    "        proto_str = f.read()\n",
    "        text_format.Merge(proto_str, config)\n",
    "else:\n",
    "    config = config_path\n",
    "    proto_str = text_format.MessageToString(config, indent=2)\n",
    "\n",
    "with (model_dir / config_file_bkp).open('w') as f:\n",
    "    f.write(proto_str)\n",
    "# Read config file\n",
    "input_cfg = config.train_input_reader\n",
    "eval_input_cfg = config.eval_input_reader\n",
    "model_cfg = config.model.second  # model's config\n",
    "train_cfg = config.train_config  # training config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-fight",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build neural network\n",
    "net = build_network(model_cfg, measure_time).to(device)\n",
    "\n",
    "target_assigner = net.target_assigner\n",
    "voxel_generator = net.voxel_generator\n",
    "print(\"num parameter: \", len(list(net.parameters())))\n",
    "torchplus.train.try_restore_latest_checkpoints(model_dir, [net])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-traveler",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pretrained_path is not None:\n",
    "    print('warning pretrain is loaded after restore, careful with resume')\n",
    "    model_dict = net.state_dict()\n",
    "    pretrained_dict = torch.load(pretrained_path)\n",
    "\n",
    "    new_pretrained_dict = {}\n",
    "    for k, v in pretrained_dict.items():\n",
    "        if k in model_dict and v.shape == model_dict[k].shape:\n",
    "            new_pretrained_dict[k] = v\n",
    "    print(\"Load pretrained parameters: \")\n",
    "    for k, v in new_pretrained_dict.items():\n",
    "        print(k, v.shape)\n",
    "    model_dict.update(new_pretrained_dict)\n",
    "    net.load_state_dict(model_dict)\n",
    "    net.clear_global_step()\n",
    "    net.clear_metrics()\n",
    "if multi_gpu:\n",
    "    net_parallel = torch.nn.DataParallel(net)\n",
    "else:\n",
    "    net_parallel = net\n",
    "optimizer_cfg = train_cfg.optimizer\n",
    "loss_scale = train_cfg.loss_scale_factor\n",
    "fastai_optimizer = optimizer_builder.build(\n",
    "    optimizer_cfg,\n",
    "    net,\n",
    "    mixed=False,\n",
    "    loss_scale=loss_scale)\n",
    "if loss_scale < 0:\n",
    "    loss_scale = \"dynamic\"\n",
    "if train_cfg.enable_mixed_precision:\n",
    "    max_num_voxels = input_cfg.preprocess.max_number_of_voxels * input_cfg.batch_size\n",
    "    print(\"max_num_voxels: %d\" % (max_num_voxels))\n",
    "\n",
    "    net, amp_optimizer = amp.initialize(net, fastai_optimizer,\n",
    "                                        opt_level=\"O1\",\n",
    "                                        keep_batchnorm_fp32=None,\n",
    "                                        loss_scale=loss_scale)\n",
    "    net.metrics_to_float()\n",
    "else:\n",
    "    amp_optimizer = fastai_optimizer\n",
    "torchplus.train.try_restore_latest_checkpoints(model_dir, [fastai_optimizer])\n",
    "lr_scheduler = lr_scheduler_builder.build(optimizer_cfg, amp_optimizer, train_cfg.steps)\n",
    "\n",
    "if train_cfg.enable_mixed_precision:\n",
    "    float_dtype = torch.float16\n",
    "else:\n",
    "    float_dtype = torch.float32\n",
    "\n",
    "if multi_gpu:\n",
    "    num_gpu = torch.cuda.device_count()\n",
    "    print(f\"MULTI_GPU: use {num_gpu} gpus\")\n",
    "    collate_fn = merge_second_batch_multigpu\n",
    "else:\n",
    "    collate_fn = merge_second_batch\n",
    "    num_gpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-arrival",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = input_reader_builder.build(\n",
    "    input_cfg,\n",
    "    model_cfg,\n",
    "    training=True,\n",
    "    voxel_generator=voxel_generator,\n",
    "    target_assigner=target_assigner,\n",
    "    multi_gpu=multi_gpu\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=input_cfg.batch_size * num_gpu,\n",
    "    shuffle=True,\n",
    "    num_workers=input_cfg.preprocess.num_workers * num_gpu,\n",
    "    pin_memory=False,\n",
    "    collate_fn=collate_fn,\n",
    "    worker_init_fn=_worker_init_fn,\n",
    "    drop_last=not multi_gpu\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-atmosphere",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-soldier",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset[1]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [line for line in dataset[0]['reg_targets'] for i in line if i != 0]\n",
    "# res = []\n",
    "# for line in dataset[1]['reg_targets']:\n",
    "#     for i in line:\n",
    "#         if i != 0:\n",
    "#             print(line)\n",
    "#             res.append(line)\n",
    "#             break\n",
    "# #     break\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([i for i in dataset[1]['labels'] if i == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-harvest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-poverty",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eval_dataset = input_reader_builder.build(\n",
    "    eval_input_cfg,\n",
    "    model_cfg,\n",
    "    training=False,\n",
    "    voxel_generator=voxel_generator,\n",
    "    target_assigner=target_assigner)\n",
    "eval_dataloader = torch.utils.data.DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=input_cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=eval_input_cfg.preprocess.num_workers,\n",
    "    pin_memory=False,\n",
    "    collate_fn=merge_second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-trade",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-honey",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_dir = model_dir / cur_time\n",
    "model_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-cologne",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-arizona",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model_logging = SimpleModelLog(model_dir)\n",
    "model_logging.open()\n",
    "model_logging.log_text(proto_str + \"\\n\", 0, tag=\"config\")\n",
    "\n",
    "start_step = net.get_global_step()\n",
    "total_step = train_cfg.steps\n",
    "t = time.time()\n",
    "steps_per_eval = train_cfg.steps_per_eval\n",
    "clear_metrics_every_epoch = train_cfg.clear_metrics_every_epoch\n",
    "\n",
    "amp_optimizer.zero_grad()\n",
    "step_times = []\n",
    "step = start_step\n",
    "run = True\n",
    "ave_valid_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-lebanon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ver2\n",
    "try:\n",
    "    start_tic = time.time()\n",
    "    print(\"num samples: %d\" % (len(dataset)))\n",
    "    while run == True:\n",
    "        if clear_metrics_every_epoch:\n",
    "            net.clear_metrics()\n",
    "        for example in tqdm_notebook(dataloader):\n",
    "            lr_scheduler.step(net.get_global_step())\n",
    "            example.pop(\"metrics\")\n",
    "            example_torch = example_convert_to_torch(example, float_dtype)\n",
    "\n",
    "            ret_dict = net_parallel(example_torch)\n",
    "            loss = ret_dict[\"loss\"].mean()\n",
    "            cls_loss_reduced = ret_dict[\"cls_loss_reduced\"].mean()\n",
    "            loc_loss_reduced = ret_dict[\"loc_loss_reduced\"].mean()\n",
    "\n",
    "            if train_cfg.enable_mixed_precision:\n",
    "                if net.get_global_step() < 100:\n",
    "                    loss *= 1e-3\n",
    "                with amp.scale_loss(loss, amp_optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), 10.0)\n",
    "            amp_optimizer.step()\n",
    "            amp_optimizer.zero_grad()\n",
    "            net.update_global_step()\n",
    "\n",
    "            cls_preds = ret_dict[\"cls_preds\"]\n",
    "            labels = example_torch[\"labels\"]\n",
    "            cared = ret_dict[\"cared\"]\n",
    "\n",
    "            net_metrics = net.update_metrics(cls_loss_reduced,\n",
    "                                             loc_loss_reduced, cls_preds,\n",
    "                                             labels, cared)\n",
    "            step_time = (time.time() - t)\n",
    "            step_times.append(step_time)\n",
    "            t = time.time()\n",
    "            metrics = {}\n",
    "            global_step = net.get_global_step()\n",
    "\n",
    "            if global_step % display_step == 0:\n",
    "                net.eval()\n",
    "                det = net(example_torch)\n",
    "                print(det[0]['label_preds'])\n",
    "                print(det[0]['scores'])\n",
    "                net.train()\n",
    "                eta = time.time() - start_tic\n",
    "                if measure_time:\n",
    "                    for name, val in net.get_avg_time_dict().items():\n",
    "                        print(f\"avg {name} time = {val * 1000:.3f} ms\")\n",
    "\n",
    "                metrics[\"step\"] = global_step\n",
    "                metrics['epoch'] = global_step / len(dataloader)\n",
    "                metrics['steptime'] = np.mean(step_times)\n",
    "                metrics['valid'] = ave_valid_loss\n",
    "                step_times = []\n",
    "\n",
    "                metrics[\"loss\"] = net_metrics['loss']['cls_loss'] + net_metrics['loss']['loc_loss']\n",
    "                metrics[\"cls_loss\"] = net_metrics['loss']['cls_loss']\n",
    "                metrics[\"loc_loss\"] = net_metrics['loss']['loc_loss']\n",
    "\n",
    "                if model_cfg.use_direction_classifier:\n",
    "                    dir_loss_reduced = ret_dict[\"dir_loss_reduced\"].mean()\n",
    "                    metrics[\"dir_rt\"] = float(\n",
    "                        dir_loss_reduced.detach().cpu().numpy())\n",
    "\n",
    "                metrics['lr'] = float(amp_optimizer.lr)\n",
    "                metrics['eta'] = time_to_str(eta)\n",
    "                model_logging.log_metrics(metrics, global_step)\n",
    "\n",
    "                net.clear_metrics()\n",
    "            if global_step % steps_per_eval == 0:\n",
    "                torchplus.train.save_models(model_dir, [net, amp_optimizer],\n",
    "                                            net.get_global_step())\n",
    "                model_logging.log_text(f\"Model saved: {model_dir}, {net.get_global_step()}\", global_step)\n",
    "                net.eval()\n",
    "                result_path_step = result_path / f\"step_{net.get_global_step()}\"\n",
    "                result_path_step.mkdir(parents=True, exist_ok=True)\n",
    "                model_logging.log_text(\"########################\", global_step)\n",
    "                model_logging.log_text(\" EVALUATE\", global_step)\n",
    "                model_logging.log_text(\"########################\", global_step)\n",
    "                model_logging.log_text(\"Generating eval predictions...\", global_step)\n",
    "                t = time.time()\n",
    "                detections = []\n",
    "                prog_bar = ProgressBar()\n",
    "                net.clear_timer()\n",
    "                cnt = 0\n",
    "                for example in tqdm_notebook(iter(eval_dataloader)):\n",
    "                    example = example_convert_to_torch(example, float_dtype)\n",
    "                    detections += net(example)\n",
    "                sec_per_ex = len(eval_dataset) / (time.time() - t)\n",
    "                model_logging.log_text(\n",
    "                    f'generate eval predictions finished({sec_per_ex:.2f}/s). Start eval:',\n",
    "                    global_step)\n",
    "                result_dict = eval_dataset.dataset.evaluation(\n",
    "                    detections, str(result_path_step)\n",
    "                )\n",
    "                for k, v in result_dict['results'].items():\n",
    "                    model_logging.log_text(f\"Evaluation {k}\", global_step)\n",
    "                    model_logging.log_text(v, global_step)\n",
    "                model_logging.log_metrics(result_dict[\"detail\"], global_step)\n",
    "                with open(result_path_step/\"result.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(detections,f)\n",
    "                net.train()\n",
    "\n",
    "            step += 1\n",
    "            if step >= total_step:\n",
    "                break\n",
    "        if step >= total_step:\n",
    "            break\n",
    "except Exception as e:\n",
    "    model_logging.log_text(str(e), step)\n",
    "    model_logging.log_text(json.dumps(example['metadata'], indent=2), step)\n",
    "    torchplus.train.save_models(model_dir, [net, amp_optimizer], step)\n",
    "    raise e\n",
    "finally:\n",
    "    model_logging.close()\n",
    "torchplus.train.save_models(model_dir, [net, amp_optimizer], step)\n",
    "# python /kaggle/code/ConeDetectionPointpillars/second/data/nusc_eval.py --root_path=\"/media/starlet/LdTho/data/sets/nuscenes/v1.0-trainval\" --version=\"v1.0-trainval\" --eval_version=detection_cvpr_2019 --res_path=\"/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/results/step_20/result_nusc.json\" --eval_set=val --output_dir=\"/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/results/step_20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver 1\n",
    "# try:\n",
    "#     start_tic = time.time()\n",
    "#     print(\"num samples: %d\" % (len(dataset)))\n",
    "#     while run == True:\n",
    "#         if clear_metrics_every_epoch:\n",
    "#             net.clear_metrics()\n",
    "#         for example in tqdm_notebook(dataloader):\n",
    "#             lr_scheduler.step(net.get_global_step())\n",
    "#             example.pop(\"metrics\")\n",
    "#             example_torch = example_convert_to_torch(example, float_dtype)\n",
    "\n",
    "#             ret_dict = net_parallel(example_torch)\n",
    "#             loss = ret_dict[\"loss\"].mean()\n",
    "#             cls_loss_reduced = ret_dict[\"cls_loss_reduced\"].mean()\n",
    "#             loc_loss_reduced = ret_dict[\"loc_loss_reduced\"].mean()\n",
    "\n",
    "#             if train_cfg.enable_mixed_precision:\n",
    "#                 if net.get_global_step() < 100:\n",
    "#                     loss *= 1e-3\n",
    "#                 with amp.scale_loss(loss, amp_optimizer) as scaled_loss:\n",
    "#                     scaled_loss.backward()\n",
    "#             else:\n",
    "#                 loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(net.parameters(), 10.0)\n",
    "#             amp_optimizer.step()\n",
    "#             amp_optimizer.zero_grad()\n",
    "#             net.update_global_step()\n",
    "\n",
    "#             cls_preds = ret_dict[\"cls_preds\"]\n",
    "#             labels = example_torch[\"labels\"]\n",
    "#             cared = ret_dict[\"cared\"]\n",
    "\n",
    "#             net_metrics = net.update_metrics(cls_loss_reduced,\n",
    "#                                              loc_loss_reduced, cls_preds,\n",
    "#                                              labels, cared)\n",
    "#             step_time = (time.time() - t)\n",
    "#             step_times.append(step_time)\n",
    "#             t = time.time()\n",
    "#             metrics = {}\n",
    "#             global_step = net.get_global_step()\n",
    "\n",
    "#             if global_step % display_step == 0:\n",
    "#                 net.eval()\n",
    "#                 det = net(example_torch)\n",
    "#                 print(det[0]['label_preds'])\n",
    "#                 print(det[0]['scores'])      \n",
    "#                 net.train()\n",
    "#                 eta = time.time() - start_tic\n",
    "#                 if measure_time:\n",
    "#                     for name, val in net.get_avg_time_dict().items():\n",
    "#                         print(f\"avg {name} time = {val * 1000:.3f} ms\")\n",
    "\n",
    "#                 metrics[\"step\"] = global_step\n",
    "#                 metrics['epoch'] = global_step / len(dataloader)\n",
    "#                 metrics['steptime'] = np.mean(step_times)\n",
    "#                 metrics['valid'] = ave_valid_loss\n",
    "#                 step_times = []\n",
    "\n",
    "#                 metrics[\"loss\"] = net_metrics['loss']['cls_loss'] + net_metrics['loss']['loc_loss']\n",
    "#                 metrics[\"cls_loss\"] = net_metrics['loss']['cls_loss']\n",
    "#                 metrics[\"loc_loss\"] = net_metrics['loss']['loc_loss']\n",
    "\n",
    "#                 if model_cfg.use_direction_classifier:\n",
    "#                     dir_loss_reduced = ret_dict[\"dir_loss_reduced\"].mean()\n",
    "#                     metrics[\"dir_rt\"] = float(\n",
    "#                         dir_loss_reduced.detach().cpu().numpy())\n",
    "\n",
    "#                 metrics['lr'] = float(amp_optimizer.lr)\n",
    "#                 metrics['eta'] = time_to_str(eta)\n",
    "#                 model_logging.log_metrics(metrics, global_step)\n",
    "\n",
    "#                 net.clear_metrics()\n",
    "#             if global_step % steps_per_eval == 0:\n",
    "#                 torchplus.train.save_models(model_dir, [net, amp_optimizer],\n",
    "#                                             net.get_global_step())\n",
    "#                 model_logging.log_text(f\"Model saved: {model_dir}, {net.get_global_step()}\", global_step)\n",
    "#                 net.eval()\n",
    "#                 result_path_step = result_path / f\"step_{net.get_global_step()}\"\n",
    "#                 result_path_step.mkdir(parents=True, exist_ok=True)\n",
    "#                 model_logging.log_text(\"########################\", global_step)\n",
    "#                 model_logging.log_text(\" EVALUATE\",global_step)\n",
    "#                 model_logging.log_text(\"########################\", global_step)\n",
    "#                 model_logging.log_text(\"Generating eval images...\", global_step)\n",
    "#                 t = time.time()\n",
    "#                 detections = []\n",
    "#                 prog_bar = ProgressBar()\n",
    "#                 net.clear_timer()\n",
    "#                 cnt = 0\n",
    "#                 for example in iter(dataloader):\n",
    "#                     cnt += 1\n",
    "#                     detection = example_convert_to_torch(example, float_dtype)\n",
    "#                     detection = net(detection)\n",
    "# #                     filtered_sample_tokens = eval_dataset.dataset.filtered_sample_tokens\n",
    "#                     filtered_sample_tokens = dataset.dataset.filtered_sample_tokens\n",
    "#                     index = filtered_sample_tokens.index(detection[0]['metadata']['token'])\n",
    "#                     det_boxes = detection[0]['box3d_lidar'].cpu().detach().numpy()\n",
    "#                     det_labels = detection[0]['label_preds'].cpu().detach().numpy()\n",
    "#                     det_scores = detection[0]['scores'].cpu().detach().numpy()\n",
    "                    \n",
    "#                     det_boxes_cones = det_boxes[det_labels == 0]\n",
    "#                     det_scores_cones = det_scores[det_labels == 0]\n",
    "#                     det_labels_cones = det_labels[det_labels == 0]\n",
    "                    \n",
    "#                     det_boxes_pedes = det_boxes[det_labels == 1]\n",
    "#                     det_scores_pedes = det_scores[det_labels == 1]\n",
    "#                     det_labels_pedes = det_labels[det_labels == 1]\n",
    "# #                     gt_example = eval_dataset.dataset.get_sensor_data(index)\n",
    "#                     gt_example = dataset.dataset.get_sensor_data(index)\n",
    "#                     points = gt_example['lidar']['points']\n",
    "        \n",
    "#                     gt_boxes = gt_example['lidar']['annotations']['boxes']\n",
    "#                     gt_labels = gt_example['lidar']['annotations']['names']\n",
    "#                     gt_boxes_cones = gt_boxes[gt_labels == \"traffic_cone\"]\n",
    "#                     gt_labels_cones = gt_labels[gt_labels == \"traffic_cone\"]\n",
    "\n",
    "#                     gt_boxes_pedes = gt_boxes[gt_labels == \"pedestrian\"]\n",
    "#                     gt_labels_pedes = gt_labels[gt_labels == \"pedestrian\"]\n",
    "\n",
    "#                     gt_scores = np.ones(len(gt_labels_cones))\n",
    "#                     c = points[:, 3].reshape(-1, 1)\n",
    "#                     c = np.concatenate([c, c, c], axis=1)\n",
    "#                     points = points[:, 0:3]\n",
    "#                     pc = o3d.geometry.PointCloud()\n",
    "#                     pc.points = o3d.utility.Vector3dVector(points)\n",
    "#                     pc.colors = o3d.utility.Vector3dVector(c)\n",
    "#                     mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=2.0,\n",
    "#                                                                                    origin=[-0, -0, -0])\n",
    "#                     geo = [pc, mesh_frame]\n",
    "#                     rbbox_corners_cones = box_np_ops.center_to_corner_box3d(det_boxes_cones[:, :3],\n",
    "#                                                                       det_boxes_cones[:, 3:6],\n",
    "#                                                                       det_boxes_cones[:, 6],\n",
    "#                                                                       origin=(0.5, 0.5, 0.5), axis=2)\n",
    "#                     rbbox_corners_pedes = box_np_ops.center_to_corner_box3d(det_boxes_pedes[:, :3],\n",
    "#                                                                       det_boxes_pedes[:, 3:6],\n",
    "#                                                                       det_boxes_pedes[:, 6],\n",
    "#                                                                       origin=(0.5, 0.5, 0.5), axis=2)\n",
    "#                     gt_cones_rbbox_corners = box_np_ops.center_to_corner_box3d(gt_boxes_cones[:, :3],\n",
    "#                                                                          gt_boxes_cones[:, 3:6],\n",
    "#                                                                          gt_boxes_cones[:, 6],\n",
    "#                                                                          origin=(0.5, 0.5, 0.5), axis=2)\n",
    "#                     gt_pedes_rbbox_corners = box_np_ops.center_to_corner_box3d(gt_boxes_pedes[:, :3],\n",
    "#                                                                          gt_boxes_pedes[:, 3:6],\n",
    "#                                                                          gt_boxes_pedes[:, 6],\n",
    "#                                                                          origin=(0.5, 0.5, 0.5), axis=2)\n",
    "#                     for i in range(len(rbbox_corners_cones)):\n",
    "#                         geo.append(buildBBox(rbbox_corners_cones[i], color=[1, 0, 0]))\n",
    "#                     for i in range(len(rbbox_corners_pedes)):\n",
    "#                         geo.append(buildBBox(rbbox_corners_pedes[i], color=[1, 1, 0]))\n",
    "#                     for i in range(len(gt_cones_rbbox_corners)):\n",
    "#                         geo.append(buildBBox(gt_cones_rbbox_corners[i], color=[0, 1, 0]))\n",
    "#                     for i in range(len(gt_pedes_rbbox_corners)):\n",
    "#                         geo.append(buildBBox(gt_pedes_rbbox_corners[i], color=[0, 0, 1]))\n",
    "# #                     o3d.visualization.draw_geometries(geo)\n",
    "#                     vis = o3d.visualization.Visualizer()\n",
    "#                     vis.create_window(visible=True)\n",
    "#                     for i in range(len(geo)):\n",
    "#                         vis.add_geometry(geo[i])\n",
    "#                         vis.update_geometry(geo[i])\n",
    "#                     vis.poll_events()\n",
    "#                     vis.update_renderer()\n",
    "#                     img_dir = str(Path(f'{model_dir}/images/{global_step}').resolve())\n",
    "#                     img_dir = Path(img_dir)\n",
    "#                     img_dir.mkdir(parents=True, exist_ok=True)\n",
    "#                     vis.capture_screen_image(f'{model_dir}/images/{global_step}/eval_{cnt}.png')\n",
    "#                     model_logging.log_text(f\"eval images saved at {model_dir}/images/{global_step}/\", global_step)\n",
    "#                     vis.destroy_window()\n",
    "#                     if cnt >= 5:\n",
    "#                         break\n",
    "#                 if 1 in detection[0]['label_preds']:\n",
    "#                     print(detection[0]['label_preds'])\n",
    "#                     print(detection[0]['scores'])\n",
    "#                 sec_per_ex = len(eval_dataset) / (time.time() - t)\n",
    "#                 model_logging.log_text(\n",
    "#                     f'generate eval images finished({sec_per_ex:.2f}/s). continue training:',\n",
    "#                     global_step)\n",
    "#                 net.train()\n",
    "                \n",
    "#             step += 1\n",
    "#             if step >= total_step:\n",
    "#                 break\n",
    "#         if step >= total_step:\n",
    "#             break\n",
    "# except Exception as e:\n",
    "#     model_logging.log_text(str(e), step)\n",
    "#     model_logging.log_text(json.dumps(example['metadata'], indent=2), step)\n",
    "#     torchplus.train.save_models(model_dir, [net, amp_optimizer], step)\n",
    "#     raise e\n",
    "# finally:\n",
    "#     model_logging.close()\n",
    "# torchplus.train.save_models(model_dir, [net, amp_optimizer], step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for example in tqdm(eval_dataloader):\n",
    "    example = example_convert_to_torch(example, float_dtype)\n",
    "    det = net(example)\n",
    "#     if len(det['label_preds']) > 0:\n",
    "    print(det)\n",
    "    detections += det\n",
    "    c += 1\n",
    "    if c == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randint(1,100,(1,))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset.dataset.evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-accommodation",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/media/starlet/LdTho/data/sets/nuscenes/v1.0-trainval/nusc_custom_dbinfos_train.pkl', 'rb') as f:\n",
    "    cus_info_file = pickle.load(f)\n",
    "with open('/media/starlet/LdTho/data/sets/nuscenes/v1.0-trainval/infos_train.pkl', 'rb') as f:\n",
    "    info_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "cus_info_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-intake",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-garden",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-correction",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[1]['num_points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-dining",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "caring-ownership",
   "metadata": {},
   "source": [
    "## Visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-acoustic",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-antigua",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_labels in [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(iter(dataloader))\n",
    "det = net(example_convert_to_torch(example, float_dtype))\n",
    "filtered_sample_tokens = dataset.dataset.filtered_sample_tokens\n",
    "index = filtered_sample_tokens.index(det[0]['metadata']['token'])\n",
    "det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_boxes = det[0]['box3d_lidar'].cpu().detach().numpy()\n",
    "det_labels = det[0]['label_preds'].cpu().detach().numpy()\n",
    "det_scores = det[0]['scores'].cpu().detach().numpy()\n",
    "token = det[0]['metadata']\n",
    "example = dataset.dataset.get_sensor_data(index)\n",
    "points = example['lidar']['points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-mixer",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = example['lidar']['annotations']['boxes']\n",
    "labels = example['lidar']['annotations']['names']\n",
    "scores = np.ones(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = det_boxes\n",
    "pc = o3d.geometry.PointCloud()\n",
    "c = points[:, 3].reshape(-1, 1)\n",
    "c = np.concatenate([c, c, c], axis=1)\n",
    "pc.points = o3d.utility.Vector3dVector(points)\n",
    "pc.colors = o3d.utility.Vector3dVector(c)\n",
    "mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size = 2.0, origin = [-0,-0,-0])\n",
    "geo = [pc, mesh_frame]\n",
    "rbbox_corners = box_np_ops.center_to_corner_box3d(boxes[:,:3],\n",
    "                                                 boxes[:,3:6],\n",
    "                                                 boxes[:,6],\n",
    "                                                 origin=(0.5,0.5,0.5), axis = 2)\n",
    "\n",
    "for i in range(boxes.shape[0]):\n",
    "    geo.append(buildBBox(rbbox_corners[i], color = [1,0,0]))\n",
    "print(geo)\n",
    "o3d.visualization.draw_geometries(geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-rings",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-enclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "# example = next(iter(dataloader))\n",
    "example = next(iter(eval_dataloader))\n",
    "\n",
    "detection = example_convert_to_torch(example, float_dtype)\n",
    "detection = net(detection)\n",
    "print(detection)\n",
    "filtered_sample_tokens = eval_dataset.dataset.filtered_sample_tokens\n",
    "# filtered_sample_tokens = dataset.dataset.filtered_sample_tokens\n",
    "index = filtered_sample_tokens.index(detection[0]['metadata']['token'])\n",
    "det_boxes = detection[0]['box3d_lidar'].cpu().detach().numpy()\n",
    "det_labels = detection[0]['label_preds'].cpu().detach().numpy()\n",
    "det_scores = detection[0]['scores'].cpu().detach().numpy()\n",
    "\n",
    "det_boxes_cones = det_boxes[det_labels == 0]\n",
    "det_scores_cones = det_scores[det_labels == 0]\n",
    "det_labels_cones = det_labels[det_labels == 0]\n",
    "\n",
    "det_boxes_pedes = det_boxes[det_labels == 1]\n",
    "det_scores_pedes = det_scores[det_labels == 1]\n",
    "det_labels_pedes = det_labels[det_labels == 1]\n",
    "gt_example = eval_dataset.dataset.get_sensor_data(index)\n",
    "# gt_example = dataset.dataset.get_sensor_data(index)\n",
    "points = gt_example['lidar']['points']\n",
    "pc_range = model_cfg.voxel_generator.point_cloud_range\n",
    "points = np.array([p for p in points if (pc_range[0] < p[0] < pc_range[3]) & (pc_range[1] < p[1] < pc_range[4]) & (pc_range[2] < p[2] < pc_range[5])])\n",
    "\n",
    "gt_boxes = gt_example['lidar']['annotations']['boxes']\n",
    "gt_labels = gt_example['lidar']['annotations']['names']\n",
    "gt_boxes_cones = gt_boxes[gt_labels == \"traffic_cone\"]\n",
    "gt_labels_cones = gt_labels[gt_labels == \"traffic_cone\"]\n",
    "\n",
    "gt_boxes_pedes = gt_boxes[gt_labels == \"pedestrian\"]\n",
    "gt_labels_pedes = gt_labels[gt_labels == \"pedestrian\"]\n",
    "\n",
    "gt_scores = np.ones(len(gt_labels_cones))\n",
    "c = points[:, 3].reshape(-1, 1)\n",
    "c = np.concatenate([c, c, c], axis=1)\n",
    "points = points[:, 0:3]\n",
    "\n",
    "pc = o3d.geometry.PointCloud()\n",
    "pc.points = o3d.utility.Vector3dVector(points)\n",
    "pc.colors = o3d.utility.Vector3dVector(c)\n",
    "mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=2.0,\n",
    "                                                               origin=[-0, -0, -0])\n",
    "geo = [pc, mesh_frame]\n",
    "rbbox_corners_cones = box_np_ops.center_to_corner_box3d(det_boxes_cones[:, :3],\n",
    "                                                  det_boxes_cones[:, 3:6],\n",
    "                                                  det_boxes_cones[:, 6],\n",
    "                                                  origin=(0.5, 0.5, 0.5), axis=2)\n",
    "rbbox_corners_pedes = box_np_ops.center_to_corner_box3d(det_boxes_pedes[:, :3],\n",
    "                                                  det_boxes_pedes[:, 3:6],\n",
    "                                                  det_boxes_pedes[:, 6],\n",
    "                                                  origin=(0.5, 0.5, 0.5), axis=2)\n",
    "gt_cones_rbbox_corners = box_np_ops.center_to_corner_box3d(gt_boxes_cones[:, :3],\n",
    "                                                     gt_boxes_cones[:, 3:6],\n",
    "                                                     gt_boxes_cones[:, 6],\n",
    "                                                     origin=(0.5, 0.5, 0.5), axis=2)\n",
    "gt_pedes_rbbox_corners = box_np_ops.center_to_corner_box3d(gt_boxes_pedes[:, :3],\n",
    "                                                     gt_boxes_pedes[:, 3:6],\n",
    "                                                     gt_boxes_pedes[:, 6],\n",
    "                                                     origin=(0.5, 0.5, 0.5), axis=2)\n",
    "for i in range(len(rbbox_corners_cones)):\n",
    "    geo.append(buildBBox(rbbox_corners_cones[i], color=[1, 0, 0]))\n",
    "for i in range(len(rbbox_corners_pedes)):\n",
    "    geo.append(buildBBox(rbbox_corners_pedes[i], color=[1, 0, 1]))\n",
    "for i in range(len(gt_cones_rbbox_corners)):\n",
    "    geo.append(buildBBox(gt_cones_rbbox_corners[i], color=[0, 1, 0]))\n",
    "for i in range(len(gt_pedes_rbbox_corners)):\n",
    "    geo.append(buildBBox(gt_pedes_rbbox_corners[i], color=[0, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries(geo)\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_range = model_cfg.voxel_generator.point_cloud_range\n",
    "pc_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = [p for p in points if (pc_range[0] < p[0] < pc_range[3]) & (pc_range[1] < p[1] < pc_range[4]) & (pc_range[2] < p[2] < pc_range[5])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _second_det_to_nusc_box(detection):\n",
    "    from nuscenes.utils.data_classes import Box\n",
    "    import pyquaternion\n",
    "    box3d = detection[\"box3d_lidar\"].detach().cpu().numpy()\n",
    "    scores = detection[\"scores\"].detach().cpu().numpy()\n",
    "    labels = detection[\"label_preds\"].detach().cpu().numpy()\n",
    "    print(labels)\n",
    "    box3d[:, 6] = -box3d[:, 6] - np.pi / 2\n",
    "    box_list = []\n",
    "    for i in range(box3d.shape[0]):\n",
    "        quat = pyquaternion.Quaternion(axis=[0, 0, 1], radians=box3d[i, 6])\n",
    "        velocity = (np.nan, np.nan, np.nan)\n",
    "        if box3d.shape[1] == 9:\n",
    "            velocity = (*box3d[i, 7:9], 0.0)\n",
    "            # velo_val = np.linalg.norm(box3d[i, 7:9])\n",
    "            # velo_ori = box3d[i, 6]\n",
    "            # velocity = (velo_val * np.cos(velo_ori), velo_val * np.sin(velo_ori), 0.0)\n",
    "        box = Box(\n",
    "            box3d[i, :3],\n",
    "            box3d[i, 3:6],\n",
    "            quat,\n",
    "            label=labels[i],\n",
    "            score=scores[i],\n",
    "            velocity=velocity)\n",
    "        box_list.append(box)\n",
    "    return box_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-reggae",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_list = _second_det_to_nusc_box(detection[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation_nusc\n",
    "import json\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-johnston",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/eval_dev_test\"\n",
    "mapped_class_names = {0: \"traffic_cone\", 1: \"pedestrian\"}\n",
    "nusc_annos = {}\n",
    "for det in detection:\n",
    "    annos = []\n",
    "    boxes = _second_det_to_nusc_box(det)\n",
    "    for i, box in enumerate(boxes):\n",
    "        name = mapped_class_names[box.label]\n",
    "        velocity = np.nan\n",
    "        nusc_anno = {\n",
    "            \"sample_token\": det[\"metadata\"][\"token\"],\n",
    "            \"translation\": box.center.tolist(),\n",
    "            \"size\": box.wlh.tolist(),\n",
    "            \"rotation\": box.orientation.elements.tolist(),\n",
    "            \"velocity\": velocity,\n",
    "            \"detection_name\":name,\n",
    "            \"detection_score\": box.score,\n",
    "            \"attribute_name\":DefaultAttribute [name]\n",
    "        }\n",
    "        annos.append(nusc_anno)\n",
    "    nusc_annos[det[\"metadata\"][\"token\"]] = annos\n",
    "nusc_submissions = {\n",
    "    \"meta\": {\n",
    "        \"use_camera\": False,\n",
    "        \"use_lidar\": False,\n",
    "        \"use_radar\": False,\n",
    "        \"use_map\": False,\n",
    "        \"use_external\": False\n",
    "    },\n",
    "    \"result\": nusc_annos\n",
    "}\n",
    "res_path = Path(output_dir)/\"result_nusc.json\"\n",
    "with open(res_path,\"wb\") as f:\n",
    "    json.dump(nusc_submissions,f)\n",
    "eval_main_file = Path(__file__).resolve().parent / \"nusc_eval.py\"\n",
    "cmd = f\"python {str(eval_main_file)} --root_path=/media/starlet/LdTho/data/sets/nuscenes/v1.0-trainval\"\n",
    "cmd += f\" --version=trainval --eval_version=cvpr_2019\"\n",
    "cmd += f\" --res_path=\\\"{str(res_path)}\\\" --eval_set=val\"\n",
    "cmd += f\" --output_dir=\\\"{output_dir}\\\"\"\n",
    "subprocess.check_output(cmd, shell = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-allah",
   "metadata": {},
   "outputs": [],
   "source": [
    "nusc_submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-sheep",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-kennedy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-tours",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-cowboy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-thesis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-stranger",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-curve",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "removable-trigger",
   "metadata": {},
   "outputs": [],
   "source": [
    "NameMappingInverse = {\n",
    "    \"barrier\": \"movable_object.barrier\",\n",
    "    \"bicycle\": \"vehicle.bicycle\",\n",
    "    \"bus\": \"vehicle.bus.rigid\",\n",
    "    \"car\": \"vehicle.car\",\n",
    "    \"construction_vehicle\": \"vehicle.construction\",\n",
    "    \"motorcycle\": \"vehicle.motorcycle\",\n",
    "    \"pedestrian\": \"human.pedestrian.adult\",\n",
    "    \"traffic_cone\": \"movable_object.trafficcone\",\n",
    "    \"trailer\": \"vehicle.trailer\",\n",
    "    \"truck\": \"vehicle.truck\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "lasting-conservative",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_evaluation(config_path, model_dir, pretrained_path, multi_gpu=False):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if isinstance(config_path, str):\n",
    "        config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "        with open(config_path, \"r\") as f:\n",
    "            proto_str = f.read()\n",
    "            text_format.Merge(proto_str, config)\n",
    "    else:\n",
    "        config = config_path\n",
    "        proto_str = text_format.MessageToString(config, indent=2)\n",
    "\n",
    "\n",
    "    # Read config file\n",
    "    input_cfg = config.train_input_reader\n",
    "    eval_input_cfg = config.eval_input_reader\n",
    "    model_cfg = config.model.second  # model's config\n",
    "    train_cfg = config.train_config\n",
    "\n",
    "    # Build neural network\n",
    "    net = build_network(model_cfg).to(device)\n",
    "\n",
    "    # Build Model\n",
    "    target_assigner = net.target_assigner\n",
    "    voxel_generator = net.voxel_generator\n",
    "    print(\"num parameter: \", len(list(net.parameters())))\n",
    "    torchplus.train.try_restore_latest_checkpoints(model_dir, [net])\n",
    "\n",
    "    if pretrained_path is not None:\n",
    "        print('warning pretrain is loaded after restore, careful with resume')\n",
    "        model_dict = net.state_dict()\n",
    "        pretrained_dict = torch.load(pretrained_path)\n",
    "\n",
    "        new_pretrained_dict = {}\n",
    "        for k, v in pretrained_dict.items():\n",
    "            if k in model_dict and v.shape == model_dict[k].shape:\n",
    "                new_pretrained_dict[k] = v\n",
    "        print(\"Load pretrained parameters: \")\n",
    "        for k, v in new_pretrained_dict.items():\n",
    "            print(k, v.shape)\n",
    "        model_dict.update(new_pretrained_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "        net.clear_global_step()\n",
    "        net.clear_metrics()\n",
    "    if multi_gpu:\n",
    "        net_parallel = torch.nn.DataParallel(net)\n",
    "    else:\n",
    "        net_parallel = net\n",
    "\n",
    "    optimizer_cfg = train_cfg.optimizer\n",
    "    loss_scale = train_cfg.loss_scale_factor\n",
    "    fastai_optimizer = optimizer_builder.build(\n",
    "        optimizer_cfg,\n",
    "        net,\n",
    "        mixed=False,\n",
    "        loss_scale=loss_scale)\n",
    "    if loss_scale < 0:\n",
    "        loss_scale = \"dynamic\"\n",
    "    if train_cfg.enable_mixed_precision:\n",
    "        max_num_voxels = input_cfg.preprocess.max_number_of_voxels * input_cfg.batch_size\n",
    "        print(\"max_num_voxels: %d\" % (max_num_voxels))\n",
    "\n",
    "        net, amp_optimizer = amp.initialize(net, fastai_optimizer,\n",
    "                                            opt_level=\"O1\",\n",
    "                                            keep_batchnorm_fp32=None,\n",
    "                                            loss_scale=loss_scale)\n",
    "        net.metrics_to_float()\n",
    "    else:\n",
    "        amp_optimizer = fastai_optimizer\n",
    "    torchplus.train.try_restore_latest_checkpoints(model_dir, [fastai_optimizer])\n",
    "    lr_scheduler = lr_scheduler_builder.build(optimizer_cfg, amp_optimizer, train_cfg.steps)\n",
    "    if train_cfg.enable_mixed_precision:\n",
    "        float_dtype = torch.float16\n",
    "    else:\n",
    "        float_dtype = torch.float32\n",
    "\n",
    "    if multi_gpu:\n",
    "        num_gpu = torch.cuda.device_count()\n",
    "        print(f\"MULTI_GPU: use {num_gpu} gpus\")\n",
    "        collate_fn = merge_second_batch_multigpu\n",
    "    else:\n",
    "        collate_fn = merge_second_batch\n",
    "        num_gpu = 1\n",
    "\n",
    "    eval_dataset = input_reader_builder.build(\n",
    "        eval_input_cfg,\n",
    "        model_cfg,\n",
    "        training=False,\n",
    "        voxel_generator=voxel_generator,\n",
    "        target_assigner=target_assigner)\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=input_cfg.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=eval_input_cfg.preprocess.num_workers,\n",
    "        pin_memory=False,\n",
    "        collate_fn=merge_second_batch)\n",
    "\n",
    "    # Start visualizing\n",
    "    net.eval()\n",
    "    # example = next(iter(dataloader))\n",
    "    for example in iter(eval_dataloader):\n",
    "        detection = example_convert_to_torch(example, float_dtype)\n",
    "        detection = net(detection)\n",
    "        print(detection)\n",
    "        filtered_sample_tokens = eval_dataset.dataset.filtered_sample_tokens\n",
    "        # filtered_sample_tokens = dataset.dataset.filtered_sample_tokens\n",
    "        index = filtered_sample_tokens.index(detection[0]['metadata']['token'])\n",
    "\n",
    "        gt_example = eval_dataset.dataset.get_sensor_data(index)\n",
    "        # gt_example = dataset.dataset.get_sensor_data(index)\n",
    "        points = gt_example['lidar']['points']\n",
    "        pc_range = model_cfg.voxel_generator.point_cloud_range\n",
    "        points = np.array(\n",
    "            [p for p in points if (pc_range[0] < p[0] < pc_range[3]) & (pc_range[1] < p[1] < pc_range[4]) & (\n",
    "                    pc_range[2] < p[2] < pc_range[5])])\n",
    "\n",
    "        gt_boxes = gt_example['lidar']['annotations']['boxes']\n",
    "        gt_labels = gt_example['lidar']['annotations']['names']\n",
    "        c = points[:, 3].reshape(-1, 1)\n",
    "        c = np.concatenate([c, c, c], axis=1)\n",
    "        points = points[:, 0:3]\n",
    "        pc = o3d.geometry.PointCloud()\n",
    "        pc.points = o3d.utility.Vector3dVector(points)\n",
    "        pc.colors = o3d.utility.Vector3dVector(c)\n",
    "        mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=2.0,\n",
    "                                                                       origin=[-0, -0, -0])\n",
    "        geo = [pc, mesh_frame]\n",
    "        geo = add_prediction_per_class(eval_dataset.dataset.nusc,\n",
    "                                       detection, gt_boxes, gt_labels,\n",
    "                                       target_assigner.classes, geo)\n",
    "        o3d.visualization.draw_geometries(geo)\n",
    "\n",
    "    net.train()\n",
    "\n",
    "color = {\n",
    "    \"traffic_cone\": (1,0,0)\n",
    "    \"gt_traffic_cone\": (0,1,0)\n",
    "    \"pedestrian\": (1,1,0)\n",
    "    \"gt_pedestrian\": (0,0,1)\n",
    "}\n",
    "\n",
    "def add_prediction_per_class(nusc, detection, gt_boxes, gt_labels, class_names, geometries):\n",
    "    det_boxes = detection[0]['box3d_lidar'].cpu().detach().numpy()\n",
    "    det_labels = detection[0]['label_preds'].cpu().detach().numpy()\n",
    "    det_scores = detection[0]['scores'].cpu().detach().numpy()\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        mask = np.logical_and(det_labels == i, det_scores > 0.7)\n",
    "        class_det_boxes = det_boxes[mask]\n",
    "        class_det_scores = det_scores[mask]\n",
    "        class_det_labels = det_labels[mask]\n",
    "        print(len(class_det_boxes),len(class_det_scores),len(class_det_labels))\n",
    "        print(class_det_scores)\n",
    "        class_gt_boxes = gt_boxes[gt_labels == class_name]\n",
    "        class_gt_labels = gt_labels[gt_labels == class_name]\n",
    "\n",
    "        rbbox_corners = box_np_ops.center_to_corner_box3d(class_det_boxes[:, :3],\n",
    "                                                          class_det_boxes[:, 3:6],\n",
    "                                                          class_det_boxes[:, 6],\n",
    "                                                          origin=(0.5, 0.5, 0.5), axis=2)\n",
    "        gt_rbbox_corners = box_np_ops.center_to_corner_box3d(class_gt_boxes[:, :3],\n",
    "                                                             class_gt_boxes[:, 3:6],\n",
    "                                                             class_gt_boxes[:, 6],\n",
    "                                                             origin=(0.5, 0.5, 0.5), axis=2)\n",
    "        for j in range(len(rbbox_corners)):\n",
    "            geometries.append(buildBBox(rbbox_corners[j],\n",
    "                                        color=np.array(nusc.colormap[NameMappingInverse[class_name]]) / 255))\n",
    "        color = [0, 0, 0]\n",
    "        color[i // 3] = 1\n",
    "        for j in range(len(gt_rbbox_corners)):\n",
    "            geometries.append(buildBBox(gt_rbbox_corners[j], color=color))\n",
    "    return geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sized-printing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num parameter:  64\n",
      "Restoring parameters from /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/cones_pp_v4/2021-2-11_9:58/voxelnet-9500.tckpt\n",
      "warning pretrain is loaded after restore, careful with resume\n",
      "Load pretrained parameters: \n",
      "global_step torch.Size([1])\n",
      "voxel_feature_extractor.pfn_layers.0.linear.weight torch.Size([64, 10])\n",
      "voxel_feature_extractor.pfn_layers.0.norm.weight torch.Size([64])\n",
      "voxel_feature_extractor.pfn_layers.0.norm.bias torch.Size([64])\n",
      "voxel_feature_extractor.pfn_layers.0.norm.running_mean torch.Size([64])\n",
      "voxel_feature_extractor.pfn_layers.0.norm.running_var torch.Size([64])\n",
      "voxel_feature_extractor.pfn_layers.0.norm.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.0.1.weight torch.Size([64, 64, 3, 3])\n",
      "rpn.blocks.0.2.weight torch.Size([64])\n",
      "rpn.blocks.0.2.bias torch.Size([64])\n",
      "rpn.blocks.0.2.running_mean torch.Size([64])\n",
      "rpn.blocks.0.2.running_var torch.Size([64])\n",
      "rpn.blocks.0.2.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.0.4.weight torch.Size([64, 64, 3, 3])\n",
      "rpn.blocks.0.5.weight torch.Size([64])\n",
      "rpn.blocks.0.5.bias torch.Size([64])\n",
      "rpn.blocks.0.5.running_mean torch.Size([64])\n",
      "rpn.blocks.0.5.running_var torch.Size([64])\n",
      "rpn.blocks.0.5.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.0.7.weight torch.Size([64, 64, 3, 3])\n",
      "rpn.blocks.0.8.weight torch.Size([64])\n",
      "rpn.blocks.0.8.bias torch.Size([64])\n",
      "rpn.blocks.0.8.running_mean torch.Size([64])\n",
      "rpn.blocks.0.8.running_var torch.Size([64])\n",
      "rpn.blocks.0.8.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.0.10.weight torch.Size([64, 64, 3, 3])\n",
      "rpn.blocks.0.11.weight torch.Size([64])\n",
      "rpn.blocks.0.11.bias torch.Size([64])\n",
      "rpn.blocks.0.11.running_mean torch.Size([64])\n",
      "rpn.blocks.0.11.running_var torch.Size([64])\n",
      "rpn.blocks.0.11.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.1.1.weight torch.Size([128, 64, 3, 3])\n",
      "rpn.blocks.1.2.weight torch.Size([128])\n",
      "rpn.blocks.1.2.bias torch.Size([128])\n",
      "rpn.blocks.1.2.running_mean torch.Size([128])\n",
      "rpn.blocks.1.2.running_var torch.Size([128])\n",
      "rpn.blocks.1.2.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.1.4.weight torch.Size([128, 128, 3, 3])\n",
      "rpn.blocks.1.5.weight torch.Size([128])\n",
      "rpn.blocks.1.5.bias torch.Size([128])\n",
      "rpn.blocks.1.5.running_mean torch.Size([128])\n",
      "rpn.blocks.1.5.running_var torch.Size([128])\n",
      "rpn.blocks.1.5.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.1.7.weight torch.Size([128, 128, 3, 3])\n",
      "rpn.blocks.1.8.weight torch.Size([128])\n",
      "rpn.blocks.1.8.bias torch.Size([128])\n",
      "rpn.blocks.1.8.running_mean torch.Size([128])\n",
      "rpn.blocks.1.8.running_var torch.Size([128])\n",
      "rpn.blocks.1.8.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.1.10.weight torch.Size([128, 128, 3, 3])\n",
      "rpn.blocks.1.11.weight torch.Size([128])\n",
      "rpn.blocks.1.11.bias torch.Size([128])\n",
      "rpn.blocks.1.11.running_mean torch.Size([128])\n",
      "rpn.blocks.1.11.running_var torch.Size([128])\n",
      "rpn.blocks.1.11.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.1.13.weight torch.Size([128, 128, 3, 3])\n",
      "rpn.blocks.1.14.weight torch.Size([128])\n",
      "rpn.blocks.1.14.bias torch.Size([128])\n",
      "rpn.blocks.1.14.running_mean torch.Size([128])\n",
      "rpn.blocks.1.14.running_var torch.Size([128])\n",
      "rpn.blocks.1.14.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.1.16.weight torch.Size([128, 128, 3, 3])\n",
      "rpn.blocks.1.17.weight torch.Size([128])\n",
      "rpn.blocks.1.17.bias torch.Size([128])\n",
      "rpn.blocks.1.17.running_mean torch.Size([128])\n",
      "rpn.blocks.1.17.running_var torch.Size([128])\n",
      "rpn.blocks.1.17.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.2.1.weight torch.Size([256, 128, 3, 3])\n",
      "rpn.blocks.2.2.weight torch.Size([256])\n",
      "rpn.blocks.2.2.bias torch.Size([256])\n",
      "rpn.blocks.2.2.running_mean torch.Size([256])\n",
      "rpn.blocks.2.2.running_var torch.Size([256])\n",
      "rpn.blocks.2.2.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.2.4.weight torch.Size([256, 256, 3, 3])\n",
      "rpn.blocks.2.5.weight torch.Size([256])\n",
      "rpn.blocks.2.5.bias torch.Size([256])\n",
      "rpn.blocks.2.5.running_mean torch.Size([256])\n",
      "rpn.blocks.2.5.running_var torch.Size([256])\n",
      "rpn.blocks.2.5.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.2.7.weight torch.Size([256, 256, 3, 3])\n",
      "rpn.blocks.2.8.weight torch.Size([256])\n",
      "rpn.blocks.2.8.bias torch.Size([256])\n",
      "rpn.blocks.2.8.running_mean torch.Size([256])\n",
      "rpn.blocks.2.8.running_var torch.Size([256])\n",
      "rpn.blocks.2.8.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.2.10.weight torch.Size([256, 256, 3, 3])\n",
      "rpn.blocks.2.11.weight torch.Size([256])\n",
      "rpn.blocks.2.11.bias torch.Size([256])\n",
      "rpn.blocks.2.11.running_mean torch.Size([256])\n",
      "rpn.blocks.2.11.running_var torch.Size([256])\n",
      "rpn.blocks.2.11.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.2.13.weight torch.Size([256, 256, 3, 3])\n",
      "rpn.blocks.2.14.weight torch.Size([256])\n",
      "rpn.blocks.2.14.bias torch.Size([256])\n",
      "rpn.blocks.2.14.running_mean torch.Size([256])\n",
      "rpn.blocks.2.14.running_var torch.Size([256])\n",
      "rpn.blocks.2.14.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.2.16.weight torch.Size([256, 256, 3, 3])\n",
      "rpn.blocks.2.17.weight torch.Size([256])\n",
      "rpn.blocks.2.17.bias torch.Size([256])\n",
      "rpn.blocks.2.17.running_mean torch.Size([256])\n",
      "rpn.blocks.2.17.running_var torch.Size([256])\n",
      "rpn.blocks.2.17.num_batches_tracked torch.Size([])\n",
      "rpn.deblocks.0.0.weight torch.Size([128, 64, 4, 4])\n",
      "rpn.deblocks.0.1.weight torch.Size([128])\n",
      "rpn.deblocks.0.1.bias torch.Size([128])\n",
      "rpn.deblocks.0.1.running_mean torch.Size([128])\n",
      "rpn.deblocks.0.1.running_var torch.Size([128])\n",
      "rpn.deblocks.0.1.num_batches_tracked torch.Size([])\n",
      "rpn.deblocks.1.0.weight torch.Size([128, 128, 2, 2])\n",
      "rpn.deblocks.1.1.weight torch.Size([128])\n",
      "rpn.deblocks.1.1.bias torch.Size([128])\n",
      "rpn.deblocks.1.1.running_mean torch.Size([128])\n",
      "rpn.deblocks.1.1.running_var torch.Size([128])\n",
      "rpn.deblocks.1.1.num_batches_tracked torch.Size([])\n",
      "rpn.deblocks.2.0.weight torch.Size([256, 128, 1, 1])\n",
      "rpn.deblocks.2.1.weight torch.Size([128])\n",
      "rpn.deblocks.2.1.bias torch.Size([128])\n",
      "rpn.deblocks.2.1.running_mean torch.Size([128])\n",
      "rpn.deblocks.2.1.running_var torch.Size([128])\n",
      "rpn.deblocks.2.1.num_batches_tracked torch.Size([])\n",
      "rpn.conv_cls.weight torch.Size([8, 384, 1, 1])\n",
      "rpn.conv_cls.bias torch.Size([8])\n",
      "rpn.conv_box.weight torch.Size([28, 384, 1, 1])\n",
      "rpn.conv_box.bias torch.Size([28])\n",
      "rpn_acc.total torch.Size([1])\n",
      "rpn_acc.count torch.Size([1])\n",
      "rpn_precision.total torch.Size([1])\n",
      "rpn_precision.count torch.Size([1])\n",
      "rpn_recall.total torch.Size([1])\n",
      "rpn_recall.count torch.Size([1])\n",
      "rpn_metrics.prec_total torch.Size([7])\n",
      "rpn_metrics.prec_count torch.Size([7])\n",
      "rpn_metrics.rec_total torch.Size([7])\n",
      "rpn_metrics.rec_count torch.Size([7])\n",
      "rpn_cls_loss.total torch.Size([1])\n",
      "rpn_cls_loss.count torch.Size([1])\n",
      "rpn_loc_loss.total torch.Size([1])\n",
      "rpn_loc_loss.count torch.Size([1])\n",
      "rpn_total_loss.total torch.Size([1])\n",
      "rpn_total_loss.count torch.Size([1])\n",
      "False _amp_stash\n",
      "max_num_voxels: 70000\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Restoring parameters from /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/cones_pp_v4/2021-2-11_9:58/adam_optimizer-9500.tckpt\n",
      "feature_map_size [1, 100, 150]\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.287 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n",
      "[{'box3d_lidar': tensor([[-11.8943, -11.3818,   0.1035,   0.3462,   0.3534,   0.7867,   1.6367],\n",
      "        [-14.4280, -11.3939,   0.0762,   0.3522,   0.3494,   0.7964,   1.6045],\n",
      "        [-14.6203, -17.8326,   0.2305,   0.3495,   0.3612,   1.2556,   1.6738],\n",
      "        [ 12.6422,   7.5413,   0.5449,   0.3332,   0.3325,   0.8914,   1.5488],\n",
      "        [-18.9910, -17.4341,   0.3789,   0.3646,   0.3756,   1.2746,   1.6602],\n",
      "        [-18.6278,  11.8670,   0.6641,   0.3667,   0.3632,   1.2907,   1.4785],\n",
      "        [-17.8358,  11.8585,   0.2793,   0.3332,   0.3427,   1.2885,   1.6904],\n",
      "        [-20.0020,  12.9414,   0.3613,   0.3570,   0.3649,   1.2906,   1.7305],\n",
      "        [ 12.8653,   9.0779,   0.1680,   0.3494,   0.3519,   1.0090,   1.6797],\n",
      "        [-22.7702, -12.1754,   0.2656,   0.3584,   0.3673,   1.2909,   1.7744],\n",
      "        [ 19.2291,   7.7398,   0.5586,   0.3295,   0.3323,   0.8772,   1.7461],\n",
      "        [-19.0938,   9.2149,   0.5312,   0.3764,   0.3858,   1.2788,   1.8340],\n",
      "        [-11.3751,  15.6965,   0.4707,   0.3598,   0.3705,   1.3106,   1.6123],\n",
      "        [-22.2867,  12.9017,   0.4824,   0.3760,   0.3888,   1.3120,   1.7451],\n",
      "        [-21.0976,   9.5880,   0.4883,   0.3679,   0.3780,   1.2738,   1.7285],\n",
      "        [-23.7757, -11.2226,   0.2676,   0.3711,   0.3977,   1.2832,   1.6289],\n",
      "        [-29.7821,   7.1470,   0.4629,   0.3553,   0.3601,   1.3107,   1.6318],\n",
      "        [-14.7997,  13.2443,   0.3203,   0.3543,   0.3679,   1.3123,   1.4443],\n",
      "        [-14.1898,  18.6834,   0.3145,   0.3405,   0.3487,   1.2966,   1.6260],\n",
      "        [ 22.5696,   5.3262,   0.1270,   0.3209,   0.3307,   0.9457,   1.7939],\n",
      "        [ 19.6757,  10.9738,   0.7188,   0.3432,   0.3480,   1.2179,   1.4639],\n",
      "        [-15.3735,  15.9949,   0.6309,   0.3516,   0.3572,   1.2917,   1.6904],\n",
      "        [ -6.7145, -11.9902,   0.7012,   0.3697,   0.3871,   1.2765,   1.6484],\n",
      "        [-13.6103,  15.8014,   0.4023,   0.3434,   0.3551,   1.3272,   1.5615],\n",
      "        [-24.6215,  11.9142,   0.7090,   0.3459,   0.3496,   1.3479,   1.6455],\n",
      "        [-19.3136,  12.0486,   0.7617,   0.3561,   0.3499,   1.3157,   1.7227],\n",
      "        [-25.8369,  14.5169,   0.3184,   0.3476,   0.3579,   1.2718,   1.7275],\n",
      "        [ 12.7009,  16.0338,   0.6328,   0.3846,   0.3814,   1.3204,   1.6553],\n",
      "        [ 29.4469,   3.9252,   0.4941,   0.3544,   0.3494,   1.0718,   1.5596],\n",
      "        [-15.7110,  13.2918,   0.2598,   0.3379,   0.3503,   1.2945,   1.4160],\n",
      "        [-15.8281,  14.3800,   0.4004,   0.3579,   0.3728,   1.2964,   1.6367],\n",
      "        [ 27.9694,  -3.8254,   0.2969,   0.3563,   0.3570,   1.2675,   1.6660],\n",
      "        [ 19.8853,  10.3300,   0.7148,   0.3516,   0.3520,   1.2928,   1.4453],\n",
      "        [-20.9794,  11.2339,   0.2930,   0.4532,   0.4547,   1.2046,   1.6602],\n",
      "        [-15.7752,  15.3759,   0.8242,   0.3677,   0.3706,   1.3096,   1.5557],\n",
      "        [ -5.7190,  17.2933,   0.3066,   0.3326,   0.3417,   1.2787,   1.7852],\n",
      "        [ 19.7486,  12.0104,   0.6523,   0.3526,   0.3626,   1.1988,   1.4199],\n",
      "        [ 11.6718,   7.3092,   0.7324,   0.3345,   0.3435,   1.3075,   1.6484],\n",
      "        [-13.2450, -17.9684,   0.2910,   0.3400,   0.3460,   1.2496,   1.5752],\n",
      "        [ 28.3317,   3.1151,   0.0352,   0.3349,   0.3359,   0.7806,   1.8018],\n",
      "        [-13.9646,  -7.8440,   0.5742,   0.3460,   0.3520,   1.2687,   1.6221],\n",
      "        [-20.3888,  11.8906,   0.7344,   0.3615,   0.3711,   1.2655,   1.7354],\n",
      "        [ 26.9352,   4.3203,   0.7129,   0.3439,   0.3564,   1.3288,   1.5449],\n",
      "        [ 25.4229,   4.2727,   0.3359,   0.3524,   0.3633,   1.3417,   1.8779],\n",
      "        [-15.9961,   7.0666,   0.7520,   0.3553,   0.3543,   1.3095,   1.6729],\n",
      "        [ 11.9667, -12.1284,   0.6191,   0.3424,   0.3496,   1.2859,   1.7246],\n",
      "        [ 25.5734,   6.3869,   0.7969,   0.3528,   0.3560,   1.4131,   1.4766],\n",
      "        [  8.8291,  17.2958,   0.2988,   0.3351,   0.3420,   1.2724,   1.6670],\n",
      "        [ -1.1832, -15.2716,   0.3867,   0.3347,   0.3481,   1.2231,   1.7109],\n",
      "        [ 28.2642,   6.1831,   0.2109,   0.3430,   0.3490,   0.8008,   1.7158],\n",
      "        [ 14.4624, -12.7879,   0.4590,   0.3285,   0.3324,   0.8194,   1.7705],\n",
      "        [-21.9207, -10.8312,   0.5820,   0.3742,   0.3769,   1.3228,   1.4326],\n",
      "        [ -2.1183,  16.3847,   0.6777,   0.3665,   0.3746,   1.3555,   1.8135],\n",
      "        [  9.5729, -14.8425,   0.6328,   0.3556,   0.3556,   1.1776,   1.7695],\n",
      "        [ 19.7750, -13.2135,   0.0508,   0.3320,   0.3325,   0.7859,   1.6611],\n",
      "        [ 25.5466,  -4.1344,   0.7227,   0.3490,   0.3512,   1.3756,   1.4775],\n",
      "        [ -6.3368, -16.2445,   0.1191,   0.3275,   0.3312,   0.7696,   1.7109],\n",
      "        [ 21.9347,  11.6849,   0.2266,   0.3710,   0.3734,   1.2428,   1.7129],\n",
      "        [-14.6853, -14.1248,   0.2656,   0.3398,   0.3575,   1.2448,   1.6504]],\n",
      "       device='cuda:0'), 'scores': tensor([0.6923, 0.6409, 0.6210, 0.6195, 0.5856, 0.5718, 0.5600, 0.5590, 0.5575,\n",
      "        0.5512, 0.5447, 0.5437, 0.5351, 0.5288, 0.5287, 0.5284, 0.5203, 0.5170,\n",
      "        0.5140, 0.5131, 0.5058, 0.5032, 0.5009, 0.4899, 0.4898, 0.4838, 0.4824,\n",
      "        0.4813, 0.4754, 0.4746, 0.4725, 0.4614, 0.4591, 0.4573, 0.4556, 0.4554,\n",
      "        0.4525, 0.4489, 0.4485, 0.4454, 0.4395, 0.4306, 0.4250, 0.4185, 0.4165,\n",
      "        0.4150, 0.4064, 0.4013, 0.3991, 0.3968, 0.3962, 0.3943, 0.3920, 0.3900,\n",
      "        0.3897, 0.3896, 0.3894, 0.3864, 0.3858], device='cuda:0'), 'label_preds': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'), 'metadata': {'token': '2e024439286d469f8f743fe9266c0e56'}}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n",
      "[]\n",
      "0 0 0\n",
      "[]\n",
      "[{'box3d_lidar': tensor([[-22.5135, -16.4521,   0.3008,   0.3483,   0.3739,   1.3226,   1.6650],\n",
      "        [ -9.1035, -17.4809,   0.3164,   0.3711,   0.3803,   1.2437,   1.5615],\n",
      "        [ 11.9650,  -9.2752,   0.4922,   0.3374,   0.3346,   0.7498,   1.6221],\n",
      "        [-14.5341,  -6.7192,   0.4258,   0.3426,   0.3407,   0.8581,   1.5801],\n",
      "        [ -7.3123, -16.2921,   0.2598,   0.3594,   0.3719,   1.2472,   1.4766],\n",
      "        [-17.9659, -17.4633,   0.3301,   0.3556,   0.3616,   1.2625,   1.7588],\n",
      "        [ 11.1957,  -9.1063,   0.3301,   0.3378,   0.3420,   0.7791,   1.5527],\n",
      "        [ -6.7862,   2.4479,   1.0430,   0.6206,   0.6873,   1.7852,   1.7109],\n",
      "        [-19.7098, -17.2021,   0.1641,   0.3425,   0.3625,   0.8855,   1.8184],\n",
      "        [ -2.0200,  17.3891,   0.4551,   0.3556,   0.3553,   1.2515,   1.6992],\n",
      "        [ 29.1923,   5.4366,   0.6504,   0.3694,   0.3681,   1.2584,   1.7334],\n",
      "        [ -6.0428,   2.7091,   0.4121,   0.3345,   0.3320,   0.7861,   1.5439],\n",
      "        [ 29.5276,  12.0444,   0.6953,   0.3712,   0.3709,   1.2678,   1.7783],\n",
      "        [ 21.8802,  13.3766,   0.3984,   0.3721,   0.3768,   1.2533,   1.6924],\n",
      "        [  4.0446, -18.9675,   0.3418,   0.3647,   0.3715,   1.2617,   1.7432],\n",
      "        [ 17.2867, -18.6096,   0.3340,   0.3581,   0.3638,   1.2601,   1.7500],\n",
      "        [ -8.6017,   3.8670,   0.6484,   0.3307,   0.3422,   1.2561,   1.6562],\n",
      "        [ 29.7912,   3.4638,   0.4590,   0.3523,   0.3501,   1.0519,   1.5732],\n",
      "        [ -9.9029,  18.1239,   0.3809,   0.3560,   0.3624,   1.2723,   1.6602],\n",
      "        [  2.7165,  16.8048,   0.7188,   0.3497,   0.3553,   1.3703,   1.6924],\n",
      "        [ 20.9172,   9.7443,   0.5586,   0.3507,   0.3567,   1.3686,   1.5195],\n",
      "        [-12.1521,   3.6683,   0.6270,   0.3307,   0.3291,   0.9976,   1.6631],\n",
      "        [ 13.1758, -18.7491,   0.3125,   0.3422,   0.3499,   1.2815,   1.6602],\n",
      "        [ -0.3629,  17.2759,   0.2363,   0.3275,   0.3363,   1.2390,   1.7324],\n",
      "        [ 18.3721,  -6.7239,   0.7207,   0.3356,   0.3415,   1.3059,   1.6143],\n",
      "        [-26.2465, -14.0758,   0.2559,   0.3318,   0.3400,   1.2286,   1.7285],\n",
      "        [ -1.9028, -18.2234,   0.4434,   0.3662,   0.3730,   1.2616,   1.7236],\n",
      "        [ 22.3583,   6.2118,   0.4043,   0.3351,   0.3522,   1.2292,   1.7256],\n",
      "        [-28.5131,   7.3374,   0.2461,   0.3211,   0.3281,   1.0423,   1.7246],\n",
      "        [  1.9791,  17.0175,   0.3281,   0.3502,   0.3606,   1.2346,   1.8115],\n",
      "        [  3.2094,  17.3580,   0.4082,   0.3472,   0.3606,   1.2439,   1.6064],\n",
      "        [ 23.6087,  10.5181,   0.7051,   0.3494,   0.3481,   1.2185,   1.6504],\n",
      "        [ -4.1566,  -8.7697,   0.0664,   0.3221,   0.3246,   0.7193,   1.8438],\n",
      "        [  4.1516,  17.9885,   0.3438,   0.3416,   0.3496,   1.2478,   1.7812],\n",
      "        [-12.8727,   3.5404,   0.6426,   0.3353,   0.3428,   1.2619,   1.6729],\n",
      "        [ 21.7813,   5.5287,   0.7852,   0.3366,   0.3448,   1.2437,   1.6367],\n",
      "        [ 17.2242,   7.0874,   0.7402,   0.3377,   0.3298,   1.1699,   1.7119],\n",
      "        [ 20.9568,   4.8128,   0.8164,   0.3285,   0.3361,   1.2584,   1.5996],\n",
      "        [ 29.5796,  -3.6512,   0.5078,   0.3540,   0.3459,   0.8716,   1.8340],\n",
      "        [ 19.0709,   9.9939,   0.7480,   0.3708,   0.3785,   1.3048,   1.8389],\n",
      "        [ 17.8175,   6.0093,   0.2578,   0.3374,   0.3459,   1.0531,   1.8730],\n",
      "        [ 22.6696,   6.9671,   0.8906,   0.3731,   0.3783,   1.3146,   1.6543],\n",
      "        [ 21.3699,  10.2353,   0.8750,   0.3802,   0.3802,   1.3405,   1.6367],\n",
      "        [ -9.3410,   3.6902,   0.6348,   0.3320,   0.3418,   1.1194,   1.7266],\n",
      "        [-27.5648, -13.5945,   0.0703,   0.3381,   0.3405,   1.0407,   1.7197],\n",
      "        [-26.2593,   6.3302,   1.2070,   0.6078,   0.6528,   1.7994,   1.6445],\n",
      "        [-22.5189,   6.4383,   0.1035,   0.3549,   0.3616,   0.7819,   1.6270],\n",
      "        [ 29.8578,   8.4892,   0.4883,   0.3503,   0.3438,   0.8930,   1.8359],\n",
      "        [  9.6286,   6.3002,   0.6543,   0.3286,   0.3318,   0.7827,   1.7314],\n",
      "        [  4.2005,  15.8150,   0.3945,   0.3587,   0.3679,   1.1701,   1.8115],\n",
      "        [ 24.0932,   9.8523,   0.2598,   0.3477,   0.3651,   1.2282,   1.7598]],\n",
      "       device='cuda:0'), 'scores': tensor([0.6738, 0.6627, 0.6301, 0.6184, 0.5725, 0.5678, 0.5635, 0.5585, 0.5364,\n",
      "        0.5237, 0.5174, 0.5162, 0.5145, 0.5013, 0.4637, 0.4630, 0.4613, 0.4603,\n",
      "        0.4599, 0.4556, 0.4467, 0.4466, 0.4454, 0.4451, 0.4169, 0.4077, 0.3976,\n",
      "        0.3856, 0.3673, 0.3641, 0.3585, 0.3543, 0.3525, 0.3498, 0.3446, 0.3415,\n",
      "        0.3406, 0.3336, 0.3331, 0.3327, 0.3293, 0.3282, 0.3278, 0.3238, 0.3214,\n",
      "        0.3211, 0.3172, 0.3171, 0.3118, 0.3107, 0.3104], device='cuda:0'), 'label_preds': tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0], device='cuda:0'), 'metadata': {'token': '4f545737bf3347fbbc9af60b0be9a963'}}]\n",
      "0 0 0\n",
      "[]\n",
      "0 0 0\n",
      "[]\n",
      "[{'box3d_lidar': tensor([[ 3.6923e+00, -1.8957e+00,  4.2383e-01,  3.4625e-01,  3.4317e-01,\n",
      "          7.3454e-01,  1.5469e+00],\n",
      "        [ 8.6746e+00, -1.5700e+00,  4.5312e-01,  3.4448e-01,  3.4435e-01,\n",
      "          7.3813e-01,  1.5488e+00],\n",
      "        [-1.8657e+00, -2.3821e+00,  3.9258e-01,  3.4637e-01,  3.4675e-01,\n",
      "          7.3994e-01,  1.5469e+00],\n",
      "        [-1.3348e+00, -1.0163e+01,  8.3984e-02,  3.5643e-01,  3.5493e-01,\n",
      "          7.5971e-01,  1.6230e+00],\n",
      "        [ 1.3172e+01, -1.1078e+00,  4.6289e-01,  3.4171e-01,  3.4050e-01,\n",
      "          7.3078e-01,  1.5723e+00],\n",
      "        [ 1.8076e+01, -5.2151e-01,  4.9023e-01,  3.5127e-01,  3.4493e-01,\n",
      "          7.6008e-01,  1.5713e+00],\n",
      "        [-6.0401e+00, -1.0549e+01,  5.8594e-02,  3.5534e-01,  3.5376e-01,\n",
      "          7.4356e-01,  1.6572e+00],\n",
      "        [ 1.2717e+01, -8.8219e+00,  1.3672e-01,  3.5774e-01,  3.6615e-01,\n",
      "          7.4410e-01,  1.6416e+00],\n",
      "        [ 8.1239e+00, -9.2210e+00,  4.5312e-01,  3.4109e-01,  3.3555e-01,\n",
      "          7.5398e-01,  1.5771e+00],\n",
      "        [-2.3844e+01,  3.5048e+00,  8.0078e-01,  3.5719e-01,  3.5841e-01,\n",
      "          1.2863e+00,  1.6396e+00],\n",
      "        [ 3.1783e+00, -9.6876e+00,  3.8477e-01,  3.4330e-01,  3.3830e-01,\n",
      "          7.6045e-01,  1.5938e+00],\n",
      "        [-1.5751e+01,  6.2410e-01,  5.8789e-01,  3.5628e-01,  3.5859e-01,\n",
      "          1.2357e+00,  1.4414e+00],\n",
      "        [-1.0666e+01,  7.1008e+00,  6.6602e-01,  4.1045e-01,  4.2128e-01,\n",
      "          1.2360e+00,  1.7266e+00],\n",
      "        [-1.8519e+01, -1.4818e+01,  2.8125e-01,  3.9331e-01,  4.0364e-01,\n",
      "          1.2501e+00,  1.5645e+00],\n",
      "        [ 2.5476e+01, -7.5587e+00,  3.5156e-01,  3.7802e-01,  3.7262e-01,\n",
      "          7.7318e-01,  1.6543e+00],\n",
      "        [-1.8082e+01,  1.8782e-01,  7.1680e-01,  3.4688e-01,  3.5182e-01,\n",
      "          1.3302e+00,  1.4512e+00],\n",
      "        [ 1.6664e+01, -8.4917e+00,  3.3008e-01,  3.7090e-01,  3.6481e-01,\n",
      "          7.9870e-01,  1.5664e+00],\n",
      "        [ 1.9916e+01, -8.2530e+00,  2.3438e-01,  3.5971e-01,  3.6341e-01,\n",
      "          8.7143e-01,  1.5566e+00],\n",
      "        [-1.6563e+01,  7.5484e-01,  5.7422e-01,  3.6103e-01,  3.6673e-01,\n",
      "          1.2519e+00,  1.5527e+00],\n",
      "        [-8.5561e+00,  6.1626e+00,  8.2031e-01,  3.4326e-01,  3.4947e-01,\n",
      "          1.2712e+00,  1.8096e+00],\n",
      "        [ 2.5361e+01,  5.6229e+00,  4.8047e-01,  3.5958e-01,  3.7162e-01,\n",
      "          1.3021e+00,  1.6055e+00],\n",
      "        [ 2.7983e+01, -1.5874e+01,  4.0820e-01,  3.5153e-01,  3.6463e-01,\n",
      "          1.2824e+00,  1.5674e+00],\n",
      "        [-2.1514e+01,  6.5239e+00,  3.0664e-01,  3.3306e-01,  3.4870e-01,\n",
      "          1.2983e+00,  1.5312e+00],\n",
      "        [-2.0472e+01, -1.5209e+01,  3.4180e-01,  4.1708e-01,  4.4061e-01,\n",
      "          1.2262e+00,  1.6621e+00],\n",
      "        [-2.2555e+01,  2.9595e+00,  2.8320e-01,  3.9492e-01,  4.0758e-01,\n",
      "          1.2043e+00,  1.7266e+00],\n",
      "        [-1.1474e+01, -3.5405e+00,  6.3867e-01,  3.6448e-01,  3.6242e-01,\n",
      "          1.2530e+00,  1.5420e+00],\n",
      "        [-1.7213e+01,  5.5915e-01,  6.6992e-01,  3.4718e-01,  3.5567e-01,\n",
      "          1.2525e+00,  1.3887e+00],\n",
      "        [-7.1965e+00, -8.6537e+00,  4.2773e-01,  3.3744e-01,  3.4246e-01,\n",
      "          1.0017e+00,  1.6689e+00],\n",
      "        [-1.5053e+01,  7.4455e+00,  7.9102e-01,  3.9930e-01,  4.0272e-01,\n",
      "          1.2360e+00,  1.6035e+00],\n",
      "        [ 1.2403e+00, -1.7270e+01,  1.2500e-01,  3.4654e-01,  3.4934e-01,\n",
      "          8.0870e-01,  1.7295e+00],\n",
      "        [-1.0065e+01,  1.2667e+01,  3.9844e-01,  3.5828e-01,  3.6202e-01,\n",
      "          1.2898e+00,  1.7275e+00],\n",
      "        [-8.7331e+00, -7.9240e+00,  9.5312e-01,  6.2941e-01,  6.6807e-01,\n",
      "          1.7758e+00,  1.7178e+00],\n",
      "        [ 2.1964e+01,  1.1982e+01,  4.1797e-01,  3.4338e-01,  3.6140e-01,\n",
      "          1.2945e+00,  1.6855e+00],\n",
      "        [-1.6753e+01,  1.1954e+01,  3.9844e-01,  3.5550e-01,  3.6490e-01,\n",
      "          1.2737e+00,  1.6611e+00],\n",
      "        [ 9.2508e+00, -1.7621e+01,  2.9102e-01,  3.5482e-01,  3.6415e-01,\n",
      "          1.2653e+00,  1.6699e+00],\n",
      "        [-1.3981e+01, -1.8531e+01,  2.1484e-02,  3.4179e-01,  3.4187e-01,\n",
      "          8.1524e-01,  1.6865e+00],\n",
      "        [ 2.7923e+01,  6.0286e+00,  3.7695e-01,  3.7126e-01,  3.8624e-01,\n",
      "          1.2895e+00,  1.6055e+00],\n",
      "        [-2.0264e+01,  1.4461e+01,  2.5195e-01,  3.6553e-01,  3.8046e-01,\n",
      "          1.2493e+00,  1.7422e+00],\n",
      "        [ 2.8387e+01,  8.1505e+00,  5.8984e-01,  3.6479e-01,  3.7024e-01,\n",
      "          1.3168e+00,  1.8105e+00],\n",
      "        [ 1.3325e+01,  7.1633e+00,  8.9648e-01,  3.6645e-01,  3.5706e-01,\n",
      "          1.1759e+00,  1.6797e+00],\n",
      "        [-1.9641e+01, -1.4959e+01,  5.4883e-01,  3.7859e-01,  3.7902e-01,\n",
      "          1.2802e+00,  1.5156e+00],\n",
      "        [-6.5234e+00,  1.4960e+01,  7.6172e-01,  3.3942e-01,  3.5139e-01,\n",
      "          1.3050e+00,  1.7070e+00],\n",
      "        [-7.1095e+00,  1.2201e+01,  8.2812e-01,  3.4117e-01,  3.4624e-01,\n",
      "          1.3994e+00,  1.6436e+00],\n",
      "        [-1.6986e+00,  1.0099e+01,  1.0273e+00,  3.8175e-01,  3.7360e-01,\n",
      "          1.1574e+00,  1.9600e+00],\n",
      "        [ 2.4504e+01,  5.4543e+00,  5.6055e-01,  3.4527e-01,  3.5011e-01,\n",
      "          1.2942e+00,  1.5693e+00]], device='cuda:0'), 'scores': tensor([0.7516, 0.7472, 0.7465, 0.6924, 0.6909, 0.6829, 0.6772, 0.6427, 0.6326,\n",
      "        0.6101, 0.6023, 0.5964, 0.5881, 0.5841, 0.5824, 0.5774, 0.5755, 0.5041,\n",
      "        0.4983, 0.4916, 0.4819, 0.4761, 0.4758, 0.4737, 0.4672, 0.4546, 0.4509,\n",
      "        0.4479, 0.4473, 0.4456, 0.4441, 0.4437, 0.4400, 0.4385, 0.4353, 0.4318,\n",
      "        0.4254, 0.4227, 0.4125, 0.4111, 0.4094, 0.4058, 0.4054, 0.4023, 0.3892],\n",
      "       device='cuda:0'), 'label_preds': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0'), 'metadata': {'token': 'a7acb150914b40bdad9a2dc4e657cbf9'}}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3 3\n",
      "[0.7516481 0.7472472 0.7465088]\n",
      "0 0 0\n",
      "[]\n",
      "[{'box3d_lidar': tensor([[ 7.1605e+00, -2.1910e+00,  4.3945e-01,  3.4773e-01,  3.4355e-01,\n",
      "          7.2598e-01,  1.5693e+00],\n",
      "        [ 4.5221e+00, -9.8753e+00,  3.7695e-01,  3.4943e-01,  3.4650e-01,\n",
      "          7.4902e-01,  1.5654e+00],\n",
      "        [ 6.0207e-01, -9.5702e+00,  1.8945e-01,  3.5684e-01,  3.5415e-01,\n",
      "          7.4392e-01,  1.6270e+00],\n",
      "        [ 2.4290e+01, -3.2126e+00,  3.8086e-01,  3.9201e-01,  3.7999e-01,\n",
      "          8.8873e-01,  1.6396e+00],\n",
      "        [-7.2266e+00, -1.4252e+00,  3.7891e-01,  3.4188e-01,  3.3810e-01,\n",
      "          7.5416e-01,  1.5596e+00],\n",
      "        [-1.5777e+01, -9.4066e-01,  3.5156e-01,  3.4807e-01,  3.4242e-01,\n",
      "          7.5859e-01,  1.6494e+00],\n",
      "        [ 2.2081e+00, -2.0365e+00,  3.9648e-01,  3.5037e-01,  3.4645e-01,\n",
      "          7.6660e-01,  1.6221e+00],\n",
      "        [-1.2860e+01, -1.1731e+00,  3.7891e-01,  3.3876e-01,  3.4162e-01,\n",
      "          7.7035e-01,  1.5127e+00],\n",
      "        [-3.9277e+00, -9.2697e+00,  1.3477e-01,  3.6096e-01,  3.6355e-01,\n",
      "          7.4211e-01,  1.5859e+00],\n",
      "        [ 3.1357e+00, -1.8518e+01,  3.7500e-01,  3.6649e-01,  3.7419e-01,\n",
      "          1.3020e+00,  1.5752e+00],\n",
      "        [-2.0866e+01, -5.7241e+00,  7.0703e-01,  3.4913e-01,  3.5938e-01,\n",
      "          1.2980e+00,  1.5762e+00],\n",
      "        [ 1.7965e+01, -2.9002e+00,  9.1797e-02,  3.4389e-01,  3.4887e-01,\n",
      "          9.4023e-01,  1.6992e+00],\n",
      "        [ 1.8190e+01, -1.0422e+01,  1.3672e-02,  3.4343e-01,  3.4964e-01,\n",
      "          7.4629e-01,  1.7305e+00],\n",
      "        [-2.9358e+01,  7.6399e+00,  3.0664e-01,  4.2076e-01,  4.2875e-01,\n",
      "          1.2041e+00,  1.7246e+00],\n",
      "        [-2.7710e+01,  4.6608e+00,  5.5469e-01,  3.7305e-01,  3.7442e-01,\n",
      "          1.2989e+00,  1.6543e+00],\n",
      "        [-2.2162e+00, -1.8497e+00,  4.1406e-01,  3.3531e-01,  3.3645e-01,\n",
      "          7.6735e-01,  1.6367e+00],\n",
      "        [ 1.9608e+01, -1.0461e+01,  1.0938e-01,  3.5016e-01,  3.5715e-01,\n",
      "          8.2425e-01,  1.6924e+00],\n",
      "        [ 1.0469e+01, -1.0205e+01,  4.0234e-01,  3.3506e-01,  3.3387e-01,\n",
      "          7.5067e-01,  1.6680e+00],\n",
      "        [-2.3382e+01, -1.7229e+00,  2.4023e-01,  3.5346e-01,  3.6887e-01,\n",
      "          1.1839e+00,  1.7383e+00],\n",
      "        [ 1.3140e+01, -2.6642e+00,  2.1875e-01,  3.6658e-01,  3.6779e-01,\n",
      "          7.1246e-01,  1.6875e+00],\n",
      "        [-1.9571e+01, -7.4284e+00,  1.4844e-01,  3.3707e-01,  3.4586e-01,\n",
      "          1.0838e+00,  1.7930e+00],\n",
      "        [ 1.8669e+01,  1.4200e+00,  6.1523e-01,  3.4150e-01,  3.4443e-01,\n",
      "          1.1957e+00,  1.6660e+00],\n",
      "        [ 2.2207e+01, -1.0610e+01,  1.7383e-01,  3.5872e-01,  3.6415e-01,\n",
      "          7.9384e-01,  1.7197e+00],\n",
      "        [-2.6970e+01,  4.3518e+00,  3.4570e-01,  3.5953e-01,  3.7349e-01,\n",
      "          1.2754e+00,  1.6934e+00],\n",
      "        [-2.6184e+01, -5.7818e-01,  2.7930e-01,  3.4267e-01,  3.5015e-01,\n",
      "          1.2743e+00,  1.7500e+00],\n",
      "        [ 1.4947e+01, -1.0268e+01,  2.2656e-01,  3.6043e-01,  3.6497e-01,\n",
      "          8.2849e-01,  1.6641e+00],\n",
      "        [-2.5950e+01,  7.3915e+00,  7.1094e-01,  5.7737e-01,  5.9380e-01,\n",
      "          1.7407e+00,  1.8428e+00],\n",
      "        [ 9.8222e+00, -7.7735e+00,  5.3711e-01,  3.3773e-01,  3.3959e-01,\n",
      "          9.0383e-01,  1.5811e+00],\n",
      "        [-8.4293e+00,  1.4002e+01,  3.4766e-01,  3.7101e-01,  3.7497e-01,\n",
      "          1.2881e+00,  1.5742e+00],\n",
      "        [-2.0117e+01, -5.7241e+00,  4.2188e-01,  3.8167e-01,  3.9161e-01,\n",
      "          1.2599e+00,  1.6533e+00],\n",
      "        [-2.1402e+01, -6.6093e+00,  3.9258e-01,  3.6046e-01,  3.7447e-01,\n",
      "          1.3354e+00,  1.5820e+00],\n",
      "        [ 2.4624e+01, -1.0636e+01,  3.1836e-01,  3.3209e-01,  3.3237e-01,\n",
      "          8.9592e-01,  1.5908e+00],\n",
      "        [-2.6980e+01, -6.2390e+00,  3.3203e-01,  3.5131e-01,  3.6533e-01,\n",
      "          1.2939e+00,  1.6201e+00],\n",
      "        [-2.0230e+01,  1.8634e+01,  5.5664e-01,  3.5945e-01,  3.6506e-01,\n",
      "          1.3082e+00,  1.8691e+00],\n",
      "        [-2.6005e+01, -4.5197e+00,  1.9727e-01,  3.4807e-01,  3.4934e-01,\n",
      "          1.0737e+00,  1.6670e+00],\n",
      "        [-5.2965e+00,  1.4293e+01,  7.0508e-01,  3.5613e-01,  3.6497e-01,\n",
      "          1.3302e+00,  1.3906e+00],\n",
      "        [-2.5613e+01,  8.1264e+00,  7.6758e-01,  6.0228e-01,  6.2725e-01,\n",
      "          1.7688e+00,  1.6133e+00],\n",
      "        [-2.6238e+01,  4.0593e+00,  2.6172e-01,  3.4709e-01,  3.5549e-01,\n",
      "          1.1166e+00,  1.7168e+00],\n",
      "        [-2.6395e+01,  6.3738e+00,  3.1836e-01,  3.7959e-01,  3.9848e-01,\n",
      "          1.2639e+00,  1.7051e+00],\n",
      "        [-1.6548e+01, -8.8220e-01,  5.4102e-01,  3.5833e-01,  3.4985e-01,\n",
      "          1.0495e+00,  1.6152e+00],\n",
      "        [-9.3422e+00,  1.1350e+01,  4.2969e-01,  3.4625e-01,  3.5942e-01,\n",
      "          1.2576e+00,  1.7246e+00],\n",
      "        [-1.8755e+01,  1.5097e+01,  3.5742e-01,  3.5489e-01,  3.6003e-01,\n",
      "          1.2701e+00,  1.7266e+00],\n",
      "        [-1.8812e+01,  1.8194e+01,  3.6328e-01,  3.5802e-01,  3.6508e-01,\n",
      "          1.2837e+00,  1.7832e+00],\n",
      "        [ 1.4715e+01, -1.8869e+01,  5.3711e-01,  3.6335e-01,  3.6339e-01,\n",
      "          1.3024e+00,  1.6855e+00],\n",
      "        [-2.7396e+01,  1.4274e+01,  3.0859e-01,  3.5467e-01,  3.6613e-01,\n",
      "          1.2515e+00,  1.7734e+00],\n",
      "        [-2.6748e+01, -1.5337e+00,  2.9883e-01,  3.1999e-01,  3.3359e-01,\n",
      "          1.2525e+00,  1.6035e+00],\n",
      "        [-2.1656e+01, -3.3420e+00,  2.3438e-01,  3.4658e-01,  3.5894e-01,\n",
      "          1.2167e+00,  1.5557e+00],\n",
      "        [-2.3471e+01, -9.1812e+00,  6.9922e-01,  3.4667e-01,  3.4832e-01,\n",
      "          1.2638e+00,  1.7383e+00],\n",
      "        [ 1.8008e+01, -1.9279e+01,  6.9727e-01,  3.4313e-01,  3.4973e-01,\n",
      "          1.3646e+00,  1.5947e+00],\n",
      "        [-1.2353e+01,  1.1913e+01,  3.1445e-01,  3.4969e-01,  3.6246e-01,\n",
      "          1.2837e+00,  1.7861e+00],\n",
      "        [-1.8214e+01, -8.4830e+00,  2.2656e-01,  3.3967e-01,  3.4058e-01,\n",
      "          7.7582e-01,  1.7363e+00],\n",
      "        [-1.9178e+01, -3.4587e+00,  5.4688e-01,  3.4964e-01,  3.5402e-01,\n",
      "          1.2862e+00,  1.3799e+00]], device='cuda:0'), 'scores': tensor([0.7456, 0.7198, 0.7055, 0.6998, 0.6939, 0.6916, 0.6699, 0.6665, 0.6600,\n",
      "        0.6480, 0.6376, 0.6229, 0.6186, 0.6107, 0.6091, 0.6047, 0.6023, 0.5716,\n",
      "        0.5672, 0.5590, 0.5487, 0.5480, 0.5476, 0.5376, 0.5366, 0.5343, 0.5306,\n",
      "        0.5274, 0.5261, 0.5130, 0.5115, 0.5062, 0.5025, 0.5009, 0.4922, 0.4895,\n",
      "        0.4846, 0.4822, 0.4799, 0.4797, 0.4780, 0.4653, 0.4642, 0.4598, 0.4579,\n",
      "        0.4521, 0.4487, 0.4455, 0.4407, 0.4384, 0.4325, 0.4310],\n",
      "       device='cuda:0'), 'label_preds': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0], device='cuda:0'), 'metadata': {'token': '67e5f88901214f3aa03d68e028185e22'}}]\n",
      "3 3 3\n",
      "[0.74558365 0.71977776 0.70548075]\n",
      "0 0 0\n",
      "[]\n",
      "[{'box3d_lidar': tensor([[  5.6170,  -2.6319,   0.4238,   0.3476,   0.3447,   0.7371,   1.5566],\n",
      "        [ -3.7492,  -1.5923,   0.4102,   0.3485,   0.3509,   0.7356,   1.5508],\n",
      "        [ -8.7172,  -0.8187,   0.3574,   0.3495,   0.3478,   0.7481,   1.5498],\n",
      "        [ 18.7993, -12.0011,   0.3301,   0.3465,   0.3476,   0.7443,   1.6084],\n",
      "        [ 17.4782, -11.7916,   0.2461,   0.3742,   0.3809,   0.7675,   1.6172],\n",
      "        [ 11.5709,  -3.4697,   0.4238,   0.3437,   0.3431,   0.7290,   1.5723],\n",
      "        [  2.4024, -10.1528,   0.3574,   0.3442,   0.3471,   0.7488,   1.6680],\n",
      "        [-14.2348,  -0.1377,   0.4121,   0.3381,   0.3394,   0.7456,   1.7188],\n",
      "        [-11.3037, -17.3415,   0.2754,   0.4312,   0.4451,   1.2106,   1.5205],\n",
      "        [ 11.2830, -11.1101,   0.0469,   0.3482,   0.3533,   0.7679,   1.6963],\n",
      "        [ -1.4194,  -9.5488,   0.3457,   0.3466,   0.3420,   0.7470,   1.6768],\n",
      "        [-26.0765,  10.5361,   0.5215,   0.3765,   0.3884,   1.2704,   1.7910],\n",
      "        [-27.7013,   0.9048,   0.2324,   0.3470,   0.3568,   1.2772,   1.4199],\n",
      "        [ 12.7829, -11.2215,   0.1699,   0.3459,   0.3445,   0.7571,   1.6660],\n",
      "        [-24.4554,  -4.9711,   0.1387,   0.3229,   0.3319,   1.2396,   1.7080],\n",
      "        [ -3.7956, -14.8460,   0.2305,   0.3402,   0.3452,   0.8883,   1.4980],\n",
      "        [-15.4529,  -7.8094,   0.1641,   0.3578,   0.3621,   0.7573,   1.6504],\n",
      "        [ 11.8614, -19.7552,   0.3164,   0.3609,   0.3714,   1.2541,   1.6240],\n",
      "        [-27.5991,   6.0558,   0.7734,   0.3644,   0.3642,   1.3023,   1.6914],\n",
      "        [-23.3797, -11.2544,   0.2812,   0.3482,   0.3599,   1.2885,   1.6426],\n",
      "        [-28.7561,  -4.1633,   0.2910,   0.3616,   0.3730,   1.2528,   1.6826],\n",
      "        [-26.2584,   1.6440,   0.0859,   0.3464,   0.3501,   1.1198,   1.5605],\n",
      "        [ -0.9640, -14.2201,   0.4648,   0.3518,   0.3621,   1.3228,   1.3613],\n",
      "        [  5.7263, -10.4775,   0.0430,   0.3379,   0.3457,   0.7603,   1.8232],\n",
      "        [-26.5914,   9.1023,   0.9336,   0.5932,   0.6396,   1.7623,   1.7559],\n",
      "        [ 19.7421,   2.0604,   0.3770,   0.3633,   0.3735,   1.2977,   1.7188],\n",
      "        [-29.3549,  -0.2989,   0.3359,   0.3610,   0.3529,   0.8961,   1.4180],\n",
      "        [-23.3996, -12.0921,   0.2832,   0.3422,   0.3478,   1.3007,   1.5918],\n",
      "        [-21.4248,  -5.9030,   0.4102,   0.3549,   0.3522,   0.9987,   1.6289],\n",
      "        [-25.1954,   1.4410,   0.2090,   0.3360,   0.3470,   1.2614,   1.5986],\n",
      "        [ 14.7391,   1.2439,   0.7012,   0.3636,   0.3791,   1.3385,   1.7627],\n",
      "        [  0.5134, -18.7063,   0.3145,   0.3548,   0.3654,   1.2723,   1.6406],\n",
      "        [ 17.0311,   1.8624,   0.2637,   0.3608,   0.3599,   1.2737,   1.4941],\n",
      "        [-21.6327,  -5.0665,   0.7129,   0.3324,   0.3365,   1.1240,   1.6152],\n",
      "        [-21.7957,  -4.0668,   0.2461,   0.3570,   0.3747,   1.2668,   1.6377],\n",
      "        [ -1.8716, -14.8687,   0.2988,   0.3351,   0.3368,   1.2037,   1.5527],\n",
      "        [-26.8161,  10.9600,   0.2617,   0.3289,   0.3427,   1.1887,   1.6924],\n",
      "        [ -6.9537, -14.9558,   0.2441,   0.3521,   0.3582,   1.2932,   1.5254],\n",
      "        [-28.5216,   6.6192,   0.5371,   0.3706,   0.3775,   1.2871,   1.6367],\n",
      "        [-23.5827,  -1.4264,   0.5332,   0.3584,   0.3582,   1.1452,   1.4521],\n",
      "        [-18.9611,  16.5943,   0.3242,   0.3548,   0.3665,   1.2709,   1.6660],\n",
      "        [ 25.8872,   2.4469,   0.7637,   0.3408,   0.3546,   1.3056,   1.6318],\n",
      "        [-12.6579, -18.6995,   0.2441,   0.4224,   0.4333,   1.1756,   1.6582],\n",
      "        [ -6.8389, -13.1139,   0.3984,   0.3608,   0.3683,   1.3716,   1.4189],\n",
      "        [-17.2036,   0.2676,   0.2559,   0.3477,   0.3538,   1.2145,   1.7783],\n",
      "        [-24.0932,  -4.3584,   0.6113,   0.3360,   0.3398,   1.1358,   1.5596],\n",
      "        [-23.1902,  -4.9813,   0.4844,   0.3462,   0.3599,   1.3384,   1.6777],\n",
      "        [ 13.9505,   1.8688,   0.7520,   0.3538,   0.3500,   1.2997,   1.6465],\n",
      "        [-20.7687,  -4.8666,   0.1992,   0.3450,   0.3487,   1.0493,   1.6270],\n",
      "        [-17.0109,  12.8348,   0.9219,   0.3511,   0.3429,   1.1980,   1.9238],\n",
      "        [-23.0441,  -6.9011,   0.2266,   0.3547,   0.3577,   1.3042,   1.5225]],\n",
      "       device='cuda:0'), 'scores': tensor([0.7637, 0.7633, 0.7437, 0.7147, 0.7132, 0.7098, 0.6918, 0.6693, 0.6437,\n",
      "        0.6432, 0.6211, 0.6207, 0.6144, 0.6032, 0.6025, 0.5987, 0.5840, 0.5737,\n",
      "        0.5692, 0.5571, 0.5559, 0.5551, 0.5455, 0.5380, 0.5273, 0.5243, 0.5199,\n",
      "        0.5186, 0.5075, 0.5052, 0.5048, 0.5011, 0.4992, 0.4942, 0.4806, 0.4783,\n",
      "        0.4777, 0.4674, 0.4669, 0.4641, 0.4637, 0.4619, 0.4589, 0.4566, 0.4454,\n",
      "        0.4370, 0.4344, 0.4306, 0.4303, 0.4298, 0.4264], device='cuda:0'), 'label_preds': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0], device='cuda:0'), 'metadata': {'token': '29e056fc277543c9bc42310b122c640e'}}]\n",
      "6 6 6\n",
      "[0.7636601  0.76330733 0.74372685 0.71472764 0.7132319  0.7098244 ]\n",
      "0 0 0\n",
      "[]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'box3d_lidar': tensor([[ -2.1326,   3.3519,   0.4355,   0.3444,   0.3502,   0.7319,   1.5732],\n",
      "        [  7.2827,   4.1399,   0.4492,   0.3524,   0.3476,   0.7408,   1.5000],\n",
      "        [  3.0227,   3.6134,   0.4551,   0.3464,   0.3449,   0.7461,   1.5898],\n",
      "        [  9.6846,   5.8746,   0.4355,   0.3443,   0.3484,   0.7421,   1.3457],\n",
      "        [ -5.6090,   3.4148,   0.4395,   0.3428,   0.3393,   0.7531,   1.5186],\n",
      "        [  9.6393,   8.2827,   0.5352,   0.3356,   0.3367,   0.7472,   1.5742],\n",
      "        [ 28.7198,   5.7545,   0.3691,   0.3632,   0.3797,   1.2370,   1.6582],\n",
      "        [-25.7222,   7.9183,   0.2852,   0.3434,   0.3552,   1.2932,   1.6162],\n",
      "        [-26.4117,  12.4297,   0.5957,   0.3649,   0.3620,   1.2887,   1.5293],\n",
      "        [  9.7401,   7.5709,   0.4629,   0.3423,   0.3404,   0.7877,   1.4541],\n",
      "        [-10.6447, -11.6956,   0.6172,   0.3542,   0.3687,   1.3117,   1.5322],\n",
      "        [-28.2675,  14.0114,   0.2793,   0.3444,   0.3555,   1.2782,   1.7031],\n",
      "        [  2.6127,  19.1639,   0.4629,   0.3654,   0.3786,   1.3098,   1.6172],\n",
      "        [-22.2942,   7.7859,   0.5254,   0.3858,   0.3888,   1.3189,   1.5068],\n",
      "        [-25.4045,  12.4590,   0.6348,   0.3598,   0.3596,   1.2743,   1.7207],\n",
      "        [-27.2042,  12.7199,   0.3184,   0.3496,   0.3576,   1.2748,   1.5186],\n",
      "        [ 26.3746,   5.8918,   0.2754,   0.3519,   0.3643,   1.2732,   1.6377],\n",
      "        [  0.2411,  19.2589,   0.2812,   0.4061,   0.4209,   1.2431,   1.5938],\n",
      "        [ 12.0694,  18.9191,   0.2676,   0.3405,   0.3540,   1.2985,   1.4453],\n",
      "        [ -6.9089,  -9.0278,   0.5977,   0.3505,   0.3577,   1.3012,   1.5781],\n",
      "        [ 25.1782,  12.2277,   0.5000,   0.3421,   0.3433,   1.2288,   1.5303],\n",
      "        [ 10.3522, -18.1694,   0.0957,   0.3251,   0.3249,   0.8103,   1.6631],\n",
      "        [ 25.1290,  17.1476,   0.3027,   0.3724,   0.3768,   1.2655,   1.7578],\n",
      "        [-24.6354,  12.8423,   0.3320,   0.3592,   0.3749,   1.2887,   1.7148],\n",
      "        [ -3.4361,  12.8314,   0.8027,   0.3875,   0.3896,   1.3029,   1.6348],\n",
      "        [ -1.2137, -13.9054,   0.7090,   0.3459,   0.3463,   1.2116,   1.8828],\n",
      "        [-29.2248,  12.3954,   0.6641,   0.3581,   0.3578,   1.3154,   1.5625],\n",
      "        [ -5.3365,  -9.0615,   0.7168,   0.3448,   0.3430,   1.1921,   1.5674],\n",
      "        [-23.4852,  12.4106,   0.3555,   0.3439,   0.3483,   1.3066,   1.7041],\n",
      "        [-28.0855,  13.3445,   0.7363,   0.3935,   0.3936,   1.3733,   1.6416],\n",
      "        [-22.5679,  12.2367,   0.2539,   0.3367,   0.3467,   1.2821,   1.6084],\n",
      "        [-20.9822,  10.0735,   0.4004,   0.3572,   0.3723,   1.2375,   1.7207],\n",
      "        [ -8.6028, -11.8029,   0.3809,   0.3453,   0.3381,   0.8166,   1.5947],\n",
      "        [ 27.8837,  -9.1807,   0.4844,   0.3626,   0.3749,   1.2642,   1.7715],\n",
      "        [ 22.8969, -13.9213,   0.3184,   0.3403,   0.3439,   1.2918,   1.4375],\n",
      "        [ 10.4489,  18.1663,   0.3262,   0.3574,   0.3663,   1.2653,   1.6367],\n",
      "        [  8.9570,  18.2295,   0.3555,   0.3306,   0.3439,   1.3430,   1.5635],\n",
      "        [  2.1362, -12.8845,   0.4277,   0.3555,   0.3622,   1.0540,   1.3867],\n",
      "        [-11.3701, -11.7645,   0.3145,   0.3556,   0.3568,   1.3175,   1.5156],\n",
      "        [ -6.0525,  10.6821,   1.0527,   0.6118,   0.6507,   1.7804,   1.6816],\n",
      "        [ 11.2469,  18.4849,   0.2949,   0.3420,   0.3488,   1.2463,   1.5264],\n",
      "        [ -2.1438,  16.8624,   0.6094,   0.3424,   0.3511,   1.3018,   1.7451],\n",
      "        [-10.6175,  18.7535,   0.2930,   0.3404,   0.3522,   1.2910,   1.6592],\n",
      "        [ 26.2186, -12.7168,   0.7676,   0.6050,   0.6233,   1.7512,   1.6270],\n",
      "        [  9.3352,  19.0277,   0.7871,   0.3479,   0.3542,   1.3693,   1.5684],\n",
      "        [  5.8419,  19.3128,   0.5957,   0.3551,   0.3602,   1.3135,   1.6484]],\n",
      "       device='cuda:0'), 'scores': tensor([0.8027, 0.7886, 0.7724, 0.7580, 0.7355, 0.6926, 0.6794, 0.6579, 0.6501,\n",
      "        0.6420, 0.6214, 0.6134, 0.6038, 0.5867, 0.5812, 0.5624, 0.5614, 0.5605,\n",
      "        0.5569, 0.5393, 0.5358, 0.5291, 0.5290, 0.5158, 0.5150, 0.4966, 0.4945,\n",
      "        0.4902, 0.4871, 0.4823, 0.4781, 0.4736, 0.4716, 0.4596, 0.4541, 0.4507,\n",
      "        0.4467, 0.4463, 0.4444, 0.4420, 0.4307, 0.4245, 0.4226, 0.4169, 0.4076,\n",
      "        0.4040], device='cuda:0'), 'label_preds': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
      "       device='cuda:0'), 'metadata': {'token': 'c5a8f6d7a0fe4858b318aeac2524098f'}}]\n",
      "5 5 5\n",
      "[0.80271024 0.78858316 0.772359   0.75797355 0.73545164]\n",
      "0 0 0\n",
      "[]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-700852c098e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m visualize_evaluation('/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/cones_pp_v4/pipeline.config',\n\u001b[0m\u001b[1;32m      2\u001b[0m                      \u001b[0;34m'/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/cones_pp_v4/2021-2-11_9:58'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                      \u001b[0;34m'/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/cones_pp_v4/2021-2-11_9:58/voxelnet-7000.tckpt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                      )\n",
      "\u001b[0;32m<ipython-input-6-52bd1827c7de>\u001b[0m in \u001b[0;36mvisualize_evaluation\u001b[0;34m(config_path, model_dir, pretrained_path, multi_gpu)\u001b[0m\n\u001b[1;32m    129\u001b[0m                                        \u001b[0mdetection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                                        target_assigner.classes, geo)\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mo3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_geometries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "visualize_evaluation('/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/cones_pp_v4/pipeline.config',\n",
    "                     '/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/cones_pp_v4/2021-2-11_9:58', \n",
    "                     '/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/cones_pp_v4/2021-2-11_9:58/voxelnet-7000.tckpt',\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-plant",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config_path = '/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/cones_pp_v4/pipeline.config'\n",
    "model_dir = '/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/cones_pp_v4/2021-2-11_9:58'\n",
    "pretrained_path = '/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/cones_pp_v4/2021-2-11_9:58/voxelnet-7000.tckpt'\n",
    "multi_gpu = False\n",
    "if isinstance(config_path, str):\n",
    "    config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "    with open(config_path, \"r\") as f:\n",
    "        proto_str = f.read()\n",
    "        text_format.Merge(proto_str, config)\n",
    "else:\n",
    "    config = config_path\n",
    "    proto_str = text_format.MessageToString(config, indent=2)\n",
    "\n",
    "\n",
    "# Read config file\n",
    "input_cfg = config.train_input_reader\n",
    "eval_input_cfg = config.eval_input_reader\n",
    "model_cfg = config.model.second  # model's config\n",
    "train_cfg = config.train_config\n",
    "\n",
    "# Build neural network\n",
    "net = build_network(model_cfg).to(device)\n",
    "\n",
    "# Build Model\n",
    "target_assigner = net.target_assigner\n",
    "voxel_generator = net.voxel_generator\n",
    "print(\"num parameter: \", len(list(net.parameters())))\n",
    "torchplus.train.try_restore_latest_checkpoints(model_dir, [net])\n",
    "\n",
    "if pretrained_path is not None:\n",
    "    print('warning pretrain is loaded after restore, careful with resume')\n",
    "    model_dict = net.state_dict()\n",
    "    pretrained_dict = torch.load(pretrained_path)\n",
    "\n",
    "    new_pretrained_dict = {}\n",
    "    for k, v in pretrained_dict.items():\n",
    "        if k in model_dict and v.shape == model_dict[k].shape:\n",
    "            new_pretrained_dict[k] = v\n",
    "    print(\"Load pretrained parameters: \")\n",
    "    for k, v in new_pretrained_dict.items():\n",
    "        print(k, v.shape)\n",
    "    model_dict.update(new_pretrained_dict)\n",
    "    net.load_state_dict(model_dict)\n",
    "    net.clear_global_step()\n",
    "    net.clear_metrics()\n",
    "if multi_gpu:\n",
    "    net_parallel = torch.nn.DataParallel(net)\n",
    "else:\n",
    "    net_parallel = net\n",
    "\n",
    "optimizer_cfg = train_cfg.optimizer\n",
    "loss_scale = train_cfg.loss_scale_factor\n",
    "fastai_optimizer = optimizer_builder.build(\n",
    "    optimizer_cfg,\n",
    "    net,\n",
    "    mixed=False,\n",
    "    loss_scale=loss_scale)\n",
    "if loss_scale < 0:\n",
    "    loss_scale = \"dynamic\"\n",
    "if train_cfg.enable_mixed_precision:\n",
    "    max_num_voxels = input_cfg.preprocess.max_number_of_voxels * input_cfg.batch_size\n",
    "    print(\"max_num_voxels: %d\" % (max_num_voxels))\n",
    "\n",
    "    net, amp_optimizer = amp.initialize(net, fastai_optimizer,\n",
    "                                        opt_level=\"O1\",\n",
    "                                        keep_batchnorm_fp32=None,\n",
    "                                        loss_scale=loss_scale)\n",
    "    net.metrics_to_float()\n",
    "else:\n",
    "    amp_optimizer = fastai_optimizer\n",
    "torchplus.train.try_restore_latest_checkpoints(model_dir, [fastai_optimizer])\n",
    "lr_scheduler = lr_scheduler_builder.build(optimizer_cfg, amp_optimizer, train_cfg.steps)\n",
    "if train_cfg.enable_mixed_precision:\n",
    "    float_dtype = torch.float16\n",
    "else:\n",
    "    float_dtype = torch.float32\n",
    "\n",
    "if multi_gpu:\n",
    "    num_gpu = torch.cuda.device_count()\n",
    "    print(f\"MULTI_GPU: use {num_gpu} gpus\")\n",
    "    collate_fn = merge_second_batch_multigpu\n",
    "else:\n",
    "    collate_fn = merge_second_batch\n",
    "    num_gpu = 1\n",
    "\n",
    "eval_dataset = input_reader_builder.build(\n",
    "    eval_input_cfg,\n",
    "    model_cfg,\n",
    "    training=False,\n",
    "    voxel_generator=voxel_generator,\n",
    "    target_assigner=target_assigner)\n",
    "eval_dataloader = torch.utils.data.DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=input_cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=eval_input_cfg.preprocess.num_workers,\n",
    "    pin_memory=False,\n",
    "    collate_fn=merge_second_batch)\n",
    "\n",
    "# Start visualizing\n",
    "net.eval()\n",
    "# example = next(iter(dataloader))\n",
    "for example in iter(eval_dataloader):\n",
    "    detection = example_convert_to_torch(example, float_dtype)\n",
    "    detection = net(detection)\n",
    "    print(detection)\n",
    "    filtered_sample_tokens = eval_dataset.dataset.filtered_sample_tokens\n",
    "    # filtered_sample_tokens = dataset.dataset.filtered_sample_tokens\n",
    "    index = filtered_sample_tokens.index(detection[0]['metadata']['token'])\n",
    "\n",
    "    gt_example = eval_dataset.dataset.get_sensor_data(index)\n",
    "    # gt_example = dataset.dataset.get_sensor_data(index)\n",
    "    points = gt_example['lidar']['points']\n",
    "    pc_range = model_cfg.voxel_generator.point_cloud_range\n",
    "    points = np.array(\n",
    "        [p for p in points if (pc_range[0] < p[0] < pc_range[3]) & (pc_range[1] < p[1] < pc_range[4]) & (\n",
    "                pc_range[2] < p[2] < pc_range[5])])\n",
    "\n",
    "    gt_boxes = gt_example['lidar']['annotations']['boxes']\n",
    "    gt_labels = gt_example['lidar']['annotations']['names']\n",
    "    c = points[:, 3].reshape(-1, 1)\n",
    "    c = np.concatenate([c, c, c], axis=1)\n",
    "    points = points[:, 0:3]\n",
    "    pc = o3d.geometry.PointCloud()\n",
    "    pc.points = o3d.utility.Vector3dVector(points)\n",
    "    pc.colors = o3d.utility.Vector3dVector(c)\n",
    "    mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=2.0,\n",
    "                                                                   origin=[-0, -0, -0])\n",
    "    geo = [pc, mesh_frame]\n",
    "    geo = add_prediction_per_class(eval_dataset.dataset.nusc,\n",
    "                                   detection, gt_boxes, gt_labels,\n",
    "                                   target_assigner.classes, geo)\n",
    "    break\n",
    "    o3d.visualization.draw_geometries(geo)\n",
    "\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_boxes = detection[0]['box3d_lidar'].cpu().detach().numpy()\n",
    "det_labels = detection[0]['label_preds'].cpu().detach().numpy()\n",
    "det_scores = detection[0]['scores'].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-blanket",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_labels == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-liechtenstein",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_scores > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logical_and(det_scores > 0.5,det_labels == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-exhaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bitwise_and(det_scores > 0.5,det_labels == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-parker",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_boxes[np.logical_and(det_scores > 0.5,det_labels == 0)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

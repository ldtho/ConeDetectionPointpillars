{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fresh-redhead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0772,  0.0050]])\n",
      "tensor([[-0.7006,  1.5704]], device='cuda:0')\n",
      "tensor([[-0.0772,  0.0050]])\n",
      "False\n",
      "tensor([[-0.0772,  0.0050]], device='cuda:0')\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "from numba.core.errors import NumbaDeprecationWarning,NumbaPendingDeprecationWarning, NumbaWarning\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', category=NumbaDeprecationWarning)\n",
    "warnings.simplefilter('ignore', category=NumbaWarning)\n",
    "import sys\n",
    "sys.path.append('/kaggle/code/ConeDetectionPointpillars')\n",
    "\n",
    "from second.data.CustomNuscDataset import * #to register dataset\n",
    "from models import * #to register model\n",
    "import torch\n",
    "from second.utils.log_tool import SimpleModelLog\n",
    "from second.builder import target_assigner_builder, voxel_builder\n",
    "from second.protos import pipeline_pb2\n",
    "from second.pytorch.builder import (box_coder_builder, input_reader_builder,\n",
    "                                    lr_scheduler_builder, optimizer_builder,\n",
    "                                    second_builder)\n",
    "from second.pytorch.core import box_torch_ops\n",
    "import torchplus\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from google.protobuf import text_format\n",
    "# from lyft_dataset_sdk.utils.geometry_utils import *\n",
    "from lyft_dataset_sdk.lyftdataset import Quaternion\n",
    "from apex import amp\n",
    "# from lyft_dataset_sdk.utils.data_classes import Box\n",
    "from nuscenes.utils.geometry_utils import *\n",
    "from nuscenes.utils.data_classes import Box\n",
    "from second.utils.progress_bar import ProgressBar\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "#commented second.core.non_max_suppression, nms_cpu, __init__.py, \n",
    "# pytorch.core.box_torch_ops line 524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "atmospheric-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _second_det_to_nusc_box(detection):\n",
    "    from nuscenes.utils.data_classes import Box\n",
    "    import pyquaternion\n",
    "    box3d = detection[\"box3d_lidar\"].detach().cpu().numpy()\n",
    "    scores = detection[\"scores\"].detach().cpu().numpy()\n",
    "    labels = detection[\"label_preds\"].detach().cpu().numpy()\n",
    "    print(labels)\n",
    "    box3d[:, 6] = -box3d[:, 6] - np.pi / 2\n",
    "    box_list = []\n",
    "    for i in range(box3d.shape[0]):\n",
    "        quat = pyquaternion.Quaternion(axis=[0, 0, 1], radians=box3d[i, 6])\n",
    "        velocity = (np.nan, np.nan, np.nan)\n",
    "        if box3d.shape[1] == 9:\n",
    "            velocity = (*box3d[i, 7:9], 0.0)\n",
    "            # velo_val = np.linalg.norm(box3d[i, 7:9])\n",
    "            # velo_ori = box3d[i, 6]\n",
    "            # velocity = (velo_val * np.cos(velo_ori), velo_val * np.sin(velo_ori), 0.0)\n",
    "        box = Box(\n",
    "            box3d[i, :3],\n",
    "            box3d[i, 3:6],\n",
    "            quat,\n",
    "            label=labels[i],\n",
    "            score=scores[i],\n",
    "            velocity=velocity)\n",
    "        box_list.append(box)\n",
    "    return box_list\n",
    "def build_network(model_cfg, measure_time=False):\n",
    "    voxel_generator = voxel_builder.build(model_cfg.voxel_generator)\n",
    "    bv_range = voxel_generator.point_cloud_range[[0, 1, 3, 4]]\n",
    "    box_coder = box_coder_builder.build(model_cfg.box_coder)\n",
    "    target_assigner_cfg = model_cfg.target_assigner\n",
    "    target_assigner = target_assigner_builder.build(target_assigner_cfg,\n",
    "                                                    bv_range, box_coder)\n",
    "    box_coder.custom_ndim = target_assigner._anchor_generators[0].custom_ndim\n",
    "    net = second_builder.build(\n",
    "        model_cfg, voxel_generator, target_assigner, measure_time=measure_time)\n",
    "    return net\n",
    "\n",
    "\n",
    "def merge_second_batch_multigpu(batch_list):\n",
    "    if isinstance(batch_list[0], list):\n",
    "        batch_list_c = []\n",
    "        for example in batch_list:\n",
    "            batch_list_c += example\n",
    "        batch_list = batch_list_c\n",
    "\n",
    "    example_merged = defaultdict(list)\n",
    "    for example in batch_list:\n",
    "        for k, v in example.items():\n",
    "            example_merged[k].append(v)\n",
    "    ret = {}\n",
    "    for key, elems in example_merged.items():\n",
    "        if key == 'metadata':\n",
    "            ret[key] = elems\n",
    "        elif key == \"calib\":\n",
    "            ret[key] = {}\n",
    "            for elem in elems:\n",
    "                for k1, v1 in elem.items():\n",
    "                    if k1 not in ret[key]:\n",
    "                        ret[key][k1] = [v1]\n",
    "                    else:\n",
    "                        ret[key][k1].append(v1)\n",
    "            for k1, v1 in ret[key].items():\n",
    "                ret[key][k1] = np.stack(v1, axis=0)\n",
    "        elif key == 'coordinates':\n",
    "            coors = []\n",
    "            for i, coor in enumerate(elems):\n",
    "                coor_pad = np.pad(\n",
    "                    coor, ((0, 0), (1, 0)), mode='constant', constant_values=i)\n",
    "                coors.append(coor_pad)\n",
    "            ret[key] = np.stack(coors, axis=0)\n",
    "        elif key in ['gt_names', 'gt_classes', 'gt_boxes']:\n",
    "            continue\n",
    "        else:\n",
    "            ret[key] = np.stack(elems, axis=0)\n",
    "\n",
    "    return ret\n",
    "\n",
    "def buildBBox(points,color = [1,0,0]):\n",
    "    #print(\"Let's draw a cubic using o3d.geometry.LineSet\")\n",
    "    # points = [[0, 0, 0], [1, 0, 0], [0, 1, 0], [1, 1, 0], [0, 0, 1], [1, 0, 1],\n",
    "    #           [0, 1, 1], [1, 1, 1]] x0y0z0, x0y0z1, x0y1z0, x0y1z1, x1y0z0, x1y0z1, x1y1z0, x1y1z1\n",
    "\n",
    "    points = points[[0,4,3,7,1,5,2,6],:]\n",
    "    lines = [[0, 1], [0, 2], [1, 3], [2, 3], [4, 5], [4, 6], [5, 7], [6, 7],\n",
    "             [0, 4], [1, 5], [2, 6], [3, 7]]\n",
    "    colors = [color for i in range(len(lines))]\n",
    "    line_set = o3d.geometry.LineSet()\n",
    "    line_set.points = o3d.utility.Vector3dVector(points)\n",
    "    line_set.lines = o3d.utility.Vector2iVector(lines)\n",
    "    line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "\n",
    "    return  line_set\n",
    "def merge_second_batch(batch_list):\n",
    "    if isinstance(batch_list[0], list):\n",
    "        batch_list_c = []\n",
    "        for example in batch_list:\n",
    "            batch_list_c += example\n",
    "        batch_list = batch_list_c\n",
    "\n",
    "    example_merged = defaultdict(list)\n",
    "    for example in batch_list:\n",
    "        for k, v in example.items():\n",
    "            example_merged[k].append(v)\n",
    "    ret = {}\n",
    "    for key, elems in example_merged.items():\n",
    "        if key in [\n",
    "            'voxels', 'num_points', 'num_gt', 'voxel_labels', 'gt_names', 'gt_classes', 'gt_boxes'\n",
    "        ]:\n",
    "            ret[key] = np.concatenate(elems, axis=0)\n",
    "        elif key == 'metadata':\n",
    "            ret[key] = elems\n",
    "        elif key == \"calib\":\n",
    "            ret[key] = {}\n",
    "            for elem in elems:\n",
    "                for k1, v1 in elem.items():\n",
    "                    if k1 not in ret[key]:\n",
    "                        ret[key][k1] = [v1]\n",
    "                    else:\n",
    "                        ret[key][k1].append(v1)\n",
    "            for k1, v1 in ret[key].items():\n",
    "                ret[key][k1] = np.stack(v1, axis=0)\n",
    "        elif key == 'coordinates':\n",
    "            coors = []\n",
    "            for i, coor in enumerate(elems):\n",
    "                coor_pad = np.pad(\n",
    "                    coor, ((0, 0), (1, 0)), mode='constant', constant_values=i)\n",
    "                coors.append(coor_pad)\n",
    "            ret[key] = np.concatenate(coors, axis=0)\n",
    "        elif key == 'metrics':\n",
    "            ret[key] = elems\n",
    "        else:\n",
    "            ret[key] = np.stack(elems, axis=0)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def _worker_init_fn(worker_id):\n",
    "    time_seed = np.array(time.time(), dtype=np.int32)\n",
    "    np.random.seed(time_seed + worker_id)\n",
    "\n",
    "\n",
    "def example_convert_to_torch(example, dtype=torch.float32,\n",
    "                             device=None) -> dict:\n",
    "    device = device or torch.device(\"cuda:0\")\n",
    "    example_torch = {}\n",
    "    float_names = [\n",
    "        \"voxels\", \"anchors\", \"reg_targets\", \"reg_weights\", \"bev_map\", \"importance\"\n",
    "    ]\n",
    "    for k, v in example.items():\n",
    "        if k in ['gt_names', 'gt_classes', 'gt_boxes', 'points']:\n",
    "            example_torch[k] = example[k]\n",
    "            continue\n",
    "\n",
    "        if k in float_names:\n",
    "            # slow when directly provide fp32 data with dtype=torch.half\n",
    "            example_torch[k] = torch.tensor(\n",
    "                v, dtype=torch.float32, device=device).to(dtype)\n",
    "        elif k in [\"coordinates\", \"labels\", \"num_points\"]:\n",
    "            example_torch[k] = torch.tensor(\n",
    "                v, dtype=torch.int32, device=device)\n",
    "        elif k in [\"anchors_mask\"]:\n",
    "            example_torch[k] = torch.tensor(\n",
    "                v, dtype=torch.uint8, device=device)\n",
    "        elif k == \"calib\":\n",
    "            calib = {}\n",
    "            for k1, v1 in v.items():\n",
    "                calib[k1] = torch.tensor(\n",
    "                    v1, dtype=dtype, device=device).to(dtype)\n",
    "            example_torch[k] = calib\n",
    "        elif k == \"num_voxels\":\n",
    "            example_torch[k] = torch.tensor(v)\n",
    "        else:\n",
    "            example_torch[k] = v\n",
    "    return example_torch\n",
    "\n",
    "\n",
    "def time_to_str(t, mode='min'):\n",
    "    if mode == 'min':\n",
    "        t = int(t) / 60\n",
    "        hr = t // 60\n",
    "        min = t % 60\n",
    "        return '%2d hr %02d m' % (hr, min)\n",
    "\n",
    "    elif mode == 'sec':\n",
    "        t = int(t)\n",
    "        min = t // 60\n",
    "        sec = t % 60\n",
    "        return '%2d min %02d sec' % (min, sec)\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "amber-jaguar",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '/kaggle/code/ConeDetectionPointpillars/customNuscenes/configs/cones_pp_full_aug.config'\n",
    "# config_path = '/kaggle/code/ConeDetectionPointpillars/customNuscenes/configs/cones_pp_initial_v2.config'\n",
    "# config_path = '/kaggle/code/ConeDetectionPointpillars/customNuscenes/configs/cones_pp_initialak.config'\n",
    "model_dir = f'/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/cones_pp_full_aug/2021-2-13_11:1'\n",
    "result_path = None\n",
    "create_folder = False\n",
    "display_step = 100\n",
    "# pretrained_path=\"/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/29-1-2021_9:53/voxelnet-5850.tckpt\"\n",
    "pretrained_path = '/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/cones_pp_full_aug/2021-2-13_11:1/voxelnet-35000.tckpt'\n",
    "multi_gpu = False\n",
    "measure_time = False\n",
    "resume = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_dir = str(Path(model_dir).resolve())\n",
    "cur_time = time.localtime(time.time())\n",
    "cur_time = f'{cur_time.tm_year}-{cur_time.tm_mon}-{cur_time.tm_mday}_{cur_time.tm_hour}:{cur_time.tm_min}'\n",
    "if create_folder:\n",
    "    if Path(model_dir).exists():\n",
    "        model_dir = torchplus.train.create_folder(model_dir)\n",
    "model_dir = Path(model_dir)\n",
    "\n",
    "if not resume and model_dir.exists():\n",
    "    raise ValueError(\"model dir exists and you don't specify resume\")\n",
    "\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "if result_path is None:\n",
    "    result_path = model_dir / 'results'/ cur_time\n",
    "config_file_bkp = \"pipeline.config\"\n",
    "\n",
    "if isinstance(config_path, str):\n",
    "    config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "    with open(config_path, \"r\") as f:\n",
    "        proto_str = f.read()\n",
    "        text_format.Merge(proto_str, config)\n",
    "else:\n",
    "    config = config_path\n",
    "    proto_str = text_format.MessageToString(config, indent=2)\n",
    "\n",
    "with (model_dir / config_file_bkp).open('w') as f:\n",
    "    f.write(proto_str)\n",
    "# Read config file\n",
    "input_cfg = config.train_input_reader\n",
    "eval_input_cfg = config.eval_input_reader\n",
    "model_cfg = config.model.second  # model's config\n",
    "train_cfg = config.train_config  # training config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "planned-permission",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num parameter:  64\n",
      "Restoring parameters from /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/cones_pp_full_aug/2021-2-13_11:1/voxelnet-35000.tckpt\n"
     ]
    }
   ],
   "source": [
    "# Build neural network\n",
    "net = build_network(model_cfg, measure_time).to(device)\n",
    "\n",
    "target_assigner = net.target_assigner\n",
    "voxel_generator = net.voxel_generator\n",
    "print(\"num parameter: \", len(list(net.parameters())))\n",
    "torchplus.train.try_restore_latest_checkpoints(model_dir, [net])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "exact-hours",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning pretrain is loaded after restore, careful with resume\n",
      "Load pretrained parameters: \n",
      "global_step torch.Size([1])\n",
      "voxel_feature_extractor.pfn_layers.0.linear.weight torch.Size([64, 10])\n",
      "voxel_feature_extractor.pfn_layers.0.norm.weight torch.Size([64])\n",
      "voxel_feature_extractor.pfn_layers.0.norm.bias torch.Size([64])\n",
      "voxel_feature_extractor.pfn_layers.0.norm.running_mean torch.Size([64])\n",
      "voxel_feature_extractor.pfn_layers.0.norm.running_var torch.Size([64])\n",
      "voxel_feature_extractor.pfn_layers.0.norm.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.0.1.weight torch.Size([64, 64, 3, 3])\n",
      "rpn.blocks.0.2.weight torch.Size([64])\n",
      "rpn.blocks.0.2.bias torch.Size([64])\n",
      "rpn.blocks.0.2.running_mean torch.Size([64])\n",
      "rpn.blocks.0.2.running_var torch.Size([64])\n",
      "rpn.blocks.0.2.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.0.4.weight torch.Size([64, 64, 3, 3])\n",
      "rpn.blocks.0.5.weight torch.Size([64])\n",
      "rpn.blocks.0.5.bias torch.Size([64])\n",
      "rpn.blocks.0.5.running_mean torch.Size([64])\n",
      "rpn.blocks.0.5.running_var torch.Size([64])\n",
      "rpn.blocks.0.5.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.0.7.weight torch.Size([64, 64, 3, 3])\n",
      "rpn.blocks.0.8.weight torch.Size([64])\n",
      "rpn.blocks.0.8.bias torch.Size([64])\n",
      "rpn.blocks.0.8.running_mean torch.Size([64])\n",
      "rpn.blocks.0.8.running_var torch.Size([64])\n",
      "rpn.blocks.0.8.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.0.10.weight torch.Size([64, 64, 3, 3])\n",
      "rpn.blocks.0.11.weight torch.Size([64])\n",
      "rpn.blocks.0.11.bias torch.Size([64])\n",
      "rpn.blocks.0.11.running_mean torch.Size([64])\n",
      "rpn.blocks.0.11.running_var torch.Size([64])\n",
      "rpn.blocks.0.11.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.1.1.weight torch.Size([128, 64, 3, 3])\n",
      "rpn.blocks.1.2.weight torch.Size([128])\n",
      "rpn.blocks.1.2.bias torch.Size([128])\n",
      "rpn.blocks.1.2.running_mean torch.Size([128])\n",
      "rpn.blocks.1.2.running_var torch.Size([128])\n",
      "rpn.blocks.1.2.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.1.4.weight torch.Size([128, 128, 3, 3])\n",
      "rpn.blocks.1.5.weight torch.Size([128])\n",
      "rpn.blocks.1.5.bias torch.Size([128])\n",
      "rpn.blocks.1.5.running_mean torch.Size([128])\n",
      "rpn.blocks.1.5.running_var torch.Size([128])\n",
      "rpn.blocks.1.5.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.1.7.weight torch.Size([128, 128, 3, 3])\n",
      "rpn.blocks.1.8.weight torch.Size([128])\n",
      "rpn.blocks.1.8.bias torch.Size([128])\n",
      "rpn.blocks.1.8.running_mean torch.Size([128])\n",
      "rpn.blocks.1.8.running_var torch.Size([128])\n",
      "rpn.blocks.1.8.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.1.10.weight torch.Size([128, 128, 3, 3])\n",
      "rpn.blocks.1.11.weight torch.Size([128])\n",
      "rpn.blocks.1.11.bias torch.Size([128])\n",
      "rpn.blocks.1.11.running_mean torch.Size([128])\n",
      "rpn.blocks.1.11.running_var torch.Size([128])\n",
      "rpn.blocks.1.11.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.1.13.weight torch.Size([128, 128, 3, 3])\n",
      "rpn.blocks.1.14.weight torch.Size([128])\n",
      "rpn.blocks.1.14.bias torch.Size([128])\n",
      "rpn.blocks.1.14.running_mean torch.Size([128])\n",
      "rpn.blocks.1.14.running_var torch.Size([128])\n",
      "rpn.blocks.1.14.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.1.16.weight torch.Size([128, 128, 3, 3])\n",
      "rpn.blocks.1.17.weight torch.Size([128])\n",
      "rpn.blocks.1.17.bias torch.Size([128])\n",
      "rpn.blocks.1.17.running_mean torch.Size([128])\n",
      "rpn.blocks.1.17.running_var torch.Size([128])\n",
      "rpn.blocks.1.17.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.2.1.weight torch.Size([256, 128, 3, 3])\n",
      "rpn.blocks.2.2.weight torch.Size([256])\n",
      "rpn.blocks.2.2.bias torch.Size([256])\n",
      "rpn.blocks.2.2.running_mean torch.Size([256])\n",
      "rpn.blocks.2.2.running_var torch.Size([256])\n",
      "rpn.blocks.2.2.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.2.4.weight torch.Size([256, 256, 3, 3])\n",
      "rpn.blocks.2.5.weight torch.Size([256])\n",
      "rpn.blocks.2.5.bias torch.Size([256])\n",
      "rpn.blocks.2.5.running_mean torch.Size([256])\n",
      "rpn.blocks.2.5.running_var torch.Size([256])\n",
      "rpn.blocks.2.5.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.2.7.weight torch.Size([256, 256, 3, 3])\n",
      "rpn.blocks.2.8.weight torch.Size([256])\n",
      "rpn.blocks.2.8.bias torch.Size([256])\n",
      "rpn.blocks.2.8.running_mean torch.Size([256])\n",
      "rpn.blocks.2.8.running_var torch.Size([256])\n",
      "rpn.blocks.2.8.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.2.10.weight torch.Size([256, 256, 3, 3])\n",
      "rpn.blocks.2.11.weight torch.Size([256])\n",
      "rpn.blocks.2.11.bias torch.Size([256])\n",
      "rpn.blocks.2.11.running_mean torch.Size([256])\n",
      "rpn.blocks.2.11.running_var torch.Size([256])\n",
      "rpn.blocks.2.11.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.2.13.weight torch.Size([256, 256, 3, 3])\n",
      "rpn.blocks.2.14.weight torch.Size([256])\n",
      "rpn.blocks.2.14.bias torch.Size([256])\n",
      "rpn.blocks.2.14.running_mean torch.Size([256])\n",
      "rpn.blocks.2.14.running_var torch.Size([256])\n",
      "rpn.blocks.2.14.num_batches_tracked torch.Size([])\n",
      "rpn.blocks.2.16.weight torch.Size([256, 256, 3, 3])\n",
      "rpn.blocks.2.17.weight torch.Size([256])\n",
      "rpn.blocks.2.17.bias torch.Size([256])\n",
      "rpn.blocks.2.17.running_mean torch.Size([256])\n",
      "rpn.blocks.2.17.running_var torch.Size([256])\n",
      "rpn.blocks.2.17.num_batches_tracked torch.Size([])\n",
      "rpn.deblocks.0.0.weight torch.Size([128, 64, 4, 4])\n",
      "rpn.deblocks.0.1.weight torch.Size([128])\n",
      "rpn.deblocks.0.1.bias torch.Size([128])\n",
      "rpn.deblocks.0.1.running_mean torch.Size([128])\n",
      "rpn.deblocks.0.1.running_var torch.Size([128])\n",
      "rpn.deblocks.0.1.num_batches_tracked torch.Size([])\n",
      "rpn.deblocks.1.0.weight torch.Size([128, 128, 2, 2])\n",
      "rpn.deblocks.1.1.weight torch.Size([128])\n",
      "rpn.deblocks.1.1.bias torch.Size([128])\n",
      "rpn.deblocks.1.1.running_mean torch.Size([128])\n",
      "rpn.deblocks.1.1.running_var torch.Size([128])\n",
      "rpn.deblocks.1.1.num_batches_tracked torch.Size([])\n",
      "rpn.deblocks.2.0.weight torch.Size([256, 128, 1, 1])\n",
      "rpn.deblocks.2.1.weight torch.Size([128])\n",
      "rpn.deblocks.2.1.bias torch.Size([128])\n",
      "rpn.deblocks.2.1.running_mean torch.Size([128])\n",
      "rpn.deblocks.2.1.running_var torch.Size([128])\n",
      "rpn.deblocks.2.1.num_batches_tracked torch.Size([])\n",
      "rpn.conv_cls.weight torch.Size([2, 384, 1, 1])\n",
      "rpn.conv_cls.bias torch.Size([2])\n",
      "rpn.conv_box.weight torch.Size([14, 384, 1, 1])\n",
      "rpn.conv_box.bias torch.Size([14])\n",
      "rpn_acc.total torch.Size([1])\n",
      "rpn_acc.count torch.Size([1])\n",
      "rpn_precision.total torch.Size([1])\n",
      "rpn_precision.count torch.Size([1])\n",
      "rpn_recall.total torch.Size([1])\n",
      "rpn_recall.count torch.Size([1])\n",
      "rpn_metrics.prec_total torch.Size([7])\n",
      "rpn_metrics.prec_count torch.Size([7])\n",
      "rpn_metrics.rec_total torch.Size([7])\n",
      "rpn_metrics.rec_count torch.Size([7])\n",
      "rpn_cls_loss.total torch.Size([1])\n",
      "rpn_cls_loss.count torch.Size([1])\n",
      "rpn_loc_loss.total torch.Size([1])\n",
      "rpn_loc_loss.count torch.Size([1])\n",
      "rpn_total_loss.total torch.Size([1])\n",
      "rpn_total_loss.count torch.Size([1])\n",
      "False _amp_stash\n",
      "max_num_voxels: 70000\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
      "Restoring parameters from /kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/cones_pp_full_aug/2021-2-13_11:1/adam_optimizer-35000.tckpt\n"
     ]
    }
   ],
   "source": [
    "if pretrained_path is not None:\n",
    "    print('warning pretrain is loaded after restore, careful with resume')\n",
    "    model_dict = net.state_dict()\n",
    "    pretrained_dict = torch.load(pretrained_path)\n",
    "\n",
    "    new_pretrained_dict = {}\n",
    "    for k, v in pretrained_dict.items():\n",
    "        if k in model_dict and v.shape == model_dict[k].shape:\n",
    "            new_pretrained_dict[k] = v\n",
    "    print(\"Load pretrained parameters: \")\n",
    "    for k, v in new_pretrained_dict.items():\n",
    "        print(k, v.shape)\n",
    "    model_dict.update(new_pretrained_dict)\n",
    "    net.load_state_dict(model_dict)\n",
    "    net.clear_global_step()\n",
    "    net.clear_metrics()\n",
    "if multi_gpu:\n",
    "    net_parallel = torch.nn.DataParallel(net)\n",
    "else:\n",
    "    net_parallel = net\n",
    "optimizer_cfg = train_cfg.optimizer\n",
    "loss_scale = train_cfg.loss_scale_factor\n",
    "fastai_optimizer = optimizer_builder.build(\n",
    "    optimizer_cfg,\n",
    "    net,\n",
    "    mixed=False,\n",
    "    loss_scale=loss_scale)\n",
    "if loss_scale < 0:\n",
    "    loss_scale = \"dynamic\"\n",
    "if train_cfg.enable_mixed_precision:\n",
    "    max_num_voxels = input_cfg.preprocess.max_number_of_voxels * input_cfg.batch_size\n",
    "    print(\"max_num_voxels: %d\" % (max_num_voxels))\n",
    "\n",
    "    net, amp_optimizer = amp.initialize(net, fastai_optimizer,\n",
    "                                        opt_level=\"O1\",\n",
    "                                        keep_batchnorm_fp32=None,\n",
    "                                        loss_scale=loss_scale)\n",
    "    net.metrics_to_float()\n",
    "else:\n",
    "    amp_optimizer = fastai_optimizer\n",
    "torchplus.train.try_restore_latest_checkpoints(model_dir, [fastai_optimizer])\n",
    "lr_scheduler = lr_scheduler_builder.build(optimizer_cfg, amp_optimizer, train_cfg.steps)\n",
    "\n",
    "if train_cfg.enable_mixed_precision:\n",
    "    float_dtype = torch.float16\n",
    "else:\n",
    "    float_dtype = torch.float32\n",
    "\n",
    "if multi_gpu:\n",
    "    num_gpu = torch.cuda.device_count()\n",
    "    print(f\"MULTI_GPU: use {num_gpu} gpus\")\n",
    "    collate_fn = merge_second_batch_multigpu\n",
    "else:\n",
    "    collate_fn = merge_second_batch\n",
    "    num_gpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ignored-samba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_map_size [1, 100, 150]\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-trainval...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "64386 instance,\n",
      "12 sensor,\n",
      "10200 calibrated_sensor,\n",
      "2631083 ego_pose,\n",
      "68 log,\n",
      "850 scene,\n",
      "34149 sample,\n",
      "2631083 sample_data,\n",
      "1166187 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 29.666 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 5.2 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "dataset = input_reader_builder.build(\n",
    "    input_cfg,\n",
    "    model_cfg,\n",
    "    training=True,\n",
    "    voxel_generator=voxel_generator,\n",
    "    target_assigner=target_assigner,\n",
    "    multi_gpu=multi_gpu\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=input_cfg.batch_size * num_gpu,\n",
    "    shuffle=True,\n",
    "    num_workers=input_cfg.preprocess.num_workers * num_gpu,\n",
    "    pin_memory=False,\n",
    "    collate_fn=collate_fn,\n",
    "    worker_init_fn=_worker_init_fn,\n",
    "    drop_last=not multi_gpu\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "broad-bahrain",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2021-02-23 12:06:32,960 - transforms - finding looplift candidates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.81603203  0.81709019  0.81762322 ...  0.60296096  0.61220096\n",
      "   0.61976863]\n",
      " [ 2.91564093  3.05792392  3.21156221 ... 10.0054728  10.03215923\n",
      "  10.02510468]\n",
      " [ 0.1343773   0.14571364  0.15668149 ...  3.2926691   3.53506267\n",
      "   3.77595573]\n",
      " [15.         15.         15.         ... 46.         36.\n",
      "  32.        ]]\n",
      "[[ 0.81603203  2.91564093  0.1343773  -0.44117647  0.        ]\n",
      " [ 0.81709019  3.05792392  0.14571364 -0.44117647  0.        ]\n",
      " [ 0.81762322  3.21156221  0.15668149 -0.44117647  0.        ]\n",
      " ...\n",
      " [ 0.60296096 10.0054728   3.2926691  -0.31960784  0.19962096]\n",
      " [ 0.61220096 10.03215923  3.53506267 -0.35882353  0.19962096]\n",
      " [ 0.61976863 10.02510468  3.77595573 -0.3745098   0.19962096]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'voxels': array([[[ 0.40021371,  3.08460096,  0.15569393, -0.44117647,\n",
       "           0.        ],\n",
       "         [ 0.41717715,  3.07956217,  0.15921866, -0.44117647,\n",
       "           0.        ],\n",
       "         [ 0.43371414,  3.07775214,  0.16078528, -0.44117647,\n",
       "           0.        ],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ]],\n",
       " \n",
       "        [[ 0.38742086,  3.22193075,  0.16668269, -0.44117647,\n",
       "           0.        ],\n",
       "         [ 0.30033613,  3.20813662,  0.17115533, -0.44509804,\n",
       "           0.        ],\n",
       "         [ 0.31610236,  3.2174703 ,  0.16700034, -0.44117647,\n",
       "           0.        ],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ]],\n",
       " \n",
       "        [[ 0.37301911,  3.37016136,  0.17731426, -0.44117647,\n",
       "           0.        ],\n",
       "         [ 0.39105645,  3.37022539,  0.17778119, -0.44901961,\n",
       "           0.        ],\n",
       "         [ 0.31234339,  3.36932187,  0.17672254, -0.44509804,\n",
       "           0.        ],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.58134417,  7.29491295,  0.33622539, -0.26078431,\n",
       "           0.19962096],\n",
       "         [-0.53906366,  7.28940106,  0.33760642, -0.31176471,\n",
       "           0.19962096],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ]],\n",
       " \n",
       "        [[-0.66951511,  8.58983668,  0.26910187, -0.34705882,\n",
       "           0.19962096],\n",
       "         [-0.61841498,  8.57339028,  0.27213416, -0.32745098,\n",
       "           0.19962096],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ]],\n",
       " \n",
       "        [[-0.16591757,  6.0941768 ,  0.44443913, -0.13529412,\n",
       "           0.19962096],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ]]]),\n",
       " 'num_points': array([22, 20, 33, ...,  2,  2,  1], dtype=int32),\n",
       " 'coordinates': array([[  0, 230, 304],\n",
       "        [  0, 232, 303],\n",
       "        [  0, 233, 303],\n",
       "        ...,\n",
       "        [  0, 272, 294],\n",
       "        [  0, 285, 293],\n",
       "        [  0, 260, 298]], dtype=int32),\n",
       " 'num_voxels': array([21201]),\n",
       " 'metrics': {'voxel_gene_time': 0.019827842712402344,\n",
       "  'prep_time': 0.6518383026123047},\n",
       " 'anchors': array([[-30.        , -20.        ,  -2.6       , ...,   0.40359262,\n",
       "           1.0623215 ,   0.        ],\n",
       "        [-29.597315  , -20.        ,  -2.6       , ...,   0.40359262,\n",
       "           1.0623215 ,   0.        ],\n",
       "        [-29.194632  , -20.        ,  -2.6       , ...,   0.40359262,\n",
       "           1.0623215 ,   0.        ],\n",
       "        ...,\n",
       "        [ 29.194632  ,  20.        ,  -2.6       , ...,   0.40359262,\n",
       "           1.0623215 ,   1.57      ],\n",
       "        [ 29.597315  ,  20.        ,  -2.6       , ...,   0.40359262,\n",
       "           1.0623215 ,   1.57      ],\n",
       "        [ 30.        ,  20.        ,  -2.6       , ...,   0.40359262,\n",
       "           1.0623215 ,   1.57      ]], dtype=float32),\n",
       " 'gt_names': array(['traffic_cone', 'traffic_cone', 'traffic_cone', 'traffic_cone',\n",
       "        'traffic_cone', 'traffic_cone', 'traffic_cone', 'traffic_cone'],\n",
       "       dtype='<U12'),\n",
       " 'labels': array([0, 0, 0, ..., 0, 0, 0], dtype=int32),\n",
       " 'reg_targets': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " 'importance': array([1., 1., 1., ..., 1., 1., 1.], dtype=float32),\n",
       " 'metadata': {'token': 'a0edf6edd121431785948038c54e036e'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "labeled-invitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.81603203  0.81709019  0.81762322 ...  0.60296096  0.61220096\n",
      "   0.61976863]\n",
      " [ 2.91564093  3.05792392  3.21156221 ... 10.0054728  10.03215923\n",
      "  10.02510468]\n",
      " [ 0.1343773   0.14571364  0.15668149 ...  3.2926691   3.53506267\n",
      "   3.77595573]\n",
      " [15.         15.         15.         ... 46.         36.\n",
      "  32.        ]]\n",
      "[[ 0.81603203  2.91564093  0.1343773  -0.44117647  0.        ]\n",
      " [ 0.81709019  3.05792392  0.14571364 -0.44117647  0.        ]\n",
      " [ 0.81762322  3.21156221  0.15668149 -0.44117647  0.        ]\n",
      " ...\n",
      " [ 0.60296096 10.0054728   3.2926691  -0.31960784  0.19962096]\n",
      " [ 0.61220096 10.03215923  3.53506267 -0.35882353  0.19962096]\n",
      " [ 0.61976863 10.02510468  3.77595573 -0.3745098   0.19962096]]\n",
      "voxels (22394, 100, 5)\n",
      "num_points (22394,)\n",
      "coordinates (22394, 3)\n",
      "num_voxels (1,)\n",
      "metrics dict\n",
      "anchors (30000, 7)\n",
      "gt_names (8,)\n",
      "labels (30000,)\n",
      "reg_targets (30000, 7)\n",
      "importance (30000,)\n",
      "metadata dict\n",
      "{'voxels': array([[[-1.2293828 , -2.52684823,  0.31437412, -0.44117647,\n",
      "          0.        ],\n",
      "        [-1.24610965, -2.51786656,  0.31815575, -0.44117647,\n",
      "          0.        ],\n",
      "        [-1.26309038, -2.51236984,  0.31983655, -0.44117647,\n",
      "          0.        ],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ]],\n",
      "\n",
      "       [[-1.24579709, -2.67391184,  0.32616378, -0.44117647,\n",
      "          0.        ],\n",
      "        [-1.26331357, -2.664758  ,  0.32975588, -0.44901961,\n",
      "          0.        ],\n",
      "        [-1.28097184, -2.65730076,  0.33234036, -0.44509804,\n",
      "          0.        ],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ]],\n",
      "\n",
      "       [[-1.26289082, -2.8327779 ,  0.33757022, -0.44117647,\n",
      "          0.        ],\n",
      "        [-1.28185527, -2.82892395,  0.33807118, -0.44901961,\n",
      "          0.        ],\n",
      "        [-1.21754316, -2.84211289,  0.33745314, -0.44509804,\n",
      "          0.        ],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-0.93241702, -6.06888905,  0.57960239, -0.23333333,\n",
      "          0.19962096],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ]],\n",
      "\n",
      "       [[-1.19701005, -7.09285032,  0.51945695, -0.24509804,\n",
      "          0.19962096],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ]],\n",
      "\n",
      "       [[-1.1872262 , -5.92701421,  0.60240166, -0.23333333,\n",
      "          0.19962096],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ]]]), 'num_points': array([24, 35, 24, ...,  1,  1,  1], dtype=int32), 'coordinates': array([[  0, 174, 287],\n",
      "       [  0, 173, 287],\n",
      "       [  0, 171, 287],\n",
      "       ...,\n",
      "       [  0, 139, 290],\n",
      "       [  0, 129, 288],\n",
      "       [  0, 140, 288]], dtype=int32), 'num_voxels': array([22394]), 'metrics': {'voxel_gene_time': 0.022377490997314453, 'prep_time': 0.026177644729614258}, 'anchors': array([[-30.        , -20.        ,  -2.6       , ...,   0.40359262,\n",
      "          1.0623215 ,   0.        ],\n",
      "       [-29.597315  , -20.        ,  -2.6       , ...,   0.40359262,\n",
      "          1.0623215 ,   0.        ],\n",
      "       [-29.194632  , -20.        ,  -2.6       , ...,   0.40359262,\n",
      "          1.0623215 ,   0.        ],\n",
      "       ...,\n",
      "       [ 29.194632  ,  20.        ,  -2.6       , ...,   0.40359262,\n",
      "          1.0623215 ,   1.57      ],\n",
      "       [ 29.597315  ,  20.        ,  -2.6       , ...,   0.40359262,\n",
      "          1.0623215 ,   1.57      ],\n",
      "       [ 30.        ,  20.        ,  -2.6       , ...,   0.40359262,\n",
      "          1.0623215 ,   1.57      ]], dtype=float32), 'gt_names': array(['traffic_cone', 'traffic_cone', 'traffic_cone', 'traffic_cone',\n",
      "       'traffic_cone', 'traffic_cone', 'traffic_cone', 'traffic_cone'],\n",
      "      dtype='<U12'), 'labels': array([0, 0, 0, ..., 0, 0, 0], dtype=int32), 'reg_targets': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'importance': array([1., 1., 1., ..., 1., 1., 1.], dtype=float32), 'metadata': {'token': 'a0edf6edd121431785948038c54e036e'}}\n"
     ]
    }
   ],
   "source": [
    "d = dataset[1]\n",
    "for k in d.keys():\n",
    "    if isinstance(d[k], dict):\n",
    "        print(k, \"dict\")\n",
    "        continue\n",
    "    print(k, d[k].shape)\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "opposed-register",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.81603203  0.81709019  0.81762322 ...  0.60296096  0.61220096\n",
      "   0.61976863]\n",
      " [ 2.91564093  3.05792392  3.21156221 ... 10.0054728  10.03215923\n",
      "  10.02510468]\n",
      " [ 0.1343773   0.14571364  0.15668149 ...  3.2926691   3.53506267\n",
      "   3.77595573]\n",
      " [15.         15.         15.         ... 46.         36.\n",
      "  32.        ]]\n",
      "[[ 0.81603203  2.91564093  0.1343773  -0.44117647  0.        ]\n",
      " [ 0.81709019  3.05792392  0.14571364 -0.44117647  0.        ]\n",
      " [ 0.81762322  3.21156221  0.15668149 -0.44117647  0.        ]\n",
      " ...\n",
      " [ 0.60296096 10.0054728   3.2926691  -0.31960784  0.19962096]\n",
      " [ 0.61220096 10.03215923  3.53506267 -0.35882353  0.19962096]\n",
      " [ 0.61976863 10.02510468  3.77595573 -0.3745098   0.19962096]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['voxels', 'num_points', 'coordinates', 'num_voxels', 'metrics', 'anchors', 'gt_names', 'labels', 'reg_targets', 'importance', 'metadata'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "coordinated-sellers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.80464275  0.80564527  0.80599708 ...  0.59282028  0.60208186\n",
      "   0.60958528]\n",
      " [ 2.91067808  3.04589131  3.19936666 ... 10.02903209 10.0436566\n",
      "  10.02856053]\n",
      " [ 0.13911276  0.15427481  0.16503456 ...  3.30142298  3.54246499\n",
      "   3.78220166]\n",
      " [16.         14.         13.         ... 48.         34.\n",
      "  32.        ]]\n",
      "[[ 0.80464275  2.91067808  0.13911276 -0.4372549   0.        ]\n",
      " [ 0.80564527  3.04589131  0.15427481 -0.44509804  0.        ]\n",
      " [ 0.80599708  3.19936666  0.16503456 -0.44901961  0.        ]\n",
      " ...\n",
      " [ 0.59282028 10.02903209  3.30142298 -0.31176471  0.19829798]\n",
      " [ 0.60208186 10.0436566   3.54246499 -0.36666667  0.19829798]\n",
      " [ 0.60958528 10.02856053  3.78220166 -0.3745098   0.19829798]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.as_tensor(dataset[0][\"coordinates\"][:,0])\n",
    "a.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "portable-prayer",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'unsqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-c81ad8e7d4e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'unsqueeze'"
     ]
    }
   ],
   "source": [
    "a.unsqueeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-socket",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-monroe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-tennessee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-courage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [line for line in dataset[0]['reg_targets'] for i in line if i != 0]\n",
    "# res = []\n",
    "# for line in dataset[1]['reg_targets']:\n",
    "#     for i in line:\n",
    "#         if i != 0:\n",
    "#             print(line)\n",
    "#             res.append(line)\n",
    "#             break\n",
    "# #     break\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-prevention",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([i for i in dataset[1]['labels'] if i == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-intervention",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "unknown-marker",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_map_size [1, 100, 150]\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.942 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = input_reader_builder.build(\n",
    "    eval_input_cfg,\n",
    "    model_cfg,\n",
    "    training=False,\n",
    "    voxel_generator=voxel_generator,\n",
    "    target_assigner=target_assigner)\n",
    "eval_dataloader = torch.utils.data.DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=input_cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=eval_input_cfg.preprocess.num_workers,\n",
    "    pin_memory=False,\n",
    "    collate_fn=merge_second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "atmospheric-amateur",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "opposite-trace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'voxels': array([[[ 2.87861935e-01, -6.90282527e+00, -2.68713817e-02,\n",
       "          -4.45098039e-01,  4.97839451e-02],\n",
       "         [ 2.08736939e-01, -6.90953233e+00, -2.69478965e-02,\n",
       "          -4.45098039e-01,  4.97839451e-02],\n",
       "         [ 2.63588540e-01, -6.97065800e+00, -2.70102925e-02,\n",
       "          -4.41176471e-01,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00]],\n",
       " \n",
       "        [[ 2.78481420e+00,  4.89472728e+00,  4.97334215e-03,\n",
       "          -4.37254902e-01,  1.99628115e-01],\n",
       "         [ 2.75554073e+00,  4.86044707e+00,  1.85989135e-02,\n",
       "          -3.66666667e-01,  0.00000000e+00],\n",
       "         [ 2.78066329e+00,  4.84081703e+00,  2.04177941e-02,\n",
       "          -3.70588235e-01,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00]],\n",
       " \n",
       "        [[ 4.50289967e+00,  1.09713373e+00,  1.47860804e-02,\n",
       "          -4.17647059e-01,  4.97839451e-02],\n",
       "         [ 4.50583335e+00,  1.07161970e+00,  1.59793379e-02,\n",
       "          -4.17647059e-01,  4.97839451e-02],\n",
       "         [ 4.51220531e+00,  1.04700137e+00,  1.56387001e-02,\n",
       "          -4.17647059e-01,  4.97839451e-02],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 9.29360164e+00,  8.33193104e+00,  3.91444988e-01,\n",
       "          -3.23529412e-01,  1.99628115e-01],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00]],\n",
       " \n",
       "        [[-2.36499260e+00,  7.19663439e+00,  2.85654311e-01,\n",
       "          -3.27450980e-01,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00]],\n",
       " \n",
       "        [[-1.33148174e+01, -3.70554990e+00, -3.42910208e-02,\n",
       "          -4.76470588e-01,  1.99628115e-01],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00]]]),\n",
       " 'num_points': array([5, 5, 4, ..., 1, 1, 1], dtype=int32),\n",
       " 'coordinates': array([[  0, 130, 302],\n",
       "        [  0, 248, 327],\n",
       "        [  0, 210, 345],\n",
       "        ...,\n",
       "        [  0, 283, 392],\n",
       "        [  0, 271, 276],\n",
       "        [  0, 162, 166]], dtype=int32),\n",
       " 'num_voxels': array([27478]),\n",
       " 'metrics': {'voxel_gene_time': 0.02937602996826172,\n",
       "  'prep_time': 0.1375129222869873},\n",
       " 'anchors': array([[-30.        , -20.        ,  -2.6       , ...,   0.40359262,\n",
       "           1.0623215 ,   0.        ],\n",
       "        [-29.597315  , -20.        ,  -2.6       , ...,   0.40359262,\n",
       "           1.0623215 ,   0.        ],\n",
       "        [-29.194632  , -20.        ,  -2.6       , ...,   0.40359262,\n",
       "           1.0623215 ,   0.        ],\n",
       "        ...,\n",
       "        [ 29.194632  ,  20.        ,  -2.6       , ...,   0.40359262,\n",
       "           1.0623215 ,   1.57      ],\n",
       "        [ 29.597315  ,  20.        ,  -2.6       , ...,   0.40359262,\n",
       "           1.0623215 ,   1.57      ],\n",
       "        [ 30.        ,  20.        ,  -2.6       , ...,   0.40359262,\n",
       "           1.0623215 ,   1.57      ]], dtype=float32),\n",
       " 'metadata': {'token': '4f545737bf3347fbbc9af60b0be9a963'}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "accredited-rental",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones([1, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-wagner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_dir = model_dir / cur_time\n",
    "model_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-patent",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model_logging = SimpleModelLog(model_dir)\n",
    "model_logging.open()\n",
    "model_logging.log_text(proto_str + \"\\n\", 0, tag=\"config\")\n",
    "\n",
    "start_step = net.get_global_step()\n",
    "total_step = train_cfg.steps\n",
    "t = time.time()\n",
    "steps_per_eval = train_cfg.steps_per_eval\n",
    "clear_metrics_every_epoch = train_cfg.clear_metrics_every_epoch\n",
    "\n",
    "amp_optimizer.zero_grad()\n",
    "step_times = []\n",
    "step = start_step\n",
    "run = True\n",
    "ave_valid_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-collar",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ver2\n",
    "print(total_step)\n",
    "try:\n",
    "    start_tic = time.time()\n",
    "    print(\"num samples: %d\" % (len(dataset)))\n",
    "    while run == True:\n",
    "        if clear_metrics_every_epoch:\n",
    "            net.clear_metrics()\n",
    "        for example in tqdm_notebook(dataloader):\n",
    "            lr_scheduler.step(net.get_global_step())\n",
    "            example.pop(\"metrics\")\n",
    "            example_torch = example_convert_to_torch(example, float_dtype)\n",
    "\n",
    "            ret_dict = net_parallel(example_torch)\n",
    "            loss = ret_dict[\"loss\"].mean()\n",
    "            cls_loss_reduced = ret_dict[\"cls_loss_reduced\"].mean()\n",
    "            loc_loss_reduced = ret_dict[\"loc_loss_reduced\"].mean()\n",
    "\n",
    "            if train_cfg.enable_mixed_precision:\n",
    "                if net.get_global_step() < 100:\n",
    "                    loss *= 1e-3\n",
    "                with amp.scale_loss(loss, amp_optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), 10.0)\n",
    "            amp_optimizer.step()\n",
    "            amp_optimizer.zero_grad()\n",
    "            net.update_global_step()\n",
    "\n",
    "            cls_preds = ret_dict[\"cls_preds\"]\n",
    "            labels = example_torch[\"labels\"]\n",
    "            cared = ret_dict[\"cared\"]\n",
    "\n",
    "            net_metrics = net.update_metrics(cls_loss_reduced,\n",
    "                                             loc_loss_reduced, cls_preds,\n",
    "                                             labels, cared)\n",
    "            step_time = (time.time() - t)\n",
    "            step_times.append(step_time)\n",
    "            t = time.time()\n",
    "            metrics = {}\n",
    "            global_step = net.get_global_step()\n",
    "\n",
    "            if global_step % display_step == 0:\n",
    "                net.eval()\n",
    "                det = net(example_torch)\n",
    "                net.train()\n",
    "                eta = time.time() - start_tic\n",
    "                if measure_time:\n",
    "                    for name, val in net.get_avg_time_dict().items():\n",
    "                        print(f\"avg {name} time = {val * 1000:.3f} ms\")\n",
    "\n",
    "                metrics[\"step\"] = global_step\n",
    "                metrics['epoch'] = global_step / len(dataloader)\n",
    "                metrics['steptime'] = np.mean(step_times)\n",
    "                metrics['valid'] = ave_valid_loss\n",
    "                step_times = []\n",
    "\n",
    "                metrics[\"loss\"] = net_metrics['loss']['cls_loss'] + net_metrics['loss']['loc_loss']\n",
    "                metrics[\"cls_loss\"] = net_metrics['loss']['cls_loss']\n",
    "                metrics[\"loc_loss\"] = net_metrics['loss']['loc_loss']\n",
    "\n",
    "                if model_cfg.use_direction_classifier:\n",
    "                    dir_loss_reduced = ret_dict[\"dir_loss_reduced\"].mean()\n",
    "                    metrics[\"dir_rt\"] = float(\n",
    "                        dir_loss_reduced.detach().cpu().numpy())\n",
    "\n",
    "                metrics['lr'] = float(amp_optimizer.lr)\n",
    "                metrics['eta'] = time_to_str(eta)\n",
    "                model_logging.log_metrics(metrics, global_step)\n",
    "\n",
    "                net.clear_metrics()\n",
    "            if global_step % steps_per_eval == 0:\n",
    "                torchplus.train.save_models(model_dir, [net, amp_optimizer],\n",
    "                                            net.get_global_step())\n",
    "                model_logging.log_text(f\"Model saved: {model_dir}, {net.get_global_step()}\", global_step)\n",
    "                net.eval()\n",
    "                result_path_step = result_path / f\"step_{net.get_global_step()}\"\n",
    "                result_path_step.mkdir(parents=True, exist_ok=True)\n",
    "                model_logging.log_text(\"########################\", global_step)\n",
    "                model_logging.log_text(\" EVALUATE\", global_step)\n",
    "                model_logging.log_text(\"########################\", global_step)\n",
    "                model_logging.log_text(\"Generating eval predictions...\", global_step)\n",
    "                t = time.time()\n",
    "                detections = []\n",
    "                prog_bar = ProgressBar()\n",
    "                net.clear_timer()\n",
    "                cnt = 0\n",
    "                for example in tqdm_notebook(iter(eval_dataloader)):\n",
    "                    example = example_convert_to_torch(example, float_dtype)\n",
    "                    detections += net(example)\n",
    "                sec_per_ex = len(eval_dataset) / (time.time() - t)\n",
    "                model_logging.log_text(\n",
    "                    f'generate eval predictions finished({sec_per_ex:.2f}/s). Start eval:',\n",
    "                    global_step)\n",
    "                result_dict = eval_dataset.dataset.evaluation(\n",
    "                    detections, str(result_path_step)\n",
    "                )\n",
    "                for k, v in result_dict['results'].items():\n",
    "                    model_logging.log_text(f\"Evaluation {k}\", global_step)\n",
    "                    model_logging.log_text(v, global_step)\n",
    "                model_logging.log_metrics(result_dict[\"detail\"], global_step)\n",
    "                with open(result_path_step/\"result.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(detections,f)\n",
    "                net.train()\n",
    "\n",
    "            step += 1\n",
    "            if step >= total_step:\n",
    "                break\n",
    "        if step >= total_step:\n",
    "            break\n",
    "except Exception as e:\n",
    "    model_logging.log_text(str(e), step)\n",
    "    model_logging.log_text(json.dumps(example['metadata'], indent=2), step)\n",
    "    torchplus.train.save_models(model_dir, [net, amp_optimizer], step)\n",
    "    raise e\n",
    "finally:\n",
    "    model_logging.close()\n",
    "torchplus.train.save_models(model_dir, [net, amp_optimizer], step)\n",
    "# python /kaggle/code/ConeDetectionPointpillars/second/data/nusc_eval.py --root_path=\"/media/starlet/LdTho/data/sets/nuscenes/v1.0-trainval\" --version=\"v1.0-trainval\" --eval_version=detection_cvpr_2019 --res_path=\"/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/results/step_20/result_nusc.json\" --eval_set=val --output_dir=\"/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/1611755918.3152874/results/step_20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-sword",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-bikini",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "numerous-virtue",
   "metadata": {},
   "source": [
    "## Visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-california",
   "metadata": {},
   "outputs": [],
   "source": [
    "NameMappingInverse = {\n",
    "    \"barrier\": \"movable_object.barrier\",\n",
    "    \"bicycle\": \"vehicle.bicycle\",\n",
    "    \"bus\": \"vehicle.bus.rigid\",\n",
    "    \"car\": \"vehicle.car\",\n",
    "    \"construction_vehicle\": \"vehicle.construction\",\n",
    "    \"motorcycle\": \"vehicle.motorcycle\",\n",
    "    \"pedestrian\": \"human.pedestrian.adult\",\n",
    "    \"traffic_cone\": \"movable_object.trafficcone\",\n",
    "    \"trailer\": \"vehicle.trailer\",\n",
    "    \"truck\": \"vehicle.truck\",\n",
    "}\n",
    "def visualize_evaluation(config_path, model_dir, pretrained_path, multi_gpu=False):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if isinstance(config_path, str):\n",
    "        config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "        with open(config_path, \"r\") as f:\n",
    "            proto_str = f.read()\n",
    "            text_format.Merge(proto_str, config)\n",
    "    else:\n",
    "        config = config_path\n",
    "        proto_str = text_format.MessageToString(config, indent=2)\n",
    "\n",
    "\n",
    "    # Read config file\n",
    "    input_cfg = config.train_input_reader\n",
    "    eval_input_cfg = config.eval_input_reader\n",
    "    model_cfg = config.model.second  # model's config\n",
    "    train_cfg = config.train_config\n",
    "\n",
    "    # Build neural network\n",
    "    net = build_network(model_cfg).to(device)\n",
    "\n",
    "    # Build Model\n",
    "    target_assigner = net.target_assigner\n",
    "    voxel_generator = net.voxel_generator\n",
    "    print(\"num parameter: \", len(list(net.parameters())))\n",
    "    torchplus.train.try_restore_latest_checkpoints(model_dir, [net])\n",
    "\n",
    "    if pretrained_path is not None:\n",
    "        print('warning pretrain is loaded after restore, careful with resume')\n",
    "        model_dict = net.state_dict()\n",
    "        pretrained_dict = torch.load(pretrained_path)\n",
    "\n",
    "        new_pretrained_dict = {}\n",
    "        for k, v in pretrained_dict.items():\n",
    "            if k in model_dict and v.shape == model_dict[k].shape:\n",
    "                new_pretrained_dict[k] = v\n",
    "        print(\"Load pretrained parameters: \")\n",
    "        for k, v in new_pretrained_dict.items():\n",
    "            print(k, v.shape)\n",
    "        model_dict.update(new_pretrained_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "        net.clear_global_step()\n",
    "        net.clear_metrics()\n",
    "    if multi_gpu:\n",
    "        net_parallel = torch.nn.DataParallel(net)\n",
    "    else:\n",
    "        net_parallel = net\n",
    "\n",
    "    optimizer_cfg = train_cfg.optimizer\n",
    "    loss_scale = train_cfg.loss_scale_factor\n",
    "    fastai_optimizer = optimizer_builder.build(\n",
    "        optimizer_cfg,\n",
    "        net,\n",
    "        mixed=False,\n",
    "        loss_scale=loss_scale)\n",
    "    if loss_scale < 0:\n",
    "        loss_scale = \"dynamic\"\n",
    "    if train_cfg.enable_mixed_precision:\n",
    "        max_num_voxels = input_cfg.preprocess.max_number_of_voxels * input_cfg.batch_size\n",
    "        print(\"max_num_voxels: %d\" % (max_num_voxels))\n",
    "\n",
    "        net, amp_optimizer = amp.initialize(net, fastai_optimizer,\n",
    "                                            opt_level=\"O1\",\n",
    "                                            keep_batchnorm_fp32=None,\n",
    "                                            loss_scale=loss_scale)\n",
    "        net.metrics_to_float()\n",
    "    else:\n",
    "        amp_optimizer = fastai_optimizer\n",
    "    torchplus.train.try_restore_latest_checkpoints(model_dir, [fastai_optimizer])\n",
    "    lr_scheduler = lr_scheduler_builder.build(optimizer_cfg, amp_optimizer, train_cfg.steps)\n",
    "    if train_cfg.enable_mixed_precision:\n",
    "        float_dtype = torch.float16\n",
    "    else:\n",
    "        float_dtype = torch.float32\n",
    "\n",
    "    if multi_gpu:\n",
    "        num_gpu = torch.cuda.device_count()\n",
    "        print(f\"MULTI_GPU: use {num_gpu} gpus\")\n",
    "        collate_fn = merge_second_batch_multigpu\n",
    "    else:\n",
    "        collate_fn = merge_second_batch\n",
    "        num_gpu = 1\n",
    "\n",
    "    eval_dataset = input_reader_builder.build(\n",
    "        eval_input_cfg,\n",
    "        model_cfg,\n",
    "        training=False,\n",
    "        voxel_generator=voxel_generator,\n",
    "        target_assigner=target_assigner)\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=input_cfg.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=eval_input_cfg.preprocess.num_workers,\n",
    "        pin_memory=False,\n",
    "        collate_fn=merge_second_batch)\n",
    "\n",
    "    # Start visualizing\n",
    "    net.eval()\n",
    "    # example = next(iter(dataloader))\n",
    "    for example in iter(eval_dataloader):\n",
    "        detection = example_convert_to_torch(example, float_dtype)\n",
    "        detection = net(detection)\n",
    "        print(detection)\n",
    "        filtered_sample_tokens = eval_dataset.dataset.filtered_sample_tokens\n",
    "        # filtered_sample_tokens = dataset.dataset.filtered_sample_tokens\n",
    "        index = filtered_sample_tokens.index(detection[0]['metadata']['token'])\n",
    "\n",
    "        gt_example = eval_dataset.dataset.get_sensor_data(index)\n",
    "        # gt_example = dataset.dataset.get_sensor_data(index)\n",
    "        points = gt_example['lidar']['points']\n",
    "        pc_range = model_cfg.voxel_generator.point_cloud_range\n",
    "        points = np.array(\n",
    "            [p for p in points if (pc_range[0] < p[0] < pc_range[3]) & (pc_range[1] < p[1] < pc_range[4]) & (\n",
    "                    pc_range[2] < p[2] < pc_range[5])])\n",
    "\n",
    "        gt_boxes = gt_example['lidar']['annotations']['boxes']\n",
    "        gt_labels = gt_example['lidar']['annotations']['names']\n",
    "        c = points[:, 3].reshape(-1, 1)\n",
    "        c = np.concatenate([c, c, c], axis=1)\n",
    "        points = points[:, 0:3]\n",
    "        pc = o3d.geometry.PointCloud()\n",
    "        pc.points = o3d.utility.Vector3dVector(points)\n",
    "        pc.colors = o3d.utility.Vector3dVector(c)\n",
    "        mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=2.0,\n",
    "                                                                       origin=[-0, -0, -0])\n",
    "        geo = [pc, mesh_frame]\n",
    "        geo = add_prediction_per_class(eval_dataset.dataset.nusc,\n",
    "                                       detection, gt_boxes, gt_labels,\n",
    "                                       target_assigner.classes, geo)\n",
    "        o3d.visualization.draw_geometries(geo)\n",
    "\n",
    "    net.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prediction_per_class(nusc, detection, gt_boxes, gt_labels, class_names, geometries):\n",
    "    color = {\n",
    "    \"traffic_cone\": (1,0,0),\n",
    "    \"gt_traffic_cone\": (0,1,0),\n",
    "    \"pedestrian\": (1,1,0),\n",
    "    \"gt_pedestrian\": (0,0,1)\n",
    "    }\n",
    "    det_boxes = detection[0]['box3d_lidar'].cpu().detach().numpy()\n",
    "    det_labels = detection[0]['label_preds'].cpu().detach().numpy()\n",
    "    det_scores = detection[0]['scores'].cpu().detach().numpy()\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        mask = np.logical_and(det_labels == i, det_scores > 0.5)\n",
    "        class_det_boxes = det_boxes[mask]\n",
    "        class_det_scores = det_scores[mask]\n",
    "        class_det_labels = det_labels[mask]\n",
    "        print(len(class_det_boxes),len(class_det_scores),len(class_det_labels))\n",
    "        print(class_det_scores)\n",
    "        class_gt_boxes = gt_boxes[gt_labels == class_name]\n",
    "        class_gt_labels = gt_labels[gt_labels == class_name]\n",
    "\n",
    "        rbbox_corners = box_np_ops.center_to_corner_box3d(class_det_boxes[:, :3],\n",
    "                                                          class_det_boxes[:, 3:6],\n",
    "                                                          class_det_boxes[:, 6],\n",
    "                                                          origin=(0.5, 0.5, 0.5), axis=2)\n",
    "        gt_rbbox_corners = box_np_ops.center_to_corner_box3d(class_gt_boxes[:, :3],\n",
    "                                                             class_gt_boxes[:, 3:6],\n",
    "                                                             class_gt_boxes[:, 6],\n",
    "                                                             origin=(0.5, 0.5, 0.5), axis=2)\n",
    "        for j in range(len(rbbox_corners)):\n",
    "            geometries.append(buildBBox(rbbox_corners[j],\n",
    "                                        color=color[class_name]))\n",
    "        for j in range(len(gt_rbbox_corners)):\n",
    "            geometries.append(buildBBox(gt_rbbox_corners[j], \n",
    "                                        color=color[f'gt_{class_name}']))\n",
    "    return geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-tulsa",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_evaluation('/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/cones_pp_full_aug/pipeline.config',\n",
    "                     '/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/cones_pp_full_aug/2021-2-13_11:1', \n",
    "                     '/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/cones_pp_full_aug/2021-2-13_11:1/voxelnet-35000.tckpt',\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-poetry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-arctic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-helicopter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "angry-carnival",
   "metadata": {},
   "source": [
    "## Test with baraja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-jimmy",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pc_file_name = \"/media/starlet/LdTho/data/sets/1612769433360603.pcd\"\n",
    "pcd = o3d.io.read_point_cloud(pc_file_name)\n",
    "points = np.asarray(pcd.points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "from second.data.preprocess import merge_second_batch, prep_pointcloud\n",
    "NameMappingInverse = {\n",
    "    \"barrier\": \"movable_object.barrier\",\n",
    "    \"bicycle\": \"vehicle.bicycle\",\n",
    "    \"bus\": \"vehicle.bus.rigid\",\n",
    "    \"car\": \"vehicle.car\",\n",
    "    \"construction_vehicle\": \"vehicle.construction\",\n",
    "    \"motorcycle\": \"vehicle.motorcycle\",\n",
    "    \"pedestrian\": \"human.pedestrian.adult\",\n",
    "    \"traffic_cone\": \"movable_object.trafficcone\",\n",
    "    \"trailer\": \"vehicle.trailer\",\n",
    "    \"truck\": \"vehicle.truck\",\n",
    "}\n",
    "#set input\n",
    "config_path = '/kaggle/code/ConeDetectionPointpillars/customNuscenes/configs/cones_pp_full_aug.config'\n",
    "model_dir = '/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/cones_pp_full_aug/2021-2-13_11:1'\n",
    "pretrained_path = '/kaggle/code/ConeDetectionPointpillars/customNuscenes/outputs/cones_pp_full_aug/2021-2-13_11:1/voxelnet-35000.tckpt'\n",
    "multi_gpu = False\n",
    "# build network and other stuff\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if isinstance(config_path, str):\n",
    "    config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "    with open(config_path, \"r\") as f:\n",
    "        proto_str = f.read()\n",
    "        text_format.Merge(proto_str, config)\n",
    "else:\n",
    "    config = config_path\n",
    "    proto_str = text_format.MessageToString(config, indent=2)\n",
    "\n",
    "\n",
    "# Read config file\n",
    "input_cfg = config.train_input_reader\n",
    "eval_input_cfg = config.eval_input_reader\n",
    "model_cfg = config.model.second  # model's config\n",
    "train_cfg = config.train_config\n",
    "\n",
    "# Build neural network\n",
    "net = build_network(model_cfg).to(device)\n",
    "\n",
    "# Build Model\n",
    "target_assigner = net.target_assigner\n",
    "voxel_generator = net.voxel_generator\n",
    "print(\"num parameter: \", len(list(net.parameters())))\n",
    "torchplus.train.try_restore_latest_checkpoints(model_dir, [net])\n",
    "\n",
    "if pretrained_path is not None:\n",
    "    print('warning pretrain is loaded after restore, careful with resume')\n",
    "    model_dict = net.state_dict()\n",
    "    pretrained_dict = torch.load(pretrained_path)\n",
    "\n",
    "    new_pretrained_dict = {}\n",
    "    for k, v in pretrained_dict.items():\n",
    "        if k in model_dict and v.shape == model_dict[k].shape:\n",
    "            new_pretrained_dict[k] = v\n",
    "    print(\"Load pretrained parameters: \")\n",
    "    for k, v in new_pretrained_dict.items():\n",
    "        print(k, v.shape)\n",
    "    model_dict.update(new_pretrained_dict)\n",
    "    net.load_state_dict(model_dict)\n",
    "    net.clear_global_step()\n",
    "    net.clear_metrics()\n",
    "if multi_gpu:\n",
    "    net_parallel = torch.nn.DataParallel(net)\n",
    "else:\n",
    "    net_parallel = net\n",
    "\n",
    "optimizer_cfg = train_cfg.optimizer\n",
    "loss_scale = train_cfg.loss_scale_factor\n",
    "fastai_optimizer = optimizer_builder.build(\n",
    "    optimizer_cfg,\n",
    "    net,\n",
    "    mixed=False,\n",
    "    loss_scale=loss_scale)\n",
    "if loss_scale < 0:\n",
    "    loss_scale = \"dynamic\"\n",
    "if train_cfg.enable_mixed_precision:\n",
    "    max_num_voxels = input_cfg.preprocess.max_number_of_voxels * input_cfg.batch_size\n",
    "    print(\"max_num_voxels: %d\" % (max_num_voxels))\n",
    "\n",
    "    net, amp_optimizer = amp.initialize(net, fastai_optimizer,\n",
    "                                        opt_level=\"O1\",\n",
    "                                        keep_batchnorm_fp32=None,\n",
    "                                        loss_scale=loss_scale)\n",
    "    net.metrics_to_float()\n",
    "else:\n",
    "    amp_optimizer = fastai_optimizer\n",
    "torchplus.train.try_restore_latest_checkpoints(model_dir, [fastai_optimizer])\n",
    "lr_scheduler = lr_scheduler_builder.build(optimizer_cfg, amp_optimizer, train_cfg.steps)\n",
    "if train_cfg.enable_mixed_precision:\n",
    "    float_dtype = torch.float16\n",
    "else:\n",
    "    float_dtype = torch.float32\n",
    "\n",
    "if multi_gpu:\n",
    "    num_gpu = torch.cuda.device_count()\n",
    "    print(f\"MULTI_GPU: use {num_gpu} gpus\")\n",
    "    collate_fn = merge_second_batch_multigpu\n",
    "else:\n",
    "    collate_fn = merge_second_batch\n",
    "    num_gpu = 1\n",
    "\n",
    "# Start visualizing\n",
    "net.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "grave-irrigation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'voxels': array([[[  2.7952251 ,   2.6065896 ,   0.11079672,   0.        ,\n",
      "           0.        ],\n",
      "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "           0.        ],\n",
      "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "           0.        ],\n",
      "        ...,\n",
      "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "           0.        ],\n",
      "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "           0.        ],\n",
      "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "           0.        ]],\n",
      "\n",
      "       [[  2.8023841 ,   2.6132655 ,   0.12595397,   0.        ,\n",
      "           0.        ],\n",
      "        [  2.8119824 ,   2.622216  ,   0.14039153,   0.        ,\n",
      "           0.        ],\n",
      "        [  2.8132339 ,   2.6032598 ,   0.19666678,   0.        ,\n",
      "           0.        ],\n",
      "        ...,\n",
      "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "           0.        ],\n",
      "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "           0.        ],\n",
      "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "           0.        ]],\n",
      "\n",
      "       [[  2.7869992 ,   2.5789833 ,   0.1172194 ,   0.        ,\n",
      "           0.        ],\n",
      "        [  2.7913644 ,   2.5830228 ,   0.13323857,   0.        ,\n",
      "           0.        ],\n",
      "        [  2.7960172 ,   2.5873282 ,   0.14923698,   0.        ,\n",
      "           0.        ],\n",
      "        ...,\n",
      "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "           0.        ],\n",
      "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "           0.        ],\n",
      "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "           0.        ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 11.509437  , -18.094053  ,   0.09573389,   0.        ,\n",
      "           0.        ],\n",
      "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "           0.        ],\n",
      "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "           0.        ],\n",
      "        ...,\n",
      "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "           0.        ],\n",
      "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "           0.        ],\n",
      "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "           0.        ]],\n",
      "\n",
      "       [[ 11.531673  , -18.129013  ,   0.19632316,   0.        ,\n",
      "           0.        ],\n",
      "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "           0.        ],\n",
      "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "           0.        ],\n",
      "        ...,\n",
      "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "           0.        ],\n",
      "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "           0.        ],\n",
      "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "           0.        ]],\n",
      "\n",
      "       [[ 11.438097  , -18.135296  ,   0.09588994,   0.        ,\n",
      "           0.        ],\n",
      "        [ 11.45816   , -18.167107  ,   0.19662407,   0.        ,\n",
      "           0.        ],\n",
      "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "           0.        ],\n",
      "        ...,\n",
      "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "           0.        ],\n",
      "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "           0.        ],\n",
      "        [  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "           0.        ]]]), 'num_points': array([  1,   6,  18, 100,  68,   9,  21,  11,   5,  21,  19,   8,   1,\n",
      "         1,   9,   4,   1,   1,   6,   3,   1,   1,   2,   2,   1,   1,\n",
      "         3,   1,   1,   2,   1,   1,   1,   2,   1,   1,   1,   4,   1,\n",
      "         1,   4,   2,   2,   7,   2,   7,   7,   1,   1,  19,  26,   9,\n",
      "         4,   8, 100,  45,   1,   1,   2], dtype=int32), 'coordinates': array([[  0, 226, 327],\n",
      "       [  0, 226, 328],\n",
      "       [  0, 225, 327],\n",
      "       [  0, 225, 328],\n",
      "       [  0, 224, 328],\n",
      "       [  0, 225, 329],\n",
      "       [  0, 224, 329],\n",
      "       [  0, 222, 353],\n",
      "       [  0, 222, 354],\n",
      "       [  0, 221, 353],\n",
      "       [  0, 221, 354],\n",
      "       [  0, 220, 387],\n",
      "       [  0, 220, 386],\n",
      "       [  0, 219, 386],\n",
      "       [  0, 219, 387],\n",
      "       [  0, 220, 418],\n",
      "       [  0, 220, 417],\n",
      "       [  0, 219, 417],\n",
      "       [  0, 219, 418],\n",
      "       [  0, 217, 452],\n",
      "       [  0, 217, 451],\n",
      "       [  0, 216, 451],\n",
      "       [  0, 216, 452],\n",
      "       [  0, 213, 490],\n",
      "       [  0, 213, 491],\n",
      "       [  0, 212, 491],\n",
      "       [  0, 212, 518],\n",
      "       [  0, 212, 517],\n",
      "       [  0, 208, 569],\n",
      "       [  0, 207, 569],\n",
      "       [  0, 178, 519],\n",
      "       [  0, 178, 520],\n",
      "       [  0, 177, 519],\n",
      "       [  0, 177, 520],\n",
      "       [  0, 171, 559],\n",
      "       [  0, 171, 560],\n",
      "       [  0, 179, 484],\n",
      "       [  0, 178, 484],\n",
      "       [  0, 178, 483],\n",
      "       [  0, 177, 484],\n",
      "       [  0, 180, 452],\n",
      "       [  0, 180, 453],\n",
      "       [  0, 183, 417],\n",
      "       [  0, 187, 386],\n",
      "       [  0, 187, 385],\n",
      "       [  0, 186, 385],\n",
      "       [  0, 186, 386],\n",
      "       [  0, 187, 354],\n",
      "       [  0, 187, 353],\n",
      "       [  0, 186, 354],\n",
      "       [  0, 186, 353],\n",
      "       [  0, 185, 353],\n",
      "       [  0, 185, 354],\n",
      "       [  0, 185, 326],\n",
      "       [  0, 185, 325],\n",
      "       [  0, 184, 325],\n",
      "       [  0,  19, 415],\n",
      "       [  0,  18, 415],\n",
      "       [  0,  18, 414]], dtype=int32), 'num_voxels': array([59]), 'metrics': {'voxel_gene_time': 0.00017905235290527344, 'prep_time': 0.002521038055419922}, 'anchors': array([[-30.        , -20.        ,  -2.6       , ...,   0.40359262,\n",
      "          1.0623215 ,   0.        ],\n",
      "       [-29.597315  , -20.        ,  -2.6       , ...,   0.40359262,\n",
      "          1.0623215 ,   0.        ],\n",
      "       [-29.194632  , -20.        ,  -2.6       , ...,   0.40359262,\n",
      "          1.0623215 ,   0.        ],\n",
      "       ...,\n",
      "       [ 29.194632  ,  20.        ,  -2.6       , ...,   0.40359262,\n",
      "          1.0623215 ,   1.57      ],\n",
      "       [ 29.597315  ,  20.        ,  -2.6       , ...,   0.40359262,\n",
      "          1.0623215 ,   1.57      ],\n",
      "       [ 30.        ,  20.        ,  -2.6       , ...,   0.40359262,\n",
      "          1.0623215 ,   1.57      ]], dtype=float32), 'metadata': {'token': '/media/starlet/LdTho/data/sets/1614053547.551486.pcd'}}]\n"
     ]
    }
   ],
   "source": [
    "from second.data.preprocess import merge_second_batch, prep_pointcloud\n",
    "\n",
    "pc_file_name = \"/media/starlet/LdTho/data/sets/1614053547.551486.pcd\"\n",
    "pcd = o3d.io.read_point_cloud(pc_file_name)\n",
    "points = np.asarray(pcd.points)\n",
    "pc_range = model_cfg.voxel_generator.point_cloud_range\n",
    "points = np.array([p for p in points if (pc_range[0] < p[0] < pc_range[3])\n",
    "                   & (pc_range[1] < p[1] < pc_range[4])\n",
    "                   & (0.05 < p[2] < pc_range[5])])\n",
    "\n",
    "points = np.concatenate([points.transpose(), \n",
    "                         np.array([np.repeat(0.0, points.shape[0])]),\n",
    "                         np.array([np.repeat(0.0, points.shape[0])])],axis = 0).transpose()\n",
    "points = points[~np.isnan(points).any(axis=1)]\n",
    "\n",
    "input_dict = {\n",
    "    'lidar': {\n",
    "        'type': 'lidar',\n",
    "        'points': points\n",
    "    },\n",
    "    'metadata': {\n",
    "        'token': pc_file_name\n",
    "    }\n",
    "}\n",
    "\n",
    "out_size_factor = model_cfg.rpn.layer_strides[0] // model_cfg.rpn.upsample_strides[0]\n",
    "example = prep_pointcloud(input_dict=input_dict,\n",
    "                          root_path= None ,\n",
    "                          voxel_generator= voxel_generator,\n",
    "                          target_assigner= target_assigner,\n",
    "                          max_voxels= 70000,\n",
    "                          training= False,\n",
    "                          create_targets=False,\n",
    "                          shuffle_points=input_cfg.preprocess.shuffle_points,\n",
    "                          num_point_features=model_cfg.num_point_features,\n",
    "                          remove_outside_points=False,\n",
    "                          anchor_cache=None,\n",
    "                          anchor_area_threshold=input_cfg.preprocess.anchor_area_threshold,\n",
    "                          out_size_factor=out_size_factor,\n",
    "                          out_dtype=np.float32\n",
    "                          )\n",
    "# example[\"points\"] = points\n",
    "example[\"metadata\"] = input_dict[\"metadata\"]\n",
    "example = [example]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "infinite-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader = torch.utils.data.DataLoader(\n",
    "        example,\n",
    "        batch_size=input_cfg.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=eval_input_cfg.preprocess.num_workers,\n",
    "        pin_memory=False,\n",
    "        collate_fn=merge_second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "retained-ukraine",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "example = next(iter(eval_dataloader))\n",
    "detection = example_convert_to_torch(example, float_dtype)\n",
    "detection = net(detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "confident-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.asarray(pcd.points)\n",
    "points = np.array(\n",
    "    [p for p in points if (pc_range[0] < p[0] < pc_range[3]) & (pc_range[1] < p[1] < pc_range[4]) & (\n",
    "           pc_range[2]  < p[2] < pc_range[5])])\n",
    "points = np.concatenate([points.transpose(), \n",
    "                         np.array([np.repeat(0.0, points.shape[0])]),\n",
    "                         np.array([np.repeat(0.0, points.shape[0])])],axis = 0).transpose()\n",
    "points = points[~np.isnan(points).any(axis=1)]\n",
    "\n",
    "c = points[:, 3].reshape(-1, 1)\n",
    "c = np.concatenate([c, c, c], axis=1)\n",
    "points = points[:, 0:3]\n",
    "\n",
    "pc = o3d.geometry.PointCloud()\n",
    "pc.points = o3d.utility.Vector3dVector(points)\n",
    "pc.colors = o3d.utility.Vector3dVector(c)\n",
    "mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=2.0,\n",
    "                                                               origin=[-0, -0, -0])\n",
    "geo = [pc, mesh_frame]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "chemical-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_boxes = detection[0]['box3d_lidar'].cpu().detach().numpy()\n",
    "det_labels = detection[0]['label_preds'].cpu().detach().numpy()\n",
    "det_scores = detection[0]['scores'].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "single-builder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 16 16\n",
      "[0.77662295 0.75201255 0.7318258  0.72671664 0.65797806 0.65323746\n",
      " 0.6278371  0.6213686  0.6204492  0.61687815 0.59496313 0.5778527\n",
      " 0.56613845 0.55953074 0.5567611  0.5071559 ]\n"
     ]
    }
   ],
   "source": [
    "class_names = ['traffic_cone']\n",
    "color = {\n",
    "\"traffic_cone\": (1,0,0),\n",
    "\"gt_traffic_cone\": (0,1,0),\n",
    "\"pedestrian\": (1,1,0),\n",
    "\"gt_pedestrian\": (0,0,1)\n",
    "}\n",
    "for i, class_name in enumerate(class_names):\n",
    "    mask = np.logical_and(det_labels == i, det_scores > 0.5)\n",
    "    class_det_boxes = det_boxes[mask]\n",
    "    class_det_scores = det_scores[mask]\n",
    "    class_det_labels = det_labels[mask]\n",
    "    print(len(class_det_boxes),len(class_det_scores),len(class_det_labels))\n",
    "    print(class_det_scores)\n",
    "    rbbox_corners = box_np_ops.center_to_corner_box3d(class_det_boxes[:, :3],\n",
    "                                                      class_det_boxes[:, 3:6],\n",
    "                                                      class_det_boxes[:, 6],\n",
    "                                                      origin=(0.5, 0.5, 0.5), axis=2)\n",
    "\n",
    "    for j in range(len(rbbox_corners)):\n",
    "        geo.append(buildBBox(rbbox_corners[j],\n",
    "                                    color=color[class_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "interior-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries(geo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-testament",
   "metadata": {},
   "outputs": [],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-weekly",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(points[:,2],bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "example['voxels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-hardware",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(example['num_points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-construction",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dataset[1]['num_points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "signal-export",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'box3d_lidar': tensor([[  5.4123,   2.1952,   0.1914,   0.3746,   0.3783,   0.8061,   1.2520],\n",
       "          [  2.5964,  -1.4935,   0.2891,   0.3673,   0.3712,   0.7470,   1.5391],\n",
       "          [  5.4012,  -1.3905,   0.1797,   0.3688,   0.3736,   0.8234,   1.1309],\n",
       "          [  2.8603,   2.5337,   0.2676,   0.3751,   0.3761,   0.7556,   1.5693],\n",
       "          [ 11.8338,   1.9946,   0.1895,   0.3701,   0.3731,   0.8448,   1.6123],\n",
       "          [ 15.3248,  -1.9501,   0.2637,   0.3800,   0.3767,   0.8296,   2.3359],\n",
       "          [  8.6497,  -1.3051,   0.2207,   0.3733,   0.3732,   0.7785,   1.9570],\n",
       "          [ 18.4739,  -2.1518,   0.2539,   0.3659,   0.3594,   0.8519,   0.9390],\n",
       "          [ 21.8709,   1.2486,   0.2500,   0.3820,   0.3793,   0.9415,   1.5713],\n",
       "          [  8.7848,   1.9966,   0.1699,   0.3512,   0.3523,   0.7662,   2.5352],\n",
       "          [ 15.2564,   1.6977,   0.2324,   0.3930,   0.3970,   0.8521,   1.7754],\n",
       "          [ 26.0562,  -2.8520,   0.2891,   0.4887,   0.4920,   1.0001,   1.5518],\n",
       "          [ 19.1292,   1.3246,   0.2695,   0.4271,   0.4294,   0.9625,   1.5645],\n",
       "          [ 11.7729,  -1.6504,   0.2051,   0.3517,   0.3610,   0.9003,   2.0469],\n",
       "          [ 22.1002,  -2.1781,   0.2773,   0.4638,   0.4661,   1.0170,   1.5811],\n",
       "          [ 27.0532,   0.7405,   0.2930,   0.3893,   0.3901,   1.0478,   1.5752],\n",
       "          [  4.0679,  -1.3288,   0.1543,   0.3525,   0.3589,   0.8515,   1.6934],\n",
       "          [  7.0462,  -1.3473,   0.0762,   0.3687,   0.3782,   0.8230,   1.1670],\n",
       "          [ 10.2586,   2.0132,   0.0918,   0.3488,   0.3535,   0.8257,   1.6973],\n",
       "          [  4.3141,   2.4322,   0.1602,   0.3649,   0.3663,   0.8413,   1.6719],\n",
       "          [ 17.0515,  -2.1232,   0.1074,   0.3657,   0.3738,   0.8695,   1.1289],\n",
       "          [  7.1404,   2.0845,   0.1836,   0.3735,   0.3819,   0.8741,   2.1133],\n",
       "          [ 11.5092, -18.1597,   0.2637,   0.3822,   0.3844,   1.0032,   1.5889],\n",
       "          [ 10.3154,  -1.4845,   0.2168,   0.3622,   0.3723,   0.8641,   2.1133],\n",
       "          [ 13.6010,   1.7936,   0.2148,   0.3731,   0.3803,   0.9052,   2.1094],\n",
       "          [ 13.5108,  -1.9363,   0.2539,   0.3791,   0.3852,   0.9403,   2.1113],\n",
       "          [ 20.7061,   1.2255,   0.1113,   0.3809,   0.3843,   0.9017,   1.6621],\n",
       "          [  3.4574,  -1.6101,   0.2090,   0.3631,   0.3570,   0.8176,   1.5898],\n",
       "          [ 25.0328,   0.8623,   0.3418,   0.3889,   0.3992,   1.1788,   2.0898],\n",
       "          [ 17.1697,   1.3459,   0.3145,   0.3904,   0.4008,   0.9682,   2.1094],\n",
       "          [ 20.3042,  -2.2997,   0.2754,   0.3881,   0.3989,   0.9503,   2.0742],\n",
       "          [ 24.3116,  -2.7588,   0.3359,   0.4041,   0.4143,   1.0597,   2.0859],\n",
       "          [ 16.2651,  -2.1438,   0.1738,   0.3678,   0.3678,   0.9280,   1.6084]],\n",
       "         device='cuda:0'),\n",
       "  'scores': tensor([0.7759, 0.7594, 0.7553, 0.7330, 0.6843, 0.6594, 0.6539, 0.6245, 0.6222,\n",
       "          0.6149, 0.5714, 0.5570, 0.5433, 0.5328, 0.5177, 0.4612, 0.4190, 0.3936,\n",
       "          0.3870, 0.3824, 0.3693, 0.3568, 0.3427, 0.3281, 0.3255, 0.2894, 0.2888,\n",
       "          0.2817, 0.2642, 0.2464, 0.2429, 0.2328, 0.2280], device='cuda:0'),\n",
       "  'label_preds': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'),\n",
       "  'metadata': {'token': '/media/starlet/LdTho/data/sets/1614053547.551486.pcd'}}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

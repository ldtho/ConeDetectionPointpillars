{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "formed-drive",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "import shutil\n",
    "import time\n",
    "from functools import partial\n",
    "import sys\n",
    "sys.path.append('/kaggle/code/ConeDetectionPointpillarsV2')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import fire\n",
    "import numpy as np\n",
    "import torch\n",
    "from google.protobuf import text_format\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torchplus\n",
    "import second.data.kitti_common as kitti\n",
    "from second.builder import target_assigner_builder, voxel_builder\n",
    "from second.data.preprocess import merge_second_batch\n",
    "from second.protos import pipeline_pb2\n",
    "from second.pytorch.builder import (box_coder_builder, input_reader_builder,\n",
    "                                      lr_scheduler_builder, optimizer_builder,\n",
    "                                      second_builder)\n",
    "from second.utils.progress_bar import ProgressBar\n",
    "\n",
    "from second.pytorch.utils import get_paddings_indicator\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from second.core import box_np_ops\n",
    "import open3d as o3d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-reggae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "communist-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_pos_neg_loss(cls_loss, labels):\n",
    "    # cls_loss: [N, num_anchors, num_class]\n",
    "    # labels: [N, num_anchors]\n",
    "    batch_size = cls_loss.shape[0]\n",
    "    if cls_loss.shape[-1] == 1 or len(cls_loss.shape) == 2:\n",
    "        cls_pos_loss = (labels > 0).type_as(cls_loss) * cls_loss.view(\n",
    "            batch_size, -1)\n",
    "        cls_neg_loss = (labels == 0).type_as(cls_loss) * cls_loss.view(\n",
    "            batch_size, -1)\n",
    "        cls_pos_loss = cls_pos_loss.sum() / batch_size\n",
    "        cls_neg_loss = cls_neg_loss.sum() / batch_size\n",
    "    else:\n",
    "        cls_pos_loss = cls_loss[..., 1:].sum() / batch_size\n",
    "        cls_neg_loss = cls_loss[..., 0].sum() / batch_size\n",
    "    return cls_pos_loss, cls_neg_loss\n",
    "\n",
    "\n",
    "def _flat_nested_json_dict(json_dict, flatted, sep=\".\", start=\"\"):\n",
    "    for k, v in json_dict.items():\n",
    "        if isinstance(v, dict):\n",
    "            _flat_nested_json_dict(v, flatted, sep, start + sep + k)\n",
    "        else:\n",
    "            flatted[start + sep + k] = v\n",
    "\n",
    "\n",
    "def flat_nested_json_dict(json_dict, sep=\".\") -> dict:\n",
    "    \"\"\"flat a nested json-like dict. this function make shadow copy.\n",
    "    \"\"\"\n",
    "    flatted = {}\n",
    "    for k, v in json_dict.items():\n",
    "        if isinstance(v, dict):\n",
    "            _flat_nested_json_dict(v, flatted, sep, k)\n",
    "        else:\n",
    "            flatted[k] = v\n",
    "    return flatted\n",
    "\n",
    "\n",
    "def example_convert_to_torch(example, dtype=torch.float32,\n",
    "                             device=None) -> dict:\n",
    "    device = device or torch.device(\"cuda:0\")\n",
    "    example_torch = {}\n",
    "    float_names = [\n",
    "        \"voxels\", \"anchors\", \"reg_targets\", \"reg_weights\", \"bev_map\", \"rect\",\n",
    "        \"Trv2c\", \"P2\"\n",
    "    ]\n",
    "\n",
    "    for k, v in example.items():\n",
    "        if k in float_names:\n",
    "            example_torch[k] = torch.as_tensor(v, dtype=dtype, device=device)\n",
    "        elif k in [\"coordinates\", \"labels\", \"num_points\"]:\n",
    "            example_torch[k] = torch.as_tensor(\n",
    "                v, dtype=torch.int32, device=device)\n",
    "        elif k in [\"anchors_mask\"]:\n",
    "            example_torch[k] = torch.as_tensor(\n",
    "                v, dtype=torch.uint8, device=device)\n",
    "        else:\n",
    "            example_torch[k] = v\n",
    "    return example_torch\n",
    "def _worker_init_fn(worker_id):\n",
    "    time_seed = np.array(time.time(), dtype=np.int32)\n",
    "    np.random.seed(time_seed + worker_id)\n",
    "\n",
    "def buildBBox(points,color = [1,0,0]):\n",
    "    #print(\"Let's draw a cubic using o3d.geometry.LineSet\")\n",
    "    # points = [[0, 0, 0], [1, 0, 0], [0, 1, 0], [1, 1, 0], [0, 0, 1], [1, 0, 1],\n",
    "    #           [0, 1, 1], [1, 1, 1]] x0y0z0, x0y0z1, x0y1z0, x0y1z1, x1y0z0, x1y0z1, x1y1z0, x1y1z1\n",
    "\n",
    "    points = points[[0,4,3,7,1,5,2,6],:]\n",
    "    lines = [[0, 1], [0, 2], [1, 3], [2, 3], [4, 5], [4, 6], [5, 7], [6, 7],\n",
    "             [0, 4], [1, 5], [2, 6], [3, 7]]\n",
    "    colors = [color for i in range(len(lines))]\n",
    "    line_set = o3d.geometry.LineSet()\n",
    "    line_set.points = o3d.utility.Vector3dVector(points)\n",
    "    line_set.lines = o3d.utility.Vector2iVector(lines)\n",
    "    line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "    return  line_set\n",
    "def add_prediction_per_class(nusc, detection, gt_boxes, gt_labels, class_names, geometries):\n",
    "    color = {\n",
    "    \"traffic_cone\": (1,0,0),\n",
    "    \"gt_traffic_cone\": (0,1,0),\n",
    "    \"pedestrian\": (1,1,0),\n",
    "    \"gt_pedestrian\": (0,0,1)\n",
    "    }\n",
    "    det_boxes = detection[0]['box3d_lidar'].cpu().detach().numpy()\n",
    "    det_labels = detection[0]['label_preds'].cpu().detach().numpy()\n",
    "    det_scores = detection[0]['scores'].cpu().detach().numpy()\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        mask = np.logical_and(det_labels == i, det_scores > 0.5)\n",
    "        class_det_boxes = det_boxes[mask]\n",
    "        class_det_scores = det_scores[mask]\n",
    "        class_det_labels = det_labels[mask]\n",
    "        print(len(class_det_boxes),len(class_det_scores),len(class_det_labels))\n",
    "        print(class_det_scores)\n",
    "        class_gt_boxes = gt_boxes[gt_labels == class_name]\n",
    "        class_gt_labels = gt_labels[gt_labels == class_name]\n",
    "\n",
    "        rbbox_corners = box_np_ops.center_to_corner_box3d(class_det_boxes[:, :3],\n",
    "                                                          class_det_boxes[:, 3:6],\n",
    "                                                          class_det_boxes[:, 6],\n",
    "                                                          origin=(0.5, 0.5, 0.5), axis=2)\n",
    "        gt_rbbox_corners = box_np_ops.center_to_corner_box3d(class_gt_boxes[:, :3],\n",
    "                                                             class_gt_boxes[:, 3:6],\n",
    "                                                             class_gt_boxes[:, 6],\n",
    "                                                             origin=(0.5, 0.5, 0.5), axis=2)\n",
    "        for j in range(len(rbbox_corners)):\n",
    "            geometries.append(buildBBox(rbbox_corners[j],\n",
    "                                        color=color[class_name]))\n",
    "        for j in range(len(gt_rbbox_corners)):\n",
    "            geometries.append(buildBBox(gt_rbbox_corners[j], \n",
    "                                        color=color[f'gt_{class_name}']))\n",
    "    return geometries\n",
    "color = {\n",
    "    \"traffic_cone\": (1,0,0),\n",
    "    \"gt_traffic_cone\": (0,1,0),\n",
    "    \"pedestrian\": (1,1,0),\n",
    "    \"gt_pedestrian\": (0,0,1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a591e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"/kaggle/code/ConeDetectionPointpillarsV2/second/configs/pointpillars/cone/xyres_10.proto\"\n",
    "model_dir = \"/kaggle/code/ConeDetectionPointpillarsV2/second/pytorch/outputs/xyres_10\"\n",
    "ckpt_path = \"/kaggle/code/ConeDetectionPointpillarsV2/second/pytorch/outputs/xyres_10/210322_205206/Pointpillars-46635.tckpt\"\n",
    "# ckpt_path = None\n",
    "optim_dir = \"/kaggle/code/ConeDetectionPointpillarsV2/second/pytorch/outputs/xyres_10/210322_205206/Adam_OneCycle-46635.tckpt\"\n",
    "result_path=None\n",
    "create_folder=True\n",
    "display_step=50\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "retained-reggae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_folder:\n",
    "    if pathlib.Path(model_dir).exists():\n",
    "        model_dir = torchplus.train.create_folder(model_dir)\n",
    "\n",
    "model_dir = pathlib.Path(model_dir)\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "eval_checkpoint_dir = model_dir / 'eval_checkpoints'\n",
    "eval_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "if result_path is None:\n",
    "    result_path = model_dir / 'results'\n",
    "config_file_bkp = \"pipeline.config\"\n",
    "config = pipeline_pb2.TrainEvalPipelineConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "powerful-corner",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_path, \"r\") as f:\n",
    "    proto_str = f.read()\n",
    "    text_format.Merge(proto_str, config)\n",
    "shutil.copyfile(config_path, str(model_dir / config_file_bkp))\n",
    "input_cfg = config.train_input_reader\n",
    "eval_input_cfg = config.eval_input_reader\n",
    "model_cfg = config.model.second\n",
    "train_cfg = config.train_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "different-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = list(input_cfg.class_names)\n",
    "######################\n",
    "# BUILD VOXEL GENERATOR\n",
    "######################\n",
    "voxel_generator = voxel_builder.build(model_cfg.voxel_generator)\n",
    "######################\n",
    "# BUILD TARGET ASSIGNER\n",
    "######################\n",
    "bv_range = voxel_generator.point_cloud_range[[0, 1, 3, 4]]\n",
    "box_coder = box_coder_builder.build(model_cfg.box_coder)\n",
    "target_assigner_cfg = model_cfg.target_assigner\n",
    "target_assigner = target_assigner_builder.build(target_assigner_cfg,\n",
    "                                                bv_range, box_coder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "certain-lafayette",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_trainable parameters: 72\n",
      "Restoring parameters from /kaggle/code/ConeDetectionPointpillarsV2/second/pytorch/outputs/xyres_10/210322_205206/Pointpillars-46635.tckpt\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "# BUILD NET\n",
    "######################\n",
    "center_limit_range = model_cfg.post_center_limit_range\n",
    "# net = second_builder.build(model_cfg, voxel_generator, target_assigner)\n",
    "net = second_builder.build(model_cfg, voxel_generator, target_assigner, input_cfg.batch_size)\n",
    "net.to(device)\n",
    "# net_train = torch.nn.DataParallel(net).cuda()\n",
    "print(\"num_trainable parameters:\", len(list(net.parameters())))\n",
    "if ckpt_path is None:\n",
    "    torchplus.train.try_restore_latest_checkpoints(model_dir, [net])\n",
    "else:\n",
    "    torchplus.train.restore(ckpt_path, net)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sunset-soccer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring parameters from /kaggle/code/ConeDetectionPointpillarsV2/second/pytorch/outputs/xyres_10/210322_205206/Adam_OneCycle-46635.tckpt\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "# BUILD OPTIMIZER\n",
    "######################\n",
    "# we need global_step to create lr_scheduler, so restore net first.\n",
    "gstep = net.get_global_step() - 1\n",
    "optimizer_cfg = train_cfg.optimizer\n",
    "if train_cfg.enable_mixed_precision:\n",
    "    net.half()\n",
    "    net.metrics_to_float()\n",
    "    net.convert_norm_to_float(net)\n",
    "# optimizer = optimizer_builder.build(optimizer_cfg, net.parameters())\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0002, weight_decay= 0.0001)\n",
    "if optim_dir:\n",
    "    torchplus.train.restore(optim_dir, optimizer)\n",
    "\n",
    "if train_cfg.enable_mixed_precision:\n",
    "    loss_scale = train_cfg.loss_scale_factor\n",
    "    mixed_optimizer = torchplus.train.MixedPrecisionWrapper(\n",
    "        optimizer, loss_scale)\n",
    "else:\n",
    "    mixed_optimizer = optimizer\n",
    "# must restore optimizer AFTER using MixedPrecisionWrapper\n",
    "\n",
    "# torchplus.train.try_restore_latest_checkpoints(optim_dir, [mixed_optimizer])\n",
    "# lr_scheduler = lr_scheduler_builder.build(optimizer_cfg, optimizer, gstep)\n",
    "\n",
    "\n",
    "if train_cfg.enable_mixed_precision:\n",
    "    float_dtype = torch.float16\n",
    "else:\n",
    "    float_dtype = torch.float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "biological-relative",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.435 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "# PREPARE INPUT\n",
    "######################\n",
    "dataset = input_reader_builder.build(\n",
    "    input_cfg,\n",
    "    model_cfg,\n",
    "    training=True,\n",
    "    voxel_generator=voxel_generator,\n",
    "    target_assigner=target_assigner)\n",
    "\n",
    "# eval_dataset = input_reader_builder.build(\n",
    "#     eval_input_cfg,\n",
    "#     model_cfg,\n",
    "#     training=False,\n",
    "#     voxel_generator=voxel_generator,\n",
    "#     target_assigner=target_assigner)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "younger-elimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, 0.003, steps_per_epoch = len(dataset),epochs = 15,\n",
    "#                                                   pct_start = 0.4,base_momentum = 0.85, max_momentum=0.95,\n",
    "#                                                   div_factor = 10.0, last_epoch = gstep)\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, 0.003, steps_per_epoch = len(dataset),epochs = 15,\n",
    "                                                  pct_start = 0.4,base_momentum = 0.85, max_momentum=0.95,\n",
    "                                                  div_factor = 10.0, last_epoch = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-steering",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gstep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = dataset[10]\n",
    "len([i for i in d1['labels'] if i == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([i for i in d1['labels'] if i == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d1['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regs = [ret for ret in d1[\"reg_targets\"] if ret.sum() != 0]\n",
    "# for reg in d1['reg_targets']:\n",
    "#     if reg.sum() >0:\n",
    "#         print(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "retired-gentleman",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=input_cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=input_cfg.num_workers,\n",
    "#     num_workers=16,\n",
    "    pin_memory=False,\n",
    "    collate_fn=merge_second_batch,\n",
    "    worker_init_fn=_worker_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-boating",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.get_global_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cathedral-fraud",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4954134"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4954134 parameter ==> 5Mil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-vinyl",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "example = next(iter(dataloader))\n",
    "example_torch = example_convert_to_torch(example, float_dtype, device = device)\n",
    "batch_size = example[\"anchors\"].shape[0]\n",
    "example_tuple = list(example_torch.values())\n",
    "\n",
    "pillar_x = example_tuple[0][:,:,0].unsqueeze(0).unsqueeze(0)\n",
    "pillar_y = example_tuple[0][:,:,1].unsqueeze(0).unsqueeze(0)\n",
    "pillar_z = example_tuple[0][:,:,2].unsqueeze(0).unsqueeze(0)\n",
    "pillar_i = example_tuple[0][:,:,3].unsqueeze(0).unsqueeze(0)\n",
    "# pillar_i = torch.ones(pillar_x.shape,dtype=torch.float32, device=pillar_x.device )\n",
    "\n",
    "num_points_per_pillar = example_tuple[1].float().unsqueeze(0)\n",
    "\n",
    "# Find distance of x, y, and z from pillar center\n",
    "# assuming xyres_16.proto\n",
    "coors_x = example_tuple[2][:, 3].float()\n",
    "coors_y = example_tuple[2][:, 2].float()\n",
    "vx, vy = voxel_generator.voxel_size[0], voxel_generator.voxel_size[1]\n",
    "x_offset = vx/2 + voxel_generator.point_cloud_range[0]\n",
    "y_offset = vy/2 + voxel_generator.point_cloud_range[1]\n",
    "# self.x_offset = self.vx / 2 + pc_range[0]\n",
    "# self.y_offset = self.vy / 2 + pc_range[1]\n",
    "# this assumes xyres 20\n",
    "# x_sub = coors_x.unsqueeze(1) * 0.16 + 0.1\n",
    "# y_sub = coors_y.unsqueeze(1) * 0.16 + -39.9\n",
    "# here assumes xyres 16\n",
    "x_sub = coors_x.unsqueeze(1)*vx + x_offset\n",
    "y_sub = coors_y.unsqueeze(1)*vy + y_offset\n",
    "# x_sub = coors_x.unsqueeze(1)*0.28 + 0.14\n",
    "# y_sub = coors_y.unsqueeze(1)*0.28 - 20.0\n",
    "ones = torch.ones([1,voxel_generator._max_num_points], dtype = torch.float32, device = pillar_x.device)\n",
    "x_sub_shaped = torch.mm(x_sub, ones).unsqueeze(0).unsqueeze(0)\n",
    "y_sub_shaped = torch.mm(y_sub, ones).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "num_points_for_a_pillar = pillar_x.size()[3]\n",
    "mask = get_paddings_indicator(num_points_per_pillar, num_points_for_a_pillar, axis=0)\n",
    "mask = mask.permute(0, 2, 1)\n",
    "mask = mask.unsqueeze(1)\n",
    "mask = mask.type_as(pillar_x)\n",
    "\n",
    "coors   = example_tuple[2]\n",
    "anchors = example_tuple[3]\n",
    "labels  = example_tuple[4]\n",
    "reg_targets = example_tuple[5]\n",
    "\n",
    "inputs = [pillar_x, pillar_y, pillar_z, pillar_i,\n",
    "         num_points_per_pillar, x_sub_shaped, y_sub_shaped, mask, coors,\n",
    "         anchors, labels, reg_targets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57435f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63e1a485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voxel_features.shape torch.Size([1, 64, 29031, 1])\n",
      "voxel_features.shape after squeeze&permute torch.Size([29031, 64])\n",
      "spatial_features.shape torch.Size([1, 64, 400, 600])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ceb20030615b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mret_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/spconv12cuda11/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/code/ConeDetectionPointpillarsV2/second/pytorch/models/voxelnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, example)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;31m#print(\"spatial_features.size()\",spatial_features.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0mpreds_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspatial_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m         \u001b[0;31m# print('preds_dict.shape', preds_dict.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m         \u001b[0;31m# return preds_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "ret_dict = net(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d29e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f81789",
   "metadata": {},
   "outputs": [],
   "source": [
    "pillar_x[0][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58d7e0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pillar_x torch.Size([1, 1, 31485, 100])\n",
      "pillar_y torch.Size([1, 1, 31485, 100])\n",
      "pillar_z torch.Size([1, 1, 31485, 100])\n",
      "pillar_i torch.Size([1, 1, 31485, 100])\n",
      "num_points_per_pillar torch.Size([1, 31485])\n",
      "x_sub_shaped torch.Size([1, 1, 31485, 100])\n",
      "y_sub_shaped torch.Size([1, 1, 31485, 100])\n",
      "mask torch.Size([1, 1, 31485, 100])\n",
      "coors torch.Size([31485, 4])\n",
      "anchors torch.Size([1, 480000, 7])\n",
      "labels torch.Size([1, 480000])\n",
      "reg_targets torch.Size([1, 480000, 7])\n"
     ]
    }
   ],
   "source": [
    "print('pillar_x',pillar_x.shape)\n",
    "print('pillar_y',pillar_y.shape)\n",
    "print('pillar_z',pillar_z.shape)\n",
    "print('pillar_i',pillar_i.shape)\n",
    "print('num_points_per_pillar',num_points_per_pillar.shape)\n",
    "print('x_sub_shaped',x_sub_shaped.shape)\n",
    "print('y_sub_shaped',y_sub_shaped.shape)\n",
    "print('mask',mask.shape)\n",
    "print('coors',coors.shape)\n",
    "print('anchors',anchors.shape)\n",
    "print('labels',labels.shape)\n",
    "print('reg_targets',reg_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f45f563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pillar_x torch.Size([1, 1, 31180, 100])\n",
    "# pillar_y torch.Size([1, 1, 31180, 100])\n",
    "# pillar_z torch.Size([1, 1, 31180, 100])\n",
    "# pillar_i torch.Size([1, 1, 31180, 100])\n",
    "# num_points_per_pillar torch.Size([1, 31180])\n",
    "# x_sub_shaped torch.Size([1, 1, 31180, 100])\n",
    "# y_sub_shaped torch.Size([1, 1, 31180, 100])\n",
    "# mask torch.Size([1, 1, 31180, 100])\n",
    "# coors torch.Size([31180, 4])\n",
    "# anchors torch.Size([1, 480000, 7])\n",
    "# labels torch.Size([1, 480000])\n",
    "# reg_targets torch.Size([1, 480000, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7aa79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pillar_x torch.Size([1, 1, 33453, 100])\n",
    "# pillar_y torch.Size([1, 1, 33453, 100])\n",
    "# pillar_z torch.Size([1, 1, 33453, 100])\n",
    "# pillar_i torch.Size([1, 1, 33453, 100])\n",
    "# num_points_per_pillar torch.Size([1, 33453])\n",
    "# x_sub_shaped torch.Size([1, 1, 33453, 100])\n",
    "# y_sub_shaped torch.Size([1, 1, 33453, 100])\n",
    "# mask torch.Size([1, 1, 33453, 100])\n",
    "# coors torch.Size([33453, 4])\n",
    "# anchors torch.Size([1, 480000, 7])\n",
    "# labels torch.Size([1, 480000])\n",
    "# reg_targets torch.Size([1, 480000, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab341744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pillar_x torch.Size([1, 1, 24090, 100])\n",
    "# pillar_y torch.Size([1, 1, 24090, 100])\n",
    "# pillar_z torch.Size([1, 1, 24090, 100])\n",
    "# pillar_i torch.Size([1, 1, 24090, 100])\n",
    "# num_points_per_pillar torch.Size([1, 24090])\n",
    "# x_sub_shaped torch.Size([1, 1, 24090, 100])\n",
    "# y_sub_shaped torch.Size([1, 1, 24090, 100])\n",
    "# mask torch.Size([1, 1, 24090, 100])\n",
    "# coors torch.Size([24090, 4])\n",
    "# anchors torch.Size([1, 480000, 7])\n",
    "# labels torch.Size([1, 480000])\n",
    "# reg_targets torch.Size([1, 480000, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c816e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voxel_features = self.voxel_feature_extractor(pillar_x, pillar_y, pillar_z, pillar_i, \n",
    "#                                               num_points, x_sub_shaped, y_sub_shaped, mask)\n",
    "# trim shape for middle_feature_extractor\n",
    "# voxel_features = voxel_features.squeeze()\n",
    "# voxel_features = voxel_features.permute(1, 0)\n",
    "# return voxel_features\n",
    "\n",
    "# coors = example[8]\n",
    "# spatial_features = self.middle_feature_extractor(\n",
    "#     voxel_features, coors)\n",
    "\n",
    "# print(\"spatial_features.size()\",spatial_features.size())\n",
    "# preds_dict = self.rpn(spatial_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dd280f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voxel_features.shape torch.Size([1, 64, 31485, 1])\n",
    "# voxel_features.shape after squeeze&permute torch.Size([31485, 64])\n",
    "# spatial_features.shape torch.Size([1, 64, 400, 600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39cf9831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voxel_features.shape torch.Size([1, 64, 29031, 1])\n",
    "# voxel_features.shape after squeeze&permute torch.Size([29031, 64])\n",
    "# spatial_features.shape torch.Size([1, 64, 400, 600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-desert",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ret_dict = net(inputs)\n",
    "ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-express",
   "metadata": {},
   "outputs": [],
   "source": [
    "example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)\n",
    "# 0 pillar_x, \n",
    "# 1 pillar_y, \n",
    "# 2 pillar_z, \n",
    "# 3 pillar_i,   \n",
    "# 4 num_points_per_pillar, \n",
    "# 5 x_sub_shaped, \n",
    "# 6 y_sub_shaped, \n",
    "# 7 mask, \n",
    "# 8 coors,        \n",
    "# 9 anchors, \n",
    "# 10 labels, \n",
    "# 11 reg_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-senator",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-local",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pillar_x.shape)\n",
    "print(pillar_y.shape)\n",
    "print(pillar_z.shape)\n",
    "print(pillar_i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-green",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 0 voxels\n",
    "# 1 num_points\n",
    "# 2 coordinates\n",
    "# 3 anchors\n",
    "# 4 anchors_mask\n",
    "# 5 labels\n",
    "# 6 reg_targets\n",
    "# 7 reg_weights\n",
    "# 8 metadata\n",
    "print(\"example['voxels'].shape\",example['voxels'].shape)\n",
    "print(\"example['num_points'].shape\",example['num_points'].shape)\n",
    "print(\"example['coordinates'].shape\",example['coordinates'].shape)\n",
    "print(\"example['anchors'].shape\",example['anchors'].shape)\n",
    "print(\"example['anchors_mask'].shape\",example['anchors_mask'].shape)\n",
    "print(\"example['labels'].shape\",example['labels'].shape)\n",
    "print(\"example['reg_targets'].shape\",example['reg_targets'].shape)\n",
    "print(\"example['reg_weights'].shape\",example['reg_weights'].shape)\n",
    "print(\"example['metadata'].shape\",example['metadata'].shape)\n",
    "pillar_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train()\n",
    "pillar_x = torch.ones([1, 1, 12000, 100],dtype=torch.float32, device=pillar_x.device )\n",
    "pillar_y = torch.ones([1, 1, 12000, 100],dtype=torch.float32, device=pillar_x.device )\n",
    "pillar_z = torch.ones([1, 1, 12000, 100],dtype=torch.float32, device=pillar_x.device )\n",
    "pillar_i = torch.ones([1, 1, 12000, 100],dtype=torch.float32, device=pillar_x.device )\n",
    "num_points_per_pillar = torch.ones([1, 12000],dtype=torch.float32, device=pillar_x.device )\n",
    "x_sub_shaped = torch.ones([1, 1, 12000, 100],dtype=torch.float32, device=pillar_x.device )\n",
    "y_sub_shaped = torch.ones([1, 1, 12000, 100],dtype=torch.float32, device=pillar_x.device )\n",
    "mask = torch.ones([1, 1, 12000, 100],dtype=torch.float32, device=pillar_x.device )\n",
    "example1 = [pillar_x, pillar_y, pillar_z, pillar_i, num_points_per_pillar, x_sub_shaped, y_sub_shaped, mask]\n",
    "det = net(example1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-facial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# TRAINING\n",
    "######################\n",
    "log_path = model_dir / 'log.txt'\n",
    "logf = open(log_path, 'a')\n",
    "logf.write(proto_str)\n",
    "logf.write(\"\\n\")\n",
    "summary_dir = model_dir / 'summary'\n",
    "summary_dir.mkdir(parents=True, exist_ok=True)\n",
    "writer = SummaryWriter(str(summary_dir))\n",
    "\n",
    "total_step_elapsed = 0\n",
    "remain_steps = train_cfg.steps - net.get_global_step()\n",
    "t = time.time()\n",
    "ckpt_start_time = t\n",
    "\n",
    "total_loop = train_cfg.steps // train_cfg.steps_per_eval + 1\n",
    "# total_loop = remain_steps // train_cfg.steps_per_eval + 1\n",
    "clear_metrics_every_epoch = train_cfg.clear_metrics_every_epoch\n",
    "\n",
    "if train_cfg.steps % train_cfg.steps_per_eval == 0:\n",
    "    total_loop -= 1\n",
    "mixed_optimizer.zero_grad()\n",
    "\n",
    "\n",
    "run = True\n",
    "debug = False\n",
    "display_step = 50\n",
    "total_epoch = 15\n",
    "epoch = 0\n",
    "gstep_to_plot = (30000,40000,50000,60000)\n",
    "losses = []\n",
    "mean_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-strip",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net.train()\n",
    "total_epoch = 15\n",
    "try:\n",
    "    while run == True:\n",
    "        print(\"num samples: %d\" % (len(dataset)))\n",
    "        epoch += 1\n",
    "        print(\"epoch\", epoch)\n",
    "        if epoch > total_epoch or debug == True:\n",
    "            break\n",
    "        if clear_metrics_every_epoch:\n",
    "            net.clear_metrics()\n",
    "        for example in tqdm_notebook(dataloader):\n",
    "            lr_scheduler.step()\n",
    "            example_torch = example_convert_to_torch(example, float_dtype)\n",
    "            batch_size = example[\"anchors\"].shape[0]\n",
    "            example_tuple = list(example_torch.values())\n",
    "            # 0 voxels\n",
    "            # 1 num_points\n",
    "            # 2 coordinates\n",
    "            # 3 anchors\n",
    "            # 4 anchors_mask\n",
    "            # 5 labels\n",
    "            # 6 reg_targets\n",
    "            # 7 reg_weights\n",
    "            # 8 metadata\n",
    "            pillar_x = example_tuple[0][:,:,0].unsqueeze(0).unsqueeze(0)\n",
    "            pillar_y = example_tuple[0][:,:,1].unsqueeze(0).unsqueeze(0)\n",
    "            pillar_z = example_tuple[0][:,:,2].unsqueeze(0).unsqueeze(0)\n",
    "            pillar_i = example_tuple[0][:,:,3].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            num_points_per_pillar = example_tuple[1].float().unsqueeze(0)\n",
    "\n",
    "            # Find distance of x, y, and z from pillar center\n",
    "            # assuming xyres_16.proto\n",
    "            coors_x = example_tuple[2][:, 3].float()\n",
    "            coors_y = example_tuple[2][:, 2].float()\n",
    "            vx, vy = voxel_generator.voxel_size[0], voxel_generator.voxel_size[1]\n",
    "            x_offset = vx/2 + voxel_generator.point_cloud_range[0]\n",
    "            y_offset = vy/2 + voxel_generator.point_cloud_range[1]\n",
    "            # self.x_offset = self.vx / 2 + pc_range[0]\n",
    "            # self.y_offset = self.vy / 2 + pc_range[1]\n",
    "            # this assumes xyres 20\n",
    "            # x_sub = coors_x.unsqueeze(1) * 0.16 + 0.1\n",
    "            # y_sub = coors_y.unsqueeze(1) * 0.16 + -39.9\n",
    "            # here assumes xyres 16\n",
    "            x_sub = coors_x.unsqueeze(1)*vx + x_offset\n",
    "            y_sub = coors_y.unsqueeze(1)*vy + y_offset\n",
    "            # x_sub = coors_x.unsqueeze(1)*0.28 + 0.14\n",
    "            # y_sub = coors_y.unsqueeze(1)*0.28 - 20.0\n",
    "            ones = torch.ones([1,voxel_generator._max_num_points], dtype = torch.float32, device = pillar_x.device)\n",
    "            x_sub_shaped = torch.mm(x_sub, ones).unsqueeze(0).unsqueeze(0)\n",
    "            y_sub_shaped = torch.mm(y_sub, ones).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            num_points_for_a_pillar = pillar_x.size()[3]\n",
    "            mask = get_paddings_indicator(num_points_per_pillar, num_points_for_a_pillar, axis=0)\n",
    "            mask = mask.permute(0, 2, 1)\n",
    "            mask = mask.unsqueeze(1)\n",
    "            mask = mask.type_as(pillar_x)\n",
    "\n",
    "            coors   = example_tuple[2]\n",
    "            anchors = example_tuple[3]\n",
    "            labels  = example_tuple[4]\n",
    "            reg_targets = example_tuple[5]\n",
    "\n",
    "            inputs = [pillar_x, pillar_y, pillar_z, pillar_i,\n",
    "                     num_points_per_pillar, x_sub_shaped, y_sub_shaped, mask, coors,\n",
    "                     anchors, labels, reg_targets]\n",
    "\n",
    "            ret_dict = net(inputs)            \n",
    "            # return 0\n",
    "            # ret_dict {\n",
    "            #     0:\"loss\": loss,\n",
    "            #     1:\"cls_loss\": cls_loss,\n",
    "            #     2:\"loc_loss\": loc_loss,\n",
    "            #     3:\"cls_pos_loss\": cls_pos_loss,\n",
    "            #     4:\"cls_neg_loss\": cls_neg_loss,\n",
    "            #     5:\"cls_preds\": cls_preds,\n",
    "            #     6:\"dir_loss_reduced\": dir_loss_reduced,\n",
    "            #     7:\"cls_loss_reduced\": cls_loss_reduced,\n",
    "            #     8:\"loc_loss_reduced\": loc_loss_reduced,\n",
    "            #     9:\"cared\": cared,\n",
    "            # }\n",
    "            # cls_preds = ret_dict[\"cls_preds\"]\n",
    "            \n",
    "            cls_preds = ret_dict[5]\n",
    "            # loss = ret_dict[\"loss\"].mean()\n",
    "            loss = ret_dict[0].mean()\n",
    "            # cls_loss_reduced = ret_dict[\"cls_loss_reduced\"].mean()\n",
    "            cls_loss_reduced = ret_dict[7].mean()\n",
    "            # loc_loss_reduced = ret_dict[\"loc_loss_reduced\"].mean()\n",
    "            loc_loss_reduced = ret_dict[8].mean()\n",
    "            # cls_pos_loss = ret_dict[\"cls_pos_loss\"]\n",
    "            cls_pos_loss = ret_dict[3]\n",
    "            # cls_neg_loss = ret_dict[\"cls_neg_loss\"]\n",
    "            cls_neg_loss = ret_dict[4]\n",
    "            # loc_loss = ret_dict[\"loc_loss\"]\n",
    "            loc_loss = ret_dict[2]\n",
    "            # cls_loss = ret_dict[\"cls_loss\"]\n",
    "            cls_loss = ret_dict[1]\n",
    "            # dir_loss_reduced = ret_dict[\"dir_loss_reduced\"]\n",
    "            dir_loss_reduced = ret_dict[6]\n",
    "            # cared = ret_dict[\"cared\"]\n",
    "            cared = ret_dict[9]\n",
    "            # labels = example_torch[\"labels\"]\n",
    "            labels = example_tuple[5]\n",
    "            if train_cfg.enable_mixed_precision:\n",
    "                loss *= loss_scale\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), 10.0)\n",
    "            mixed_optimizer.step()\n",
    "            mixed_optimizer.zero_grad()\n",
    "            \n",
    "            net.update_global_step()\n",
    "#             net_metrics = net.update_metrics(cls_loss_reduced,\n",
    "#                                              loc_loss_reduced, cls_preds,\n",
    "#                                              labels, cared)\n",
    "            \n",
    "            step_time = (time.time() - t)\n",
    "            t = time.time()\n",
    "            metrics = {}\n",
    "            num_pos = int((labels > 0)[0].float().sum().cpu().numpy())\n",
    "            num_neg = int((labels == 0)[0].float().sum().cpu().numpy())\n",
    "            # if 'anchors_mask' not in example_torch:\n",
    "            #     num_anchors = example_torch['anchors'].shape[1]\n",
    "            # else:\n",
    "            #     num_anchors = int(example_torch['anchors_mask'][0].sum())\n",
    "#             num_anchors = int(example_tuple[7][0].sum())\n",
    "            global_step = net.get_global_step()\n",
    "            losses.append(loss.detach().cpu().item())\n",
    "            if loss > 30:\n",
    "                print(\"weird_global_step\", global_step, \"mean_loss\",np.mean(losses),\"loss\", loss.detach().cpu(),\"loc_loss\", \n",
    "                      loc_loss.detach().cpu().sum(),\"cls_loss\", cls_loss.detach().cpu().sum())\n",
    "\n",
    "            if global_step % display_step == 0:\n",
    "                print(\"global_step\", global_step, \"mean_loss\",np.mean(losses),\"loss\", loss.detach().cpu(),\"loc_loss\", \n",
    "                      loc_loss.detach().cpu().sum(),\"cls_loss\", cls_loss.detach().cpu().sum())\n",
    "                mean_losses.append(np.mean(losses))\n",
    "                losses = []\n",
    "#                 loc_loss_elem = [\n",
    "#                     float(loc_loss[:, :, i].sum().detach().cpu().numpy() /\n",
    "#                           batch_size) for i in range(loc_loss.shape[-1])\n",
    "#                 ]\n",
    "#                 metrics[\"step\"] = global_step\n",
    "#                 metrics[\"steptime\"] = step_time\n",
    "#                 metrics.update(net_metrics)\n",
    "#                 metrics[\"loss\"] = {}\n",
    "#                 metrics[\"loss\"][\"loc_elem\"] = loc_loss_elem\n",
    "#                 metrics[\"loss\"][\"cls_pos_rt\"] = float(\n",
    "#                     cls_pos_loss.detach().cpu().numpy())\n",
    "#                 metrics[\"loss\"][\"cls_neg_rt\"] = float(\n",
    "#                     cls_neg_loss.detach().cpu().numpy())\n",
    "#                 # if unlabeled_training:\n",
    "#                 #     metrics[\"loss\"][\"diff_rt\"] = float(\n",
    "#                 #         diff_loc_loss_reduced.detach().cpu().numpy())\n",
    "#                 if model_cfg.use_direction_classifier:\n",
    "#                     metrics[\"loss\"][\"dir_rt\"] = float(\n",
    "#                         dir_loss_reduced.detach().cpu().numpy())\n",
    "#                 # metrics[\"num_vox\"] = int(example_torch[\"voxels\"].shape[0])\n",
    "#                 metrics[\"num_vox\"] = int(example_tuple[0].shape[0])\n",
    "#                 metrics[\"num_pos\"] = int(num_pos)\n",
    "#                 metrics[\"num_neg\"] = int(num_neg)\n",
    "#                 metrics[\"num_anchors\"] = int(num_anchors)\n",
    "#                 metrics[\"lr\"] = float(\n",
    "#                     mixed_optimizer.param_groups[0]['lr'])\n",
    "#                 # metrics[\"image_idx\"] = example['image_idx'][0]\n",
    "#                 flatted_metrics = flat_nested_json_dict(metrics)\n",
    "#                 flatted_summarys = flat_nested_json_dict(metrics, \"/\")\n",
    "#                 for k, v in flatted_summarys.items():\n",
    "#                     if isinstance(v, (list, tuple)):\n",
    "#                         v = {str(i): e for i, e in enumerate(v)}\n",
    "#                         writer.add_scalars(k, v, global_step)\n",
    "#                     else:\n",
    "#                         writer.add_scalar(k, v, global_step)\n",
    "#                 metrics_str_list = []\n",
    "#                 for k, v in flatted_metrics.items():\n",
    "#                     if isinstance(v, float):\n",
    "#                         metrics_str_list.append(f\"{k}={v:.3}\")\n",
    "#                     elif isinstance(v, (list, tuple)):\n",
    "#                         if v and isinstance(v[0], float):\n",
    "#                             v_str = ', '.join([f\"{e:.3}\" for e in v])\n",
    "#                             metrics_str_list.append(f\"{k}=[{v_str}]\")\n",
    "#                         else:\n",
    "#                             metrics_str_list.append(f\"{k}={v}\")\n",
    "#                     else:\n",
    "#                         metrics_str_list.append(f\"{k}={v}\")\n",
    "#                 log_str = ', '.join(metrics_str_list)\n",
    "#                 print(log_str, file=logf)\n",
    "#                 print(log_str)\n",
    "            ckpt_elasped_time = time.time() - ckpt_start_time\n",
    "            if ckpt_elasped_time > train_cfg.save_checkpoints_secs:\n",
    "                torchplus.train.save_models(model_dir, {f\"Pointpillars\":net, f\"Adam_OneCycle\":optimizer},\n",
    "                                            net.get_global_step())\n",
    "                ckpt_start_time = time.time()\n",
    "#         total_step_elapsed += steps\n",
    "        torchplus.train.save_models(model_dir, {f\"Pointpillars\":net, f\"Adam_OneCycle\":optimizer},\n",
    "                                    net.get_global_step())\n",
    "#         if global_step in gstep_to_plot:\n",
    "#             example = next(iter(dataloader))\n",
    "#             example_torch = example_convert_to_torch(example, float_dtype)\n",
    "#             batch_size = example[\"anchors\"].shape[0]\n",
    "#             example_tuple = list(example_torch.values())\n",
    "\n",
    "#             pillar_x = example_tuple[0][:,:,0].unsqueeze(0).unsqueeze(0)\n",
    "#             pillar_y = example_tuple[0][:,:,1].unsqueeze(0).unsqueeze(0)\n",
    "#             pillar_z = example_tuple[0][:,:,2].unsqueeze(0).unsqueeze(0)\n",
    "#             # pillar_i = example_tuple[0][:,:,3].unsqueeze(0).unsqueeze(0)\n",
    "#             pillar_i = torch.ones(pillar_x.shape,dtype=torch.float32, device=pillar_x.device )\n",
    "\n",
    "#             num_points_per_pillar = example_tuple[1].float().unsqueeze(0)\n",
    "\n",
    "#             # Find distance of x, y, and z from pillar center\n",
    "#             # assuming xyres_16.proto\n",
    "#             coors_x = example_tuple[2][:, 3].float()\n",
    "#             coors_y = example_tuple[2][:, 2].float()\n",
    "#             vx, vy = voxel_generator.voxel_size[0], voxel_generator.voxel_size[1]\n",
    "#             x_offset = vx/2 + voxel_generator.point_cloud_range[0]\n",
    "#             y_offset = vy/2 + voxel_generator.point_cloud_range[1]\n",
    "#             # self.x_offset = self.vx / 2 + pc_range[0]\n",
    "#             # self.y_offset = self.vy / 2 + pc_range[1]\n",
    "#             # this assumes xyres 20\n",
    "#             # x_sub = coors_x.unsqueeze(1) * 0.16 + 0.1\n",
    "#             # y_sub = coors_y.unsqueeze(1) * 0.16 + -39.9\n",
    "#             # here assumes xyres 16\n",
    "#             x_sub = coors_x.unsqueeze(1)*vx + x_offset\n",
    "#             y_sub = coors_y.unsqueeze(1)*vy + y_offset\n",
    "#             # x_sub = coors_x.unsqueeze(1)*0.28 + 0.14\n",
    "#             # y_sub = coors_y.unsqueeze(1)*0.28 - 20.0\n",
    "#             ones = torch.ones([1,voxel_generator._max_num_points], dtype = torch.float32, device = pillar_x.device)\n",
    "#             x_sub_shaped = torch.mm(x_sub, ones)\n",
    "#             y_sub_shaped = torch.mm(y_sub, ones)\n",
    "\n",
    "#             num_points_for_a_pillar = pillar_x.size()[3]\n",
    "#             mask = get_paddings_indicator(num_points_per_pillar, num_points_for_a_pillar, axis=0)\n",
    "#             mask = mask.permute(0, 2, 1)\n",
    "#             mask = mask.unsqueeze(1)\n",
    "#             mask = mask.type_as(pillar_x)\n",
    "\n",
    "#             coors   = example_tuple[2]\n",
    "#             anchors = example_tuple[3]\n",
    "#             labels  = example_tuple[4]\n",
    "#             reg_targets = example_tuple[5]\n",
    "\n",
    "#             inputs = [pillar_x, pillar_y, pillar_z, pillar_i,\n",
    "#                      num_points_per_pillar, x_sub_shaped, y_sub_shaped, mask, coors,\n",
    "#                      anchors, labels, reg_targets]\n",
    "\n",
    "#             ret_dict = net(inputs)         \n",
    "\n",
    "#             example_token = example[\"metadata\"][0]['token']\n",
    "#             filtered_sample_tokens = dataset.dataset.filtered_sample_tokens\n",
    "#             index = filtered_sample_tokens.index(example[\"metadata\"][0]['token'])\n",
    "#             gt_example = dataset.dataset.get_sensor_data(index)\n",
    "#             points = gt_example['lidar']['points']\n",
    "#             pc_range = model_cfg.voxel_generator.point_cloud_range\n",
    "#             points = np.array(\n",
    "#                 [p for p in points if (pc_range[0] < p[0] < pc_range[3]) & (pc_range[1] < p[1] < pc_range[4]) & (\n",
    "#                         pc_range[2] < p[2] < pc_range[5])])\n",
    "#             gt_boxes = gt_example['lidar']['annotations']['gt_boxes']\n",
    "#             gt_labels = gt_example['lidar']['annotations']['gt_names']\n",
    "#             c = np.zeros(points[:, 3].shape[0]).reshape(-1, 1)\n",
    "#             c = np.concatenate([c, c, c], axis=1)\n",
    "#             points = points[:, 0:3]\n",
    "#             pc = o3d.geometry.PointCloud()\n",
    "#             pc.points = o3d.utility.Vector3dVector(points)\n",
    "#             pc.colors = o3d.utility.Vector3dVector(c)\n",
    "#             mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=2.0,\n",
    "#                                                                            origin=[-0, -0, -0])\n",
    "#             geo = [pc, mesh_frame]\n",
    "#             detection = [{'box3d_lidar': ret_dict[0][0], \"label_preds\": ret_dict[0][2], 'scores': ret_dict[0][1]}]\n",
    "#             geo = add_prediction_per_class(dataset.dataset.nusc,\n",
    "#                                            detection, gt_boxes, gt_labels,\n",
    "#                                            [\"traffic_cone\"], geo)\n",
    "#             o3d.visualization.draw_geometries(geo)\n",
    "#             model.train()\n",
    "            \n",
    "            # Ensure that all evaluation points are saved forever\n",
    "        torchplus.train.save_models(eval_checkpoint_dir, {f\"Pointpillars\":net, f\"Adam_OneCycle\":optimizer}, net.get_global_step(), max_to_keep=100)\n",
    "\n",
    "except Exception as e:\n",
    "    torchplus.train.save_models(model_dir, {f\"Pointpillars\":net, f\"Adam_OneCycle\":optimizer},\n",
    "                                net.get_global_step())\n",
    "    logf.close()\n",
    "    raise e\n",
    "finally:\n",
    "    torchplus.train.save_models(model_dir, {f\"Pointpillars\":net, f\"Adam_OneCycle\":optimizer}, net.get_global_step())\n",
    "    logf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-prior",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train()\n",
    "torchplus.train.save_models(eval_checkpoint_dir, {f\"Pointpillars\":net, f\"Adam_OneCycle\":optimizer}, net.get_global_step(), max_to_keep=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_neg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-conditioning",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-therapist",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-discussion",
   "metadata": {},
   "outputs": [],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-heather",
   "metadata": {},
   "source": [
    "## Export ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_torch = example_convert_to_torch(example, float_dtype)\n",
    "batch_size = example[\"anchors\"].shape[0]\n",
    "example_tuple = list(example_torch.values())\n",
    "\n",
    "pillar_x = example_tuple[0][:,:,0].unsqueeze(0).unsqueeze(0)\n",
    "pillar_y = example_tuple[0][:,:,1].unsqueeze(0).unsqueeze(0)\n",
    "pillar_z = example_tuple[0][:,:,2].unsqueeze(0).unsqueeze(0)\n",
    "#             pillar_i = example_tuple[0][:,:,3].unsqueeze(0).unsqueeze(0)\n",
    "pillar_i = torch.ones(pillar_x.shape,dtype=torch.float32, device=pillar_x.device )\n",
    "\n",
    "num_points_per_pillar = example_tuple[1].float().unsqueeze(0)\n",
    "\n",
    "# Find distance of x, y, and z from pillar center\n",
    "# assuming xyres_16.proto\n",
    "coors_x = example_tuple[2][:, 3].float()\n",
    "coors_y = example_tuple[2][:, 2].float()\n",
    "vx, vy = voxel_generator.voxel_size[0], voxel_generator.voxel_size[1]\n",
    "x_offset = vx/2 + voxel_generator.point_cloud_range[0]\n",
    "y_offset = vy/2 + voxel_generator.point_cloud_range[1]\n",
    "# self.x_offset = self.vx / 2 + pc_range[0]\n",
    "# self.y_offset = self.vy / 2 + pc_range[1]\n",
    "# this assumes xyres 20\n",
    "# x_sub = coors_x.unsqueeze(1) * 0.16 + 0.1\n",
    "# y_sub = coors_y.unsqueeze(1) * 0.16 + -39.9\n",
    "# here assumes xyres 16\n",
    "x_sub = coors_x.unsqueeze(1)*vx + x_offset\n",
    "y_sub = coors_y.unsqueeze(1)*vy + y_offset\n",
    "# x_sub = coors_x.unsqueeze(1)*0.28 + 0.14\n",
    "# y_sub = coors_y.unsqueeze(1)*0.28 - 20.0\n",
    "ones = torch.ones([1,voxel_generator._max_num_points], dtype = torch.float32, device = pillar_x.device)\n",
    "x_sub_shaped = torch.mm(x_sub, ones).unsqueeze(0).unsqueeze(0)\n",
    "y_sub_shaped = torch.mm(y_sub, ones).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "num_points_for_a_pillar = pillar_x.size()[3]\n",
    "mask = get_paddings_indicator(num_points_per_pillar, num_points_for_a_pillar, axis=0)\n",
    "mask = mask.permute(0, 2, 1)\n",
    "mask = mask.unsqueeze(1)\n",
    "mask = mask.type_as(pillar_x)\n",
    "\n",
    "coors   = example_tuple[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-explorer",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pillar_x.size()\",pillar_x.size())\n",
    "print(\"pillar_y.size()\",pillar_y.size())\n",
    "print(\"pillar_z.size()\",pillar_z.size())\n",
    "print(\"pillar_i.size()\",pillar_i.size())\n",
    "print(\"num_points_per_pillar.size()\",num_points_per_pillar.size())\n",
    "print(\"x_sub_shaped.size()\",x_sub_shaped.size())\n",
    "print(\"y_sub_shaped.size()\",y_sub_shaped.size())\n",
    "print(\"mask.size()\",mask.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-announcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "pillar_x = torch.ones([1, 1, 70000, 100], dtype=torch.float32, device=pillar_x.device)\n",
    "pillar_y = torch.ones([1, 1, 70000, 100], dtype=torch.float32, device=pillar_x.device)\n",
    "pillar_z = torch.ones([1, 1, 70000, 100], dtype=torch.float32, device=pillar_x.device)\n",
    "pillar_i = torch.ones([1, 1, 70000, 100], dtype=torch.float32, device=pillar_x.device)\n",
    "num_points_per_pillar = torch.ones([1, 70000], dtype=torch.float32, device=pillar_x.device)\n",
    "x_sub_shaped = torch.ones([1, 1, 70000, 100], dtype=torch.float32, device=pillar_x.device)\n",
    "y_sub_shaped = torch.ones([1, 1, 70000, 100], dtype=torch.float32, device=pillar_x.device)\n",
    "mask = torch.ones([1, 1, 70000, 100], dtype=torch.float32, device=pillar_x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-myrtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "example1 = [pillar_x, pillar_y, pillar_z, pillar_i,\n",
    "         num_points_per_pillar, x_sub_shaped, y_sub_shaped, mask]\n",
    "input_names = ['pillar_x', 'pillar_y', 'pillar_z', 'pillar_i',\n",
    "         'num_points_per_pillar', 'x_sub_shaped', 'y_sub_shaped', 'mask']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(net, example1, \"pfe.onnx\", verbose=False, input_names=input_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_input = torch.ones([1, 64, 400, 600], dtype=torch.float32, device=pillar_x.device)\n",
    "torch.onnx.export(net.rpn, rpn_input, \"rpn.onnx\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-hygiene",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-composition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-generator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "manual-healing",
   "metadata": {},
   "source": [
    "## Plot eval once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-development",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-xerox",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "example_torch = example_convert_to_torch(example, float_dtype)\n",
    "batch_size = example[\"anchors\"].shape[0]\n",
    "example_tuple = list(example_torch.values())\n",
    "\n",
    "pillar_x = example_tuple[0][:,:,0].unsqueeze(0).unsqueeze(0)\n",
    "pillar_y = example_tuple[0][:,:,1].unsqueeze(0).unsqueeze(0)\n",
    "pillar_z = example_tuple[0][:,:,2].unsqueeze(0).unsqueeze(0)\n",
    "# pillar_i = example_tuple[0][:,:,3].unsqueeze(0).unsqueeze(0)\n",
    "pillar_i = torch.ones(pillar_x.shape,dtype=torch.float32, device=pillar_x.device )\n",
    "\n",
    "num_points_per_pillar = example_tuple[1].float().unsqueeze(0)\n",
    "\n",
    "# Find distance of x, y, and z from pillar center\n",
    "# assuming xyres_16.proto\n",
    "coors_x = example_tuple[2][:, 3].float()\n",
    "coors_y = example_tuple[2][:, 2].float()\n",
    "vx, vy = voxel_generator.voxel_size[0], voxel_generator.voxel_size[1]\n",
    "x_offset = vx/2 + voxel_generator.point_cloud_range[0]\n",
    "y_offset = vy/2 + voxel_generator.point_cloud_range[1]\n",
    "# self.x_offset = self.vx / 2 + pc_range[0]\n",
    "# self.y_offset = self.vy / 2 + pc_range[1]\n",
    "# this assumes xyres 20\n",
    "# x_sub = coors_x.unsqueeze(1) * 0.16 + 0.1\n",
    "# y_sub = coors_y.unsqueeze(1) * 0.16 + -39.9\n",
    "# here assumes xyres 16\n",
    "x_sub = coors_x.unsqueeze(1)*vx + x_offset\n",
    "y_sub = coors_y.unsqueeze(1)*vy + y_offset\n",
    "# x_sub = coors_x.unsqueeze(1)*0.28 + 0.14\n",
    "# y_sub = coors_y.unsqueeze(1)*0.28 - 20.0\n",
    "ones = torch.ones([1,voxel_generator._max_num_points], dtype = torch.float32, device = pillar_x.device)\n",
    "x_sub_shaped = torch.mm(x_sub, ones)\n",
    "y_sub_shaped = torch.mm(y_sub, ones)\n",
    "\n",
    "num_points_for_a_pillar = pillar_x.size()[3]\n",
    "mask = get_paddings_indicator(num_points_per_pillar, num_points_for_a_pillar, axis=0)\n",
    "mask = mask.permute(0, 2, 1)\n",
    "mask = mask.unsqueeze(1)\n",
    "mask = mask.type_as(pillar_x)\n",
    "\n",
    "coors   = example_tuple[2]\n",
    "anchors = example_tuple[3]\n",
    "labels  = example_tuple[4]\n",
    "reg_targets = example_tuple[5]\n",
    "\n",
    "inputs = [pillar_x, pillar_y, pillar_z, pillar_i,\n",
    "         num_points_per_pillar, x_sub_shaped, y_sub_shaped, mask, coors,\n",
    "         anchors, labels, reg_targets]\n",
    "\n",
    "ret_dict = net(inputs) \n",
    "ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# example_token = example[\"metadata\"][0]['token']\n",
    "# filtered_sample_tokens = dataset.dataset.filtered_sample_tokens\n",
    "# index = filtered_sample_tokens.index(example[\"metadata\"][0]['token'])\n",
    "# gt_example = dataset.dataset.get_sensor_data(index)\n",
    "points = example['points'][0]\n",
    "pc_range = model_cfg.voxel_generator.point_cloud_range\n",
    "points = np.array(\n",
    "    [p for p in points if (pc_range[0] < p[0] < pc_range[3]) & (pc_range[1] < p[1] < pc_range[4]) & (\n",
    "            pc_range[2] < p[2] < pc_range[5])])\n",
    "gt_boxes = example['gt_boxes']\n",
    "gt_labels = example['gt_names'][0]\n",
    "c = np.zeros(points[:, 3].shape[0]).reshape(-1, 1)\n",
    "c = np.concatenate([c, c, c], axis=1)\n",
    "points = points[:, 0:3]\n",
    "pc = o3d.geometry.PointCloud()\n",
    "pc.points = o3d.utility.Vector3dVector(points)\n",
    "pc.colors = o3d.utility.Vector3dVector(c)\n",
    "mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=2.0,\n",
    "                                                               origin=[-0, -0, -0])\n",
    "geo = [pc, mesh_frame]\n",
    "detection = [{'box3d_lidar': ret_dict[0][0], \"label_preds\": ret_dict[0][2], 'scores': ret_dict[0][1]}]\n",
    "geo = add_prediction_per_class(dataset.dataset.nusc,\n",
    "                               detection, gt_boxes, gt_labels,\n",
    "                               [\"traffic_cone\"], geo)\n",
    "o3d.visualization.draw_geometries(geo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-flooring",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-insert",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(range(len(mean_losses)))*50,mean_losses,\"r.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-roman",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-helmet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-choice",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-simulation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-symphony",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "broadband-unemployment",
   "metadata": {},
   "source": [
    "# Eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "from second.data.preprocess import merge_second_batch, prep_pointcloud\n",
    "\n",
    "# pc_file_name = \"/media/starlet/LdTho/data/sets/1614053547.551486.pcd\"\n",
    "# pc_file_name = \"/media/starlet/LdTho/data/sets/1612769433360603.pcd\"\n",
    "pc_file_name = \"/media/starlet/LdTho/data/sets/baraja_lidar/1612769435143246.pcd\"\n",
    "pcd = o3d.io.read_point_cloud(pc_file_name)\n",
    "points = np.asarray(pcd.points)\n",
    "pc_range = model_cfg.voxel_generator.point_cloud_range\n",
    "points = np.array([p for p in points if (pc_range[0] < p[0] < pc_range[3])\n",
    "                   & (pc_range[1] < p[1] < pc_range[4])\n",
    "                   & (pc_range[2] < p[2] < pc_range[5])])\n",
    "\n",
    "points = np.concatenate([points.transpose(), \n",
    "                         np.array([np.ones(points.shape[0])]),],axis = 0).transpose()\n",
    "points = points[~np.isnan(points).any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_dict = {\n",
    "    'lidar': {\n",
    "        'type': 'lidar',\n",
    "        'points': points\n",
    "    },\n",
    "    'metadata': {\n",
    "        'token': pc_file_name\n",
    "    }\n",
    "}\n",
    "\n",
    "out_size_factor = model_cfg.rpn.layer_strides[0] // model_cfg.rpn.upsample_strides[0]\n",
    "example = prep_pointcloud(input_dict=input_dict,\n",
    "                          root_path= None ,\n",
    "                          voxel_generator= voxel_generator,\n",
    "                          target_assigner= target_assigner,\n",
    "                          max_voxels= 70000,\n",
    "                          training= False,\n",
    "                          create_targets=False,\n",
    "                          shuffle_points=False,\n",
    "                          num_point_features=model_cfg.num_point_features,\n",
    "                          remove_outside_points=False,\n",
    "                          anchor_cache=None,\n",
    "                          anchor_area_threshold=-1,\n",
    "                          out_size_factor=out_size_factor,\n",
    "                          out_dtype=np.float32\n",
    "                          )\n",
    "# example[\"points\"] = points\n",
    "example[\"metadata\"] = input_dict[\"metadata\"]\n",
    "example = [example]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-bowling",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader = torch.utils.data.DataLoader(\n",
    "        example,\n",
    "        batch_size=input_cfg.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=False,\n",
    "        collate_fn=merge_second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-medication",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net.eval()\n",
    "example = next(iter(eval_dataloader))\n",
    "example_torch = example_convert_to_torch(example, float_dtype)\n",
    "batch_size = example[\"anchors\"].shape[0]\n",
    "example_tuple = list(example_torch.values())\n",
    "\n",
    "pillar_x = example_tuple[0][:,:,0].unsqueeze(0).unsqueeze(0)\n",
    "pillar_y = example_tuple[0][:,:,1].unsqueeze(0).unsqueeze(0)\n",
    "pillar_z = example_tuple[0][:,:,2].unsqueeze(0).unsqueeze(0)\n",
    "# pillar_i = example_tuple[0][:,:,3].unsqueeze(0).unsqueeze(0)\n",
    "pillar_i = torch.ones(pillar_x.shape,dtype=torch.float32, device=pillar_x.device )\n",
    "\n",
    "num_points_per_pillar = example_tuple[1].float().unsqueeze(0)\n",
    "\n",
    "# Find distance of x, y, and z from pillar center\n",
    "# assuming xyres_16.proto\n",
    "coors_x = example_tuple[2][:, 3].float()\n",
    "coors_y = example_tuple[2][:, 2].float()\n",
    "vx, vy = voxel_generator.voxel_size[0], voxel_generator.voxel_size[1]\n",
    "x_offset = vx/2 + voxel_generator.point_cloud_range[0]\n",
    "y_offset = vy/2 + voxel_generator.point_cloud_range[1]\n",
    "# self.x_offset = self.vx / 2 + pc_range[0]\n",
    "# self.y_offset = self.vy / 2 + pc_range[1]\n",
    "# this assumes xyres 20\n",
    "# x_sub = coors_x.unsqueeze(1) * 0.16 + 0.1\n",
    "# y_sub = coors_y.unsqueeze(1) * 0.16 + -39.9\n",
    "# here assumes xyres 16\n",
    "x_sub = coors_x.unsqueeze(1)*vx + x_offset\n",
    "y_sub = coors_y.unsqueeze(1)*vy + y_offset\n",
    "# x_sub = coors_x.unsqueeze(1)*0.28 + 0.14\n",
    "# y_sub = coors_y.unsqueeze(1)*0.28 - 20.0\n",
    "ones = torch.ones([1,voxel_generator._max_num_points], dtype = torch.float32, device = pillar_x.device)\n",
    "x_sub_shaped = torch.mm(x_sub, ones)\n",
    "y_sub_shaped = torch.mm(y_sub, ones)\n",
    "\n",
    "num_points_for_a_pillar = pillar_x.size()[3]\n",
    "mask = get_paddings_indicator(num_points_per_pillar, num_points_for_a_pillar, axis=0)\n",
    "mask = mask.permute(0, 2, 1)\n",
    "mask = mask.unsqueeze(1)\n",
    "mask = mask.type_as(pillar_x)\n",
    "\n",
    "coors   = example_tuple[2]\n",
    "anchors = example_tuple[3]\n",
    "labels  = example_tuple[4]\n",
    "# reg_targets = example_tuple[5]\n",
    "\n",
    "inputs = [pillar_x, pillar_y, pillar_z, pillar_i,\n",
    "         num_points_per_pillar, x_sub_shaped, y_sub_shaped, mask, coors,\n",
    "         anchors, labels]\n",
    "\n",
    "ret_dict = net(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-clone",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.asarray(pcd.points)\n",
    "points = np.array(\n",
    "    [p for p in points if (pc_range[0] < p[0] < pc_range[3]) & (pc_range[1] < p[1] < pc_range[4]) & (\n",
    "           pc_range[2]  < p[2] < pc_range[5])])\n",
    "points = np.concatenate([points.transpose(), \n",
    "                         np.array([np.repeat(0.0, points.shape[0])]),\n",
    "                         np.array([np.repeat(0.0, points.shape[0])])],axis = 0).transpose()\n",
    "points = points[~np.isnan(points).any(axis=1)]\n",
    "\n",
    "c = points[:, 3].reshape(-1, 1)\n",
    "c = np.concatenate([c, c, c], axis=1)\n",
    "points = points[:, 0:3]\n",
    "\n",
    "pc = o3d.geometry.PointCloud()\n",
    "pc.points = o3d.utility.Vector3dVector(points)\n",
    "pc.colors = o3d.utility.Vector3dVector(c)\n",
    "mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=2.0,\n",
    "                                                               origin=[-0, -0, -0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-salon",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = [pc, mesh_frame]\n",
    "detection = [{'box3d_lidar': ret_dict[0][0], \"label_preds\": ret_dict[0][2], 'scores': ret_dict[0][1]}]\n",
    "det_boxes = detection[0]['box3d_lidar'].cpu().detach().numpy()\n",
    "det_labels = detection[0]['label_preds'].cpu().detach().numpy()\n",
    "det_scores = detection[0]['scores'].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-divorce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['traffic_cone']\n",
    "color = {\n",
    "\"traffic_cone\": (1,0,0),\n",
    "\"gt_traffic_cone\": (0,1,0),\n",
    "\"pedestrian\": (1,1,0),\n",
    "\"gt_pedestrian\": (0,0,1)\n",
    "}\n",
    "for i, class_name in enumerate(class_names):\n",
    "    mask = np.logical_and(det_labels == i, det_scores > 0.21)\n",
    "    class_det_boxes = det_boxes[mask]\n",
    "    class_det_scores = det_scores[mask]\n",
    "    class_det_labels = det_labels[mask]\n",
    "    print(len(class_det_boxes),len(class_det_scores),len(class_det_labels))\n",
    "    print(class_det_scores)\n",
    "    rbbox_corners = box_np_ops.center_to_corner_box3d(class_det_boxes[:, :3],\n",
    "                                                      class_det_boxes[:, 3:6],\n",
    "                                                      class_det_boxes[:, 6],\n",
    "                                                      origin=(0.5, 0.5, 0.5), axis=2)\n",
    "\n",
    "    for j in range(len(rbbox_corners)):\n",
    "        geo.append(buildBBox(rbbox_corners[j],\n",
    "                                    color=color[class_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries(geo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "pillar_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-agreement",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-praise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-heading",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_path = \"/media/starlet/LdTho/data/sets/KITTI/training/velodyne/000000.bin\"\n",
    "points = np.fromfile(\n",
    "        str(v_path), dtype=np.float32,\n",
    "        count=-1).reshape([-1, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-destination",
   "metadata": {},
   "outputs": [],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-brunei",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
